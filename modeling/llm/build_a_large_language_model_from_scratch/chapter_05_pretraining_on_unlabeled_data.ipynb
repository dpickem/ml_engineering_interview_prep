{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretraining from unlabeled data (chapter 5)\n",
    "\n",
    "This notebook explores pretraining process of LLMs based on Sebastian Raschka's book (Chapter 5). In particular, it discusses the following:\n",
    "\n",
    "1. Computing the **training** and **validation set losses** to assess the quality of LLM-generated text during training\n",
    "2. Implementing a **training function** and pretraining the LLM\n",
    "3. **Saving and loading model weights** to continue training an LLM\n",
    "4. **Loading pretrained weights** from OpenAI\n",
    "\n",
    "## Acknowledgment\n",
    "\n",
    "All concepts, architectures, and implementation approaches are credited to Sebastian Raschka's work.  This repository serves as my personal implementation and notes while working through the book's content.\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Sebastian Raschka's GitHub](https://github.com/rasbt)\n",
    "- [Book Information](https://www.manning.com/books/build-a-large-language-model-from-scratch)\n",
    "    - [Chapter 5](https://livebook.manning.com/book/build-a-large-language-model-from-scratch/chapter-5)\n",
    "- [Pytorch Lightning - great tutorial collection](https://lightning.ai/docs/pytorch/stable/levels/core_skills.html#)\n",
    "\n",
    "![Topic overview](https://drek4537l1klr.cloudfront.net/raschka/Figures/5-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipynb in /home/dpickem/miniconda3/envs/avpc-off-vehicle-qa-agent/lib/python3.12/site-packages (0.5.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# This installs the ipynb package which enables importing functions defined in other notebooks.\n",
    "%pip install ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.7.0\n",
      "Total number of character: 20479\n",
      "Total number of tokens: 5145\n",
      "x: [290, 4920, 2241, 287]\n",
      "y:      [4920, 2241, 287, 257]\n",
      " and ---->  established\n",
      " and established ---->  himself\n",
      " and established himself ---->  in\n",
      " and established himself in ---->  a\n",
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n",
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n",
      "torch.Size([6, 3])\n",
      "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n",
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n",
      "Token IDs: tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "Inputs shape: torch.Size([8, 4])\n",
      "Embedding shape: torch.Size([8, 4, 256])\n",
      "Position embedding layer shape: torch.Size([4, 256])\n",
      "Input embeddings shape: torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "# Import previous chapter dependencies.\n",
    "# See https://stackoverflow.com/questions/44116194/import-a-function-from-another-ipynb-file\n",
    "# NOTE: Importing these functions seems to run the entire cell the symbol is defined in, which would\n",
    "#       suggest that symbols should be defined in separate cells from the test code.\n",
    "from ipynb.fs.full.chapter_04_gpt_from_scratch import (\n",
    "    GPTConfig,\n",
    "    GPTModel,\n",
    "    generate_text_simple,\n",
    ")\n",
    "from ipynb.fs.full.chapter_02_dataset_creation import create_dataloader_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the GPT-2 configuration with shortened context length.\n",
    "GPT_CONFIG_124M = GPTConfig(\n",
    "    vocab_size=50257,  # as used by the BPE tokenizer for GPT-2.\n",
    "    context_length=256,\n",
    "    emb_dim=768,\n",
    "    n_heads=12,\n",
    "    n_layers=12,\n",
    "    dropout_rate=0.1,\n",
    "    qkv_bias=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two training examples in a batch.\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch: tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "Output shape: torch.Size([2, 4, 50257])\n"
     ]
    }
   ],
   "source": [
    "# Test the GPT model.\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# Run the model on the batch.\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()\n",
    "out = model(batch)\n",
    "\n",
    "print(f\"Input batch: {batch}\")\n",
    "print(f\"Output shape: {out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text encoder and decoder utilities\n",
    "\n",
    "![Topic overview](https://drek4537l1klr.cloudfront.net/raschka/Figures/5-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text to token conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_token_ids(\n",
    "    text: str, tokenizer: Optional[tiktoken.Encoding] = None\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Convert a text string to a tensor of token IDs.\n",
    "\n",
    "    Args:\n",
    "        text: The text to convert to token IDs.\n",
    "        tokenizer: The tokenizer to use.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of token IDs.\n",
    "    \"\"\"\n",
    "    # Instantiate a default tokenizer (if non was provided).\n",
    "    # Tokenize the input text.\n",
    "    encoded = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "    # Convert the tokenized text to a tensor.\n",
    "    # NOTE: .unsqueeze(0) adds the batch dimension.\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "    return encoded_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token to text conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_ids_to_text(\n",
    "    token_ids: torch.Tensor, tokenizer: Optional[tiktoken.Encoding] = None\n",
    ") -> str:\n",
    "    \"\"\"Convert a tensor of token IDs to a text string.\n",
    "\n",
    "    Args:\n",
    "        token_ids: The tensor of token IDs to convert to text.\n",
    "        tokenizer: The tokenizer to use.\n",
    "\n",
    "    Returns:\n",
    "        str: The text string.\n",
    "    \"\"\"\n",
    "    # Instantiate a default tokenizer (if non was provided).\n",
    "    # NOTE: .squeeze(0) removes the batch dimension.\n",
    "    flat = token_ids.squeeze(0)\n",
    "    return tokenizer.decode(flat.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "# Test the text to token conversion.\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M.context_length,\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function\n",
    "\n",
    "Computing the loss involves 5 steps as shown in the following figure. The example below uses a seven word vocabulary for illustration purposes.\n",
    "\n",
    "For each of the three input tokens, shown on the left, we compute a vector containing probability scores corresponding to each token in the vocabulary. The index position of the highest probability score in each vector represents the most likely next token ID. These token IDs associated with the highest probability scores are selected and mapped back into a text that represents the text generated by the model.\n",
    "\n",
    "![Text generation loss](https://drek4537l1klr.cloudfront.net/raschka/Figures/5-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example - step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probas shape: torch.Size([2, 3, 50257])\n",
      "Token IDs:\n",
      " tensor([[[16657],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n",
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  Armed heNetflix\n"
     ]
    }
   ],
   "source": [
    "# Develop the loss function using a batch of two simple examples.\n",
    "inputs = torch.tensor(\n",
    "    [[16833, 3626, 6100], [40, 1107, 588]],  # [\"every effort moves\", \"I really like\"]\n",
    ")\n",
    "\n",
    "# Define the targets, which are the next tokens in the sequences.\n",
    "targets = torch.tensor(\n",
    "    [\n",
    "        [3626, 6100, 345],\n",
    "        [1107, 588, 11311],\n",
    "    ]  # [\" effort moves you\", \" really like chocolate\"]\n",
    ")\n",
    "\n",
    "# Compute the logits for the inputs.\n",
    "# NOTE: We disable gradient computation since gradients are only used for training.\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "\n",
    "# Compute the probabilities of each token in the vocabulary.\n",
    "# NOTE: The shape of probas is [B, T, V] where\n",
    "#\n",
    "# B is the batch size\n",
    "# T is the sequence length\n",
    "# V is the vocabulary size.\n",
    "probas = torch.softmax(logits, dim=-1)\n",
    "print(f\"Probas shape: {probas.shape}\")\n",
    "\n",
    "# Step 3 and 4: Convert the probabilities to token IDs via a greedy decoding strategy.\n",
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "# Print both batches of token IDs.\n",
    "print(\"Token IDs:\\n\", token_ids)\n",
    "\n",
    "# Step 5: Convert the token IDs back to text.\n",
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1:\" f\" {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probas.shape: torch.Size([2, 3, 50257])\n",
      "Text 1: tensor([    0.0001,     0.0000,     0.0000])\n",
      "Text 2: tensor([    0.0000,     0.0001,     0.0000])\n"
     ]
    }
   ],
   "source": [
    "# For each of the two input texts, we can print the initial softmax probability scores\n",
    "# corresponding to the target tokens using the following code:\n",
    "\n",
    "batch_idx = 0\n",
    "# TODO: Why can't we just use probas[batch_idx, :, targets[batch_idx]] since T = 3?\n",
    "target_probas_1 = probas[batch_idx, [0, 1, 2], targets[batch_idx]]\n",
    "print(f\"probas.shape: {probas.shape}\")\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "batch_idx = 1\n",
    "target_probas_2 = probas[batch_idx, [0, 1, 2], targets[batch_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the loss step by step\n",
    "\n",
    "![Loss computation](https://drek4537l1klr.cloudfront.net/raschka/Figures/5-7.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_probas: tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])\n",
      "avg_log_probas: -10.793964385986328\n",
      "neg_avg_log_probas: 10.793964385986328\n"
     ]
    }
   ],
   "source": [
    "# Compute the log probabilities of the target tokens.\n",
    "# NOTE: Working with logarithms of probability scores is more manageable in mathematical\n",
    "#       optimization than handling the scores directly.\n",
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(f\"log_probas: {log_probas}\")\n",
    "\n",
    "# Compute the average log probability of the target tokens.\n",
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(f\"avg_log_probas: {avg_log_probas}\")\n",
    "\n",
    "# The goal is to get the average log probability as close to 0 as possible by updating the model’s\n",
    "# weights as part of the training process. However, in deep learning, the common practice isn’t to\n",
    "# push the average log probability up to 0 but rather to bring the negative average log probability\n",
    "# down to 0. The negative average log probability is simply the average log probability multiplied\n",
    "# by –1.\n",
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(f\"neg_avg_log_probas: {neg_avg_log_probas}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n",
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n",
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "# As we can see, the logits tensor has three dimensions: batch size, number of tokens, and\n",
    "# vocabulary size. The targets tensor has two dimensions: batch size and number of tokens.\n",
    "# For the cross_entropy loss function in PyTorch, we want to flatten these tensors by combining\n",
    "# them over the batch dimension:\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "print(\"Targets shape:\", targets.shape)\n",
    "\n",
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)\n",
    "\n",
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The difference between cross-entropy, perplexity, and KL-divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-entropy\n",
    "\n",
    "Cross-entropy measures how well a predicted probability distribution $q$ matches a true distribution $p$. It’s defined as:\n",
    "\n",
    "$$\n",
    "H(p, q) = -\\sum_{x} p(x) \\log q(x)\n",
    "$$\n",
    "\n",
    "where $x$ runs over all possible events. Intuitively, it’s the average number of bits needed to encode samples from $p$, if they’re encoded according to $q$. The lower the cross-entropy, the closer $q$ is to $p$. \n",
    "\n",
    "[According to Wikipedia](https://en.wikipedia.org/wiki/Cross-entropy), in information theory, the cross-entropy between two probability distributions ${\\displaystyle p}$ and ${\\displaystyle q}$, over the same underlying set of events, measures the average number of bits needed to identify an event drawn from the set when the coding scheme used for the set is optimized for an estimated probability distribution ${\\displaystyle q}$, rather than the true distribution \n",
    "${\\displaystyle p}$.\n",
    "\n",
    "This statement reflects a fundamental idea from information theory: cross-entropy measures the cost of encoding data from one distribution $p$ under the assumptions of another distribution $q$. The unit “bits” arises because we’re working in the context of binary information encoding. Intuitively, each bit represents a yes/no choice, and the cross-entropy tells us, on average, how many such choices we’d need to make to encode the true outcomes from $p$, given that our model assigns probabilities according to $q$.\n",
    "\n",
    "- If $q$ perfectly matches $p$, the encoding is as efficient as possible—this is essentially the entropy $H(p)$ of the true distribution.  \n",
    "- If $q$ differs from $p$, the encoder based on $q$ will make less informed decisions, leading to longer or more error-prone codes on average.  \n",
    "- The “lower” cross-entropy means we’re closer to the ideal scenario where $q \\approx p$, which indicates our model (represented by $q$) is doing a better job of approximating the true distribution $p$.  \n",
    "- Conversely, a higher cross-entropy indicates that $q$ diverges significantly from $p$, causing inefficiencies and increasing the average number of bits needed.\n",
    "\n",
    "So, the cross-entropy not only quantifies the difference between two distributions, but also translates that difference into the practical costs of encoding data.\n",
    "\n",
    "**Example**:  \n",
    "- True distribution: $p = [0.7, 0.2, 0.1]$\n",
    "- Predicted distribution 1: $q_1 = [0.6, 0.3, 0.1]$\n",
    "- Predicted distribution 2: $q_2 = [0.9, 0.05, 0.05]$\n",
    "  - $H(p, q_1)$ will be lower than $H(p, q_2)$, because $q_1$ is closer to $p$ than $q_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity\n",
    "\n",
    "Perplexity is often used in language modeling and other probabilistic models to measure how well a model predicts a sample. It’s defined as the exponentiated average negative log-probability:\n",
    "\n",
    "$$\n",
    "\\text{Perplexity}(p, q) = 2^{H(p, q)}\n",
    "$$\n",
    "\n",
    "This represents the effective number of choices the model assigns to each outcome. A lower perplexity means the model is more confident in its predictions. Perplexity is often viewed as a normalized measure of cross-entropy, expressed in terms of the equivalent branching factor. For instance, if a language model’s perplexity is 10, it implies the model is, on average, as uncertain as making a single choice out of 10 equally likely outcomes.\n",
    "\n",
    "[According to Wikipedia](https://en.wikipedia.org/wiki/Perplexity), in information theory, perplexity is a measure of uncertainty in the value of a sample from a discrete probability distribution. The larger the perplexity, the less likely it is that an observer can guess the value which will be drawn from the distribution.\n",
    "\n",
    "[From Sebastian Raschka's book:](https://www.manning.com/books/build-a-large-language-model-from-scratch)\n",
    "\n",
    "Perplexity is a measure often used alongside cross entropy loss to evaluate the performance of models in tasks like language modeling. It can provide a more interpretable way to understand the uncertainty of a model in predicting the next token in a sequence.\n",
    "\n",
    "Perplexity measures how well the probability distribution predicted by the model matches the actual distribution of the words in the dataset. Similar to the loss, a lower perplexity indicates that the model predictions are closer to the actual distribution. Perplexity can be calculated as ```perplexity = torch.exp(loss)```, which returns ```tensor(48725.8203)``` when applied to the previously calculated loss.\n",
    "\n",
    "Perplexity is often considered more interpretable than the raw loss value because it signifies the effective vocabulary size about which the model is uncertain at each step. In the given example, this would translate to the model being unsure about which among 48,725 tokens in the vocabulary to generate as the next token.\n",
    "\n",
    "[ChatGPT](https://chatgpt.com/c/67f355f7-0c14-800f-84ad-1fa039a6025d) provides a similar intuitive explanation. If we consider a language model predicting the next word in a sentence, perplexity provides a numerical summary of how uncertain or \"perplexed\" the model is, on average, when choosing among possible outcomes. A perplexity value of 10, for example, indicates that the model’s uncertainty is equivalent to having 10 equally likely choices for each word it predicts. In other words, lower perplexity means the model is more confident in its predictions, as it can narrow down the possible outcomes to a smaller, more focused set. Higher perplexity indicates greater uncertainty or poorer model performance, since the model must spread its probability mass across more outcomes, essentially \"considering\" a larger range of possibilities before making a prediction.\n",
    "\n",
    "This interpretation of perplexity as a kind of \"average branching factor\" makes it particularly useful in evaluating the quality of language models. Instead of dealing with abstract bits or logarithms (as in cross-entropy), perplexity translates the model’s predictive efficiency into a form that’s more intuitive.\n",
    "\n",
    "**Example**:  \n",
    "- Suppose a language model predicts a sentence like “The cat sat on the ____” with probabilities for possible words:  \n",
    "  - $p(\\text{mat})$ = 0.8, $p(\\text{floor})$ = 0.15, $p(\\text{roof})$ = 0.05\n",
    "  - If the true word is “mat” and the model’s probabilities closely match this, the perplexity will be low.  \n",
    "  - If the model assigns much lower probability to “mat” and higher to other options, the perplexity will increase, indicating worse predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KL Divergence (Kullback-Leibler Divergence)  \n",
    "\n",
    "KL divergence measures how one probability distribution \\( q \\) diverges from a reference distribution \\( p \\). It’s given by:\n",
    "\n",
    "$$\n",
    "D_{KL}(p \\parallel q) = \\sum_{x} p(x) \\log\\frac{p(x)}{q(x)}\n",
    "$$\n",
    "\n",
    "KL divergence is always non-negative and equals zero only when \\( p = q \\). Unlike cross-entropy, it explicitly quantifies the “distance” (in an information-theoretic sense) between the two distributions. While cross-entropy tells us how many bits are needed to encode \\( p \\) using \\( q \\), KL divergence tells us how many extra bits are needed compared to using the true distribution \\( p \\) itself.\n",
    "\n",
    "**Example**:  \n",
    "- True distribution: p = [0.5, 0.5]\n",
    "- Predicted distribution 1: $q_1$ = [0.6, 0.4]\n",
    "- Predicted distribution 2: $q_2$ = [0.9, 0.1]\n",
    "  - $D_{KL}(p \\parallel q_1)$ is smaller than $D_{KL}(p \\parallel q_2)$, because $q_1$ is closer to $p$.  \n",
    "  - If $q_1$ becomes equal to $p$, the KL divergence will be zero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the Concepts\n",
    "\n",
    "1. **Cross-Entropy vs. KL Divergence**:  \n",
    "   - Cross-entropy combines the entropy of $p$, which is fixed for a given $p$, and the KL divergence from $p$ to $q$:  \n",
    "     $$\n",
    "     H(p, q) = H(p) + D_{KL}(p \\parallel q)\n",
    "     $$\n",
    "   - While cross-entropy measures the total coding cost under $q$, KL divergence isolates the inefficiency due to $q$’s divergence from $p$.\n",
    "\n",
    "2. **Perplexity and Cross-Entropy**:  \n",
    "   - Perplexity is derived directly from cross-entropy, converting the measure into an interpretable “average number of choices.” It essentially provides a more human-readable version of the model’s performance.  \n",
    "   - Both low perplexity and low cross-entropy indicate a better model fit, but perplexity is the exponential form and gives a more intuitive sense of the model’s uncertainty.\n",
    "\n",
    "3. **Perplexity and KL Divergence**:  \n",
    "   - While perplexity is connected to cross-entropy, KL divergence is a more nuanced measure that focuses on how much $q$ deviates from $p$ rather than the raw efficiency of encoding.  \n",
    "   - Perplexity doesn’t directly measure divergence; instead, it measures how well the model predicts, which can be related to divergence indirectly through the cross-entropy.\n",
    "\n",
    "In summary, cross-entropy and perplexity are practical metrics for evaluating how well a predictive model matches a true distribution, with perplexity offering a more intuitive interpretation. KL divergence, on the other hand, is a more fundamental information-theoretic measure that quantifies how much one distribution differs from another, forming a building block for understanding the inefficiencies captured by cross-entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and validation set losses\n",
    "\n",
    "When preparing the data loaders, we split the input text into training and validation set portions. Then we tokenize the text (only shown for the training set portion for simplicity) and divide the tokenized text into chunks of a user-specified length (here, 6). Finally, we shuffle the rows and organize the chunked text into batches (here, batch size 2), which we can use for model training.\n",
    "\n",
    "![Data splits](https://drek4537l1klr.cloudfront.net/raschka/Figures/5-9.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n",
      "Train data (chars): 18431\n",
      "Validation data (chars): 2048\n"
     ]
    }
   ],
   "source": [
    "# Load example dataset.\n",
    "file_path = \"data/the_verdict.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text_data = file.read()\n",
    "\n",
    "# Pritn statistics.\n",
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)\n",
    "\n",
    "# Divide the dataset into training and validation sets.\n",
    "# NOTE: This is a simple and naive approach to splitting the dataset and should be replaced with\n",
    "#       tooling from pytorch (e.g. https://pytorch.org/docs/stable/data.html#torch.utils.data.random_split)\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "print(\"Train data (chars):\", len(train_data))\n",
    "print(\"Validation data (chars):\", len(val_data))\n",
    "\n",
    "# Create the dataloaders.\n",
    "torch.manual_seed(123)\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M.context_length,\n",
    "    stride=GPT_CONFIG_124M.context_length,\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M.context_length,\n",
    "    stride=GPT_CONFIG_124M.context_length,\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avpc-off-vehicle-qa-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
