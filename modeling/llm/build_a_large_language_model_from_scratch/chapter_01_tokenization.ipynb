{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization (chapter 1)\n",
    "\n",
    "## What is Tokenization?\n",
    "\n",
    "Tokenization is a fundamental process in Natural Language Processing (NLP) that involves breaking text into smaller units called \"tokens.\" These tokens serve as the basic building blocks that machine learning models can process.\n",
    "\n",
    "For Large Language Models (LLMs), tokenization is a critical first step that converts human-readable text into numerical formats the model can understand. When you send a prompt to an LLM such as GPT or Claude, the model doesn't directly read your text - it processes sequences of tokens that represent your text.\n",
    "\n",
    "There are several approaches to tokenization:\n",
    "\n",
    "- **Word tokenization**: Splitting text by words (separated by spaces or punctuation)\n",
    "- **Subword tokenization**: Breaking words into meaningful subunits (most common in modern LLMs)\n",
    "- **Character tokenization**: Dividing text into individual characters\n",
    "\n",
    "Tokenization presents various challenges, including handling punctuation, contractions, compound words, and rare words. The choice of tokenization method significantly impacts an LLM's performance, vocabulary size, and ability to handle different languages.\n",
    "\n",
    "This notebook explores tokenization techniques based on Sebastian Raschka's book (Chapter 2), implementing various tokenization approaches and analyzing their effects.\n",
    "\n",
    "## Tokenization Process\n",
    "Borrowed from [Manning's Live Books](https://livebook.manning.com/wiki/categories/llm/token)\n",
    "\n",
    "![Tokenization Process](https://drek4537l1klr.cloudfront.net/raschka/Figures/2-6.png)\n",
    "\n",
    "## Acknowledgment\n",
    "\n",
    "All concepts, architectures, and implementation approaches are credited to Sebastian Raschka's work. This repository serves as my personal implementation and notes while working through the book's content.\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Sebastian Raschka's GitHub](https://github.com/rasbt)\n",
    "- [Book Information](https://www.manning.com/books/build-a-large-language-model-from-scratch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies.\n",
    "%pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Dict, List\n",
    "import urllib.request\n",
    "\n",
    "from importlib.metadata import version\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.7.0\n"
     ]
    }
   ],
   "source": [
    "# Verify library versions.\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))  # expected: 0.7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download sample data to a text file.\n",
    "local_filename = \"data/the_verdict.txt\"\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "    \"the-verdict.txt\"\n",
    ")\n",
    "urllib.request.urlretrieve(url, local_filename)\n",
    "\n",
    "# Read the text file into a string.\n",
    "with open(local_filename, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "print(\"Total number of character:\", len(raw_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare text for tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split on whitespace only.\n",
    "text = raw_text[:150]\n",
    "print(text)\n",
    "print(re.split(r\"(\\s)\", text))\n",
    "\n",
    "# Split on whitespace and punctuation.\n",
    "# NOTE: This regular expression combines two or-conditions:\n",
    "# 1. A comma or period (e.g., \"hello,\" or \"world.\")\n",
    "# 2. A whitespace character (e.g., \"hello world\")\n",
    "# 3. A dash (e.g., \"hello-world\")\n",
    "result = re.split(r\"([,.-]|\\s)\", text)\n",
    "print(result)\n",
    "\n",
    "# Handle a wider range of punctuation.\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "print(result)\n",
    "\n",
    "# Remove whitespace from the result.\n",
    "result = [item for item in result if item.strip()]\n",
    "print(result)\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Split on whitespace and punctuation.\n",
    "    result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "    return [item for item in result if item.strip()]\n",
    "\n",
    "\n",
    "# Preprocess the text.\n",
    "preprocessed_text = preprocess_text(raw_text)\n",
    "print(f\"Raw text: {len(raw_text)} characters\")\n",
    "print(f\"Preprocessed text: {len(preprocessed_text)} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a vocabulary of unique tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the set of unique tokens and sort alphabetically.\n",
    "all_words = sorted(set(preprocessed_text))\n",
    "vocab_size = len(all_words)\n",
    "print(f\"Vocabulary size: {vocab_size} unique tokens\")\n",
    "\n",
    "# Create a lookup table that maps tokens to integers.\n",
    "token_to_id = {token: integer for integer, token in enumerate(all_words)}\n",
    "id_to_token = {integer: token for token, integer in token_to_id.items()}\n",
    "for i, item in enumerate(token_to_id.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V1: A simple tokenizer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab: Dict[str, int]):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {integer: token for token, integer in vocab.items()}\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess(text: str) -> List[str]:\n",
    "        \"\"\"Split on whitespace and punctuation.\"\"\"\n",
    "        result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        return [item for item in result if item.strip()]\n",
    "\n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        \"\"\"Preprocess the text and convert tokens to integers.\"\"\"\n",
    "        tokens = SimpleTokenizerV1.preprocess(text)\n",
    "        return [self.str_to_int[token] for token in tokens]\n",
    "\n",
    "    def decode(self, ids: List[int]) -> str:\n",
    "        \"\"\"Convert integers back to tokens.\"\"\"\n",
    "        # Convert integers back to tokens and concatenate them.\n",
    "        text = \" \".join([self.int_to_str[id] for id in ids])\n",
    "\n",
    "        # Removes spaces before the specified punctuation (i.e. before commas, periods, etc.).\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r\"\\1\", text)\n",
    "        return text\n",
    "\n",
    "\n",
    "# Test the tokenizer.\n",
    "tokenizer = SimpleTokenizerV1(vocab=token_to_id)\n",
    "text = (\n",
    "    \"\"\"\"It's the last he painted, you know, Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    ")\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)\n",
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V2: A more advanced tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new special tokens:\n",
    "# - <|endoftext|>\n",
    "# - <|unk|>\n",
    "all_tokens = sorted(list(set(preprocessed_text)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "vocab = {token: integer for integer, token in enumerate(all_tokens)}\n",
    "print(len(vocab.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab: Dict[str, int]):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {integer: token for token, integer in vocab.items()}\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess(text: str) -> List[str]:\n",
    "        \"\"\"Split on whitespace and punctuation.\"\"\"\n",
    "        result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        return [item for item in result if item.strip()]\n",
    "\n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        \"\"\"Preprocess the text and convert tokens to integers.\"\"\"\n",
    "        # Preprocess the text.\n",
    "        tokens = SimpleTokenizerV1.preprocess(text)\n",
    "\n",
    "        # Handle unknown tokens.\n",
    "        tokens = [token if token in self.str_to_int else \"<|unk|>\" for token in tokens]\n",
    "\n",
    "        # Convert tokens to integers (i.e. token IDs).\n",
    "        return [self.str_to_int[token] for token in tokens]\n",
    "\n",
    "    def decode(self, ids: List[int]) -> str:\n",
    "        \"\"\"Convert integers back to tokens.\"\"\"\n",
    "        # Convert integers back to tokens and concatenate them.\n",
    "        text = \" \".join([self.int_to_str[id] for id in ids])\n",
    "\n",
    "        # Removes spaces before the specified punctuation (i.e. before commas, periods, etc.).\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r\"\\1\", text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the new tokenizer.\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print(text)\n",
    "\n",
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "print(tokenizer.encode(text))\n",
    "print(tokenizer.decode(tokenizer.encode(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V3: Using tiktoken\n",
    "\n",
    "Notes:\n",
    "1. The endoftext token has a fairly large ID (50256) given the large vocabulary size for GPT-2.\n",
    "2. The BPE tokenizer correctly encodes/decodes unknown words.\n",
    "\n",
    "The Byte Pair Encoding (BPE) algorithm is explained in detail [here](https://vizuara.substack.com/p/understanding-byte-pair-encoding)\n",
    "\n",
    "## Algorithm explained via a simple example\n",
    "\n",
    "![BPE algorithm explained via a simple example](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcd437de5-c5fb-4626-8980-2dbd2ce9684f_1122x390.png)\n",
    "\n",
    "## Example of BPE tokenization for unknown words\n",
    "\n",
    "![BPE tokenization for unknown words](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0196ea0d-95a4-4ad2-bf9f-15262a630fd8_1654x1036.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n",
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n",
      "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "    \"of someunknownPlace.\"\n",
    ")\n",
    "print(text)\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)\n",
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Akwirw ier'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(\"Akwirw ier\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avpc-off-vehicle-qa-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
