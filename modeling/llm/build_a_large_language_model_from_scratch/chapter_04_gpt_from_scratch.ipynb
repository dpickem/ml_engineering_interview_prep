{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT from scratch (chapter 4)\n",
    "\n",
    "This notebook explores full LLM architecture for GPT based on Sebastian Raschka's book (Chapter 4),\n",
    "implementing normalization layers, shortcut connections, and transformer blocks. This notebook also\n",
    "shows how to compute the parameter count as well as the storage requirements of GPT-like models.\n",
    "\n",
    "## Acknowledgment\n",
    "\n",
    "All concepts, architectures, and implementation approaches are credited to Sebastian Raschka's work.\n",
    "This repository serves as my personal implementation and notes while working through the book's content.\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Sebastian Raschka's GitHub](https://github.com/rasbt)\n",
    "- [Book Information](https://www.manning.com/books/build-a-large-language-model-from-scratch)\n",
    "    - [Chapter 4](https://livebook.manning.com/book/build-a-large-language-model-from-scratch/chapter-4)\n",
    "- [GPT-2 original paper: \"Language Models Are Unsupervised Multitask Learners\"](https://mng.bz/yoBq)\n",
    "\n",
    "![GPT architecture](https://drek4537l1klr.cloudfront.net/raschka/Figures/4-2.png)\n",
    "\n",
    "The components of a GPT-like architecture are shown in the image below.\n",
    "\n",
    "![GPT components](https://drek4537l1klr.cloudfront.net/raschka/Figures/4-3.png)\n",
    "\n",
    "The following figure shows the flow of data through the model and how they are transformed at each\n",
    "stage.\n",
    "\n",
    "![GPT data flow](https://drek4537l1klr.cloudfront.net/raschka/Figures/4-4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass(frozen=True)\n",
    "class GPTConfig:\n",
    "    \"\"\"Configuration for the GPT model.\n",
    "\n",
    "    Attributes:\n",
    "        vocab_size: The size of the vocabulary.\n",
    "        context_length: The maximum number of tokens that the model can process in a single forward\n",
    "            pass (i.e. the maximum sequence length). Also denotes the number of input tokens the\n",
    "            model can handle via the positional embeddings (from chapter 2).\n",
    "        emb_dim: The dimension of the token embeddings.\n",
    "        n_heads: The number of attention heads.\n",
    "        n_layers: The number of transformer layers.\n",
    "        dropout_rate: The dropout rate for the transformer layers.\n",
    "        qkv_bias: Whether to use bias in the QKV projections.\n",
    "    \"\"\"\n",
    "\n",
    "    vocab_size: int\n",
    "    context_length: int\n",
    "    emb_dim: int\n",
    "    n_heads: int\n",
    "    n_layers: int\n",
    "    dropout_rate: float\n",
    "    qkv_bias: bool\n",
    "\n",
    "\n",
    "# Instantiate the GPT-2 configuration.\n",
    "GPT_CONFIG_124M = GPTConfig(\n",
    "    vocab_size=50257,  # as used by the BPE tokenizer for GPT-2.\n",
    "    context_length=1024,\n",
    "    emb_dim=768,\n",
    "    n_heads=12,\n",
    "    n_layers=12,\n",
    "    dropout_rate=0.1,\n",
    "    qkv_bias=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Placeholder architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[-0.5640, -0.8061,  0.5556,  ..., -0.0121, -0.1054, -0.6956],\n",
      "         [ 0.7765, -1.0823,  1.1478,  ...,  0.3540,  0.9335, -0.1086],\n",
      "         [-1.2292,  1.0940, -1.0646,  ..., -1.6435, -0.0090,  0.0854],\n",
      "         [-1.6669, -1.4979,  0.6711,  ...,  0.4817, -0.1961, -0.8135]],\n",
      "\n",
      "        [[-0.9549, -0.7139,  0.3969,  ...,  0.0884, -0.2823, -0.7652],\n",
      "         [ 1.2106,  0.4950,  1.3336,  ...,  0.4385, -1.0175, -0.2729],\n",
      "         [ 1.2351,  0.0197,  0.5197,  ..., -1.0924,  1.2276, -0.4298],\n",
      "         [-0.6795, -1.1446,  1.1755,  ...,  0.7063, -0.5953, -1.7645]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim: int):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg: GPTConfig):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg.vocab_size, cfg.emb_dim)\n",
    "        self.pos_emb = nn.Embedding(cfg.context_length, cfg.emb_dim)\n",
    "        self.drop_emb = nn.Dropout(cfg.dropout_rate)\n",
    "        self.transformer_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg.n_layers)]\n",
    "        )\n",
    "        self.final_norm = DummyLayerNorm(cfg.emb_dim)\n",
    "        self.out_head = nn.Linear(cfg.emb_dim, cfg.vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.transformer_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# Instantiate the placeholder model.\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "# Feed example data through it.\n",
    "torch.manual_seed(123)\n",
    "logits = model(batch)\n",
    "print(\"Output shape:\", logits.shape)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer normalization\n",
    "\n",
    "Training deep neural networks with many layers can sometimes prove challenging due to problems like vanishing or exploding gradients. These problems lead to unsta- ble training dynamics and make it difficult for the network to effectively adjust its weights, which means the learning process struggles to find a set of parameters (weights) for the neural network that minimizes the loss function.\n",
    "\n",
    "The main idea behind layer normalization is to adjust the activations (outputs) of a neural network layer to have a mean of 0 and a variance of 1, also known as unit variance. This adjustment speeds up the convergence to effective weights and ensures consistent, reliable training.\n",
    "\n",
    "![Layer normalization](https://drek4537l1klr.cloudfront.net/raschka/Figures/4-5.png)\n",
    "\n",
    "Example of aggregating across different dimensions of a tensor illustrating the use of the *dim* argument of aggregation functions like *mean* or *std*.\n",
    "\n",
    "![Illustration of dim argument](https://drek4537l1klr.cloudfront.net/raschka/Figures/4-6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Raw layer outputs:\n",
      "------------------\n",
      "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
      "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "Mean:\n",
      " tensor([[0.1324],\n",
      "        [0.2170]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[0.0231],\n",
      "        [0.0398]], grad_fn=<VarBackward0>)\n",
      "\n",
      "Normalized layer outputs:\n",
      "-------------------------\n",
      "tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
      "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "\n",
      "Mean:\n",
      "tensor([[    0.0000],\n",
      "        [    0.0000]], grad_fn=<MeanBackward1>)\n",
      "\n",
      "Variance:\n",
      "tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Example of layer normalization.\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# Create two training examples with 5 features each.\n",
    "batch_example = torch.randn(2, 5)\n",
    "\n",
    "# Create a basic neural network layer consisting of a Linear layer followed by a non-linear\n",
    "# activation function, ReLU (short for rectified linear unit).\n",
    "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
    "out = layer(batch_example)\n",
    "\n",
    "# Compute mean and variance along the feature dimension (i.e. the last dimension).\n",
    "# NOTE: Using keepdim=True in operations like mean or variance calculation ensures that the output\n",
    "#       tensor retains the same number of dimensions as the input tensor, even though the operation\n",
    "#       reduces the tensor along the dimension specified via dim. Here, without keepdim=True, the\n",
    "#       output tensor would be a two-dimensional vector (e.g. [1, 2]) rather than a 2x1-dimensional\n",
    "#       matrix (e.g. [[1], [2]]).\n",
    "mean = out.mean(dim=-1, keepdim=True)\n",
    "var = out.var(dim=-1, keepdim=True)\n",
    "print(f\"\\nRaw layer outputs:\\n{'-' * 18}\\n{out}\")\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)\n",
    "\n",
    "# Apply layer normalization.\n",
    "out_norm = (out - mean) / torch.sqrt(var)\n",
    "mean = out_norm.mean(dim=-1, keepdim=True)\n",
    "var = out_norm.var(dim=-1, keepdim=True)\n",
    "print(f\"\\nNormalized layer outputs:\\n{'-' * 25}\\n{out_norm}\")\n",
    "print(f\"\\nMean:\\n{mean}\")\n",
    "print(f\"\\nVariance:\\n{var}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A layer normalization class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[    -0.0000],\n",
      "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"Layer normalization (https://arxiv.org/abs/1607.06450).\n",
    "\n",
    "    This specific implementation of layer normalization operates on the last dimension of\n",
    "    the input tensor x, which represents the embedding dimension (emb_dim).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, emb_dim: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # Use a small constant which will be added to the variance to prevent division by zero.\n",
    "        self.eps = 1e-5\n",
    "\n",
    "        # The scale and shift are two trainable parameters (of the same dimension as the input)\n",
    "        # that the LLM automatically adjusts during training.\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "\n",
    "\n",
    "# Test the layer normalization class.\n",
    "ln = LayerNorm(emb_dim=5)\n",
    "out_ln = ln(batch_example)\n",
    "mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "var = out_ln.var(dim=-1, unbiased=False, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GELU activation function\n",
    "\n",
    "This section is skipped in this notebook since torch already implements to full version of GELU as well as the curve fitting approximation.\n",
    "\n",
    "See [GELU activation function](https://pytorch.org/docs/stable/generated/torch.nn.GELU.html)\n",
    "\n",
    "The below figure shows a comparison of GELU and ReLU.\n",
    "\n",
    "![Feed forward network](https://drek4537l1klr.cloudfront.net/raschka/Figures/4-8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed Forward NN module\n",
    "\n",
    "An overview of the connections between the layers of the feed forward neural network. This neural network can accommodate variable batch sizes and numbers of tokens in the input. However, the embedding size for each token is determined and fixed when initializing the weights.\n",
    "\n",
    "Note how the size of the embedding dimension is increased by 4x before it is shrunk down to the input embedding dimension.\n",
    "\n",
    "![Feed forward network](https://drek4537l1klr.cloudfront.net/raschka/Figures/4-9.png)\n",
    "\n",
    "The FeedForward module plays a crucial role in enhancing the model’s ability to learn from and generalize the data. Although the input and output dimensions of this module are the same, it internally expands the embedding dimension into a higher-dimensional space through the first linear layer (as shown below). This expansion is followed by a nonlinear GELU activation and then a contraction back to the original dimension with the second linear transformation. Such a design allows for the exploration of a richer representation space.\n",
    "\n",
    "![Feed forward network](https://drek4537l1klr.cloudfront.net/raschka/Figures/4-10.png)\n",
    "\n",
    "Moreover, the uniformity in input and output dimensions simplifies the architecture by enabling the stacking of multiple layers, as we will do later, without the need to adjust dimensions between them, thus making the model more scalable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Feed Forward Neural Network (FFNN) module.\n",
    "\n",
    "    This module implements a feed-forward neural network with two linear layers and a GELU\n",
    "    activation function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg.emb_dim, 4 * cfg.emb_dim),\n",
    "            torch.nn.GELU(approximate=\"tanh\"),\n",
    "            nn.Linear(4 * cfg.emb_dim, cfg.emb_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "# Test the feed forward neural network.\n",
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "x = torch.rand(2, 3, 768)\n",
    "out = ffn(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip connections\n",
    "\n",
    "Shortcut connections are also known as skip or residual connections. Originally, shortcut connections were proposed for deep networks in computer vision (specifically, in residual networks) to mitigate the challenge of vanishing gradients. The vanishing gradient problem refers to the issue where gradients (which guide weight updates during training) become progressively smaller as they propagate backward through the layers, making it difficult to effectively train earlier layers.\n",
    "\n",
    "They play a crucial role in preserving the flow of gradients during the backward pass in training.\n",
    "\n",
    "![Skip connections](https://drek4537l1klr.cloudfront.net/raschka/Figures/4-12.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Model without shortcut connections:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "layers.0.0.weight has gradient mean of 0.0002017359365709126\n",
      "layers.1.0.weight has gradient mean of 0.00012011162471026182\n",
      "layers.2.0.weight has gradient mean of 0.0007152041071094573\n",
      "layers.3.0.weight has gradient mean of 0.0013988733990117908\n",
      "layers.4.0.weight has gradient mean of 0.005049645435065031\n",
      "----------------------------------------------------------------------------------------------------\n",
      "layers.0.0.weight has gradient mean of 0.22169798612594604\n",
      "layers.1.0.weight has gradient mean of 0.20694109797477722\n",
      "layers.2.0.weight has gradient mean of 0.3289699852466583\n",
      "layers.3.0.weight has gradient mean of 0.26657330989837646\n",
      "layers.4.0.weight has gradient mean of 1.3258543014526367\n",
      "Model with shortcut connections:\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "\n",
    "        # Implement a deep neural network with 5 layers of linear transformations and GELU\n",
    "        # activations.\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(layer_sizes[0], layer_sizes[1]),\n",
    "                    torch.nn.GELU(approximate=\"tanh\"),\n",
    "                ),\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(layer_sizes[1], layer_sizes[2]),\n",
    "                    torch.nn.GELU(approximate=\"tanh\"),\n",
    "                ),\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(layer_sizes[2], layer_sizes[3]),\n",
    "                    torch.nn.GELU(approximate=\"tanh\"),\n",
    "                ),\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(layer_sizes[3], layer_sizes[4]),\n",
    "                    torch.nn.GELU(approximate=\"tanh\"),\n",
    "                ),\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(layer_sizes[4], layer_sizes[5]),\n",
    "                    torch.nn.GELU(approximate=\"tanh\"),\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Compute the output of the current layer.\n",
    "        for layer in self.layers:\n",
    "            layer_output = layer(x)\n",
    "\n",
    "            # Check if shortcut can be applied.\n",
    "            if self.use_shortcut and x.shape == layer_output.shape:\n",
    "                x = x + layer_output\n",
    "            else:\n",
    "                x = layer_output\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Implement a function for printing gradients.\n",
    "def print_gradients(model, x):\n",
    "    \"\"\"Print gradients of the model.\"\"\"\n",
    "    # Forward pass.\n",
    "    output = model(x)\n",
    "    target = torch.tensor([[0.0]])\n",
    "\n",
    "    # Calculates loss based on how close the target and output are\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)\n",
    "\n",
    "    # Backward pass to compute gradients.\n",
    "    loss.backward()\n",
    "\n",
    "    # Visualize the gradients of the model.\n",
    "    # NOTE: This loop iterates over all of the model's parameters and prints the mean of the\n",
    "    #       absolute mean values of the gradients for each parameter.\n",
    "    # NOTE: Suppose we have a 3 × 3 weight parameter matrix for a given layer. In that case,\n",
    "    #       this layer will have 3 × 3 gradient values, and we print the mean absolute gradient\n",
    "    #       of these 3 × 3 gradient values to obtain a single gradient value per layer to compare\n",
    "    #       the gradients between layers more easily.\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"weight\" in name:\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")\n",
    "\n",
    "\n",
    "# Test the network with and without shortcut connections.\n",
    "\n",
    "# Specifies random seed for the initial weights for reproducibility\n",
    "torch.manual_seed(123)\n",
    "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
    "sample_input = torch.tensor([[1.0, 0.0, -1.0]])\n",
    "\n",
    "# Instantiate the model without shortcut connections.\n",
    "# NOTE: The gradients in earlier layers become too small to be useful for training, which is due\n",
    "#       to the vanishing gradient problem.\n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(layer_sizes, use_shortcut=False)\n",
    "print(\"-\" * 100)\n",
    "print(f\"Model without shortcut connections:\\n{'-' * 100}\")\n",
    "print_gradients(model_without_shortcut, sample_input)\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# Instantiate the model with shortcut connections.\n",
    "# NOTE: The gradients in earlier layers are now more stable and useful for training, which is due\n",
    "#       to the preservation of the flow of gradients during the backward pass.\n",
    "torch.manual_seed(123)\n",
    "model_with_shortcut = ExampleDeepNeuralNetwork(layer_sizes, use_shortcut=True)\n",
    "print_gradients(model_with_shortcut, sample_input)\n",
    "print(f\"Model with shortcut connections:\\n{'-' * 100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer block\n",
    "\n",
    "The operations within the transformer block, including multi-head attention and feed forward layers, are designed to transform these vectors in a way that preserves their dimensionality. The idea is that the self-attention mechanism in the multi-head attention block identifies and analyzes relationships between elements in the input sequence. In contrast, the feed forward network modifies the data individually at each position. This combination not only enables a more nuanced understanding and processing of the input but also enhances the model’s overall capacity for handling complex data patterns.\n",
    "\n",
    "![Transformer block](https://drek4537l1klr.cloudfront.net/raschka/Figures/4-13.png)\n",
    "\n",
    "As we can see, the transformer block maintains the input dimensions in its output, indicating that the transformer architecture processes sequences of data without altering their shape throughout the network.\n",
    "\n",
    "The preservation of shape throughout the transformer block architecture is not incidental but a crucial aspect of its design. This design enables its effective application across a wide range of sequence-to-sequence tasks, where each output vector directly corresponds to an input vector, maintaining a one-to-one relationship. However, the output is a context vector that encapsulates information from the entire input sequence. This means that while the physical dimensions of the sequence (length and feature size) remain unchanged as it passes through the transformer block, the content of each output vector is re-encoded to integrate contextual information from across the entire input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This is the MultiHeadAttention class from chapter 3 of the book\n",
    "#       (see 03_attention_mechanisms.ipynb).\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_in: int,\n",
    "        d_out: int,\n",
    "        context_length: int,\n",
    "        dropout: float,\n",
    "        num_heads: int,\n",
    "        qkv_bias: bool = False,\n",
    "    ):\n",
    "        \"\"\"Initialize the multi-head attention class.\n",
    "\n",
    "        Args:\n",
    "            d_in: The dimension of the input.\n",
    "            d_out: The dimension of the output.\n",
    "            context_length: The length of the context. This argument sets the length of the causal\n",
    "                mask, i.e. the maximum supported sequence length.\n",
    "            dropout: The dropout probability.\n",
    "            num_heads: The number of attention heads.\n",
    "            qkv_bias: Whether to use a bias in the query, key, and value projections.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Verify that the output dimension is divisible by the number of heads, which is required\n",
    "        # for splitting the output into the specified number of heads.\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        # Cache the output dimension and the number of heads for later use.\n",
    "        # NOTE: This reduces the projection dim to match the desired output dim.\n",
    "        # NOTE: The key operation is to split the d_out dimension into num_heads and head_dim,\n",
    "        #       where head_dim = d_out / num_heads. This splitting is then achieved using the .view\n",
    "        #       method: a tensor of dimensions (b, num_tokens, d_out) is reshaped to dimension\n",
    "        #       (b, num_tokens, num_heads, head_dim).\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        # Initialize the weight matrices.\n",
    "        # NOTE: Only a single weight matrix is initialized for the query, key, and value projections\n",
    "        #       since we will split the output into the specified number of heads.\n",
    "        self.W_q = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_k = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_v = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "        # Initialize the output projection layer.\n",
    "        # NOTE: This implementation uses a Linear layer to combine all head outputs.\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "\n",
    "        # Initialize the dropout layer.\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Register a buffer for the mask.\n",
    "        self.register_buffer(\n",
    "            \"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Extract input dimensions.\n",
    "        # NOTE: The customary notation for these dimensions is [B, T, D] where:\n",
    "        #\n",
    "        #   B = batch size\n",
    "        #   T = sequence length (num_tokens)\n",
    "        #   D = model dimension (d_in)\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        # Project the input into query, key, and value vectors.\n",
    "        # NOTE: The shape of the projected query, key, and value vectors is [B, T, D]\n",
    "        keys = self.W_k(x)  # [B, T, D]\n",
    "        queries = self.W_q(x)  # [B, T, D]\n",
    "        values = self.W_v(x)  # [B, T, D]\n",
    "\n",
    "        # Split the query, key, and value vectors into the specified number of heads.\n",
    "        # NOTE: We implicitly split the matrix by adding a num_heads dimension. Then we unroll the\n",
    "        #       last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim).\n",
    "        # Compute head dimension\n",
    "        head_dim = self.d_out // self.num_heads\n",
    "\n",
    "        # Reshape to [B, T, H, D_h] where:\n",
    "        #\n",
    "        #   B = batch size\n",
    "        #   T = sequence length (num_tokens)\n",
    "        #   H = number of heads\n",
    "        #   D_h = dimension per head (head_dim)\n",
    "        #\n",
    "        # NOTE: We implicitly split the matrix by adding a num_heads dimension. Then we unroll the\n",
    "        #       last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim).\n",
    "        # NOTE: See section \"A note on views\" for more details.\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, head_dim)\n",
    "\n",
    "        # Transpose from [B, T, H, D_h] to [B, H, T, D_h], i.e. from shape\n",
    "        # (b, num_tokens, num_heads, head_dim) to (b, num_heads, num_tokens, head_dim)\n",
    "        #\n",
    "        # NOTE: This transposition is crucial for correctly aligning the queries, keys, and values\n",
    "        #       across the different heads and performing batched matrix multiplications\n",
    "        #       efficiently.\n",
    "        # NOTE: This reshaping results in each head having access to the full sequence of tokens\n",
    "        #       (i.e. a tensor of shape T x D_h).\n",
    "        keys = keys.transpose(1, 2)  # shape [B, H, T, D_h]\n",
    "        queries = queries.transpose(1, 2)  # shape [B, H, T, D_h]\n",
    "        values = values.transpose(1, 2)  # shape [B, H, T, D_h]\n",
    "\n",
    "        # Compute the unnormalized attention scores via a dot product for each head.\n",
    "        # NOTE: We transpose the T and D_h dimension (i.e. num_tokens and head_dim) just like we\n",
    "        #       have already done in the \"Trainable self-attention\" section.\n",
    "        # NOTE: Change key shape from [B, H, T, D_h] to [B, H, D_h, T]\n",
    "        # NOTE: The output shape of attn_scores is [B, H, T, T], i.e. a square matrix of size T\n",
    "        #       (i.e. sequence length) for each head.\n",
    "        # NOTE: The following operation does a batched matrix multiplication between queries and\n",
    "        #       keys. In this case, the matrix multiplication implementation in PyTorch handles the\n",
    "        #       four-dimensional input tensor so that the matrix multiplication is carried out\n",
    "        #       between the two last dimensions (num_tokens, head_dim) and then repeated for the\n",
    "        #       individual heads.\n",
    "        # NOTE: See section \"A note on batched matrix multiplications\" for more details.\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # shape [B, H, T, T]\n",
    "\n",
    "        # The mask is truncated to the number of tokens in the input sequence (i.e. sequence\n",
    "        # length T)\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]  # shape [T, T]\n",
    "\n",
    "        # Apply the mask to the attention scores.\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)  # shape [B, H, T, T]\n",
    "\n",
    "        # Compute the normalized attention weights (as before)\n",
    "        # NOTE: The scaling factor is the last dimension of the keys tensor (i.e. head_dim, see\n",
    "        #       line 88).\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)  # shape [B, H, T, T]\n",
    "\n",
    "        # Compute the context vectors.\n",
    "        # NOTE: The shapes of the individual tensors are:\n",
    "        #       - attn_weights: [B, H, T, T]\n",
    "        #       - values: [B, H, T, D_h]\n",
    "        #       - context_vec: [B, H, T, D_h]\n",
    "        #       - context_vec.transposed: [B, T, H, D_h]\n",
    "        # NOTE: The context vectors from all heads are transposed back to the shape\n",
    "        #       (b, num_tokens, num_heads, head_dim).\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)  # shape [B, T, H, D_h]\n",
    "\n",
    "        # Reshape/flatten the context vectors from [B, T, H, D_h] to [B, T, D_out], i.e. combine all\n",
    "        # individual attention heads (d_out = H * D_h).\n",
    "        # NOTE: Combines heads, where self.d_out = self.num_heads * self.head_dim (see line 32).\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "\n",
    "        # Optionally project the context vectors to the output dimension.\n",
    "        # NOTE: This output projection layer is not strictly necessary (see appendix B of the book\n",
    "        #       for more details), but it is commonly used in many LLM architectures.\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize the multi-head attention module.\n",
    "        self.mha = MultiHeadAttention(\n",
    "            d_in=cfg.emb_dim,\n",
    "            d_out=cfg.emb_dim,\n",
    "            context_length=cfg.context_length,\n",
    "            num_heads=cfg.n_heads,\n",
    "            dropout=cfg.dropout_rate,\n",
    "            qkv_bias=cfg.qkv_bias,\n",
    "        )\n",
    "\n",
    "        # Initialize the feed forward module.\n",
    "        self.ff = FeedForward(cfg)\n",
    "\n",
    "        # Initialize the layer normalization modules.\n",
    "        self.pre_attention_norm = LayerNorm(cfg.emb_dim)\n",
    "        self.pre_ff_norm = LayerNorm(cfg.emb_dim)\n",
    "\n",
    "        # Initialize the dropout module.\n",
    "        self.drop_shortcut = nn.Dropout(cfg.dropout_rate)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass for the transformer block.\n",
    "\n",
    "        NOTE: Layer normalization (LayerNorm) is applied before each of these two components,\n",
    "              and dropout is applied after them to regularize the model and prevent overfitting.\n",
    "              This is also known as Pre-LayerNorm. Older architectures, such as the original\n",
    "              transformer model, applied layer normalization after the self-attention and feed\n",
    "              forward networks instead, known as Post-LayerNorm, which often leads to worse\n",
    "              training dynamics.\n",
    "        \"\"\"\n",
    "        # Shortcut connection for attention block (this is the original input).\n",
    "        shortcut = x\n",
    "\n",
    "        # Layer norm + attention + dropout.\n",
    "        x = self.pre_attention_norm(x)\n",
    "        x = self.mha(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "\n",
    "        # Add the original input back.\n",
    "        x = x + shortcut\n",
    "\n",
    "        # Shortcut connection for feed forward block (this is the output of the attention block).\n",
    "        shortcut = x\n",
    "\n",
    "        # Layer norm + feed forward + dropout.\n",
    "        x = self.pre_ff_norm(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "\n",
    "        # Add the post-attention shortcut output back.\n",
    "        x = x + shortcut\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Test the transformer block.\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# Creates sample input of shape [batch_size, num_tokens, emb_dim].\n",
    "# NOTE: In more standard notation, this is [B, T, D] where:\n",
    "#\n",
    "#   B = batch size\n",
    "#   T = sequence length (num_tokens)\n",
    "#   D = model dimension (emb_dim)\n",
    "x = torch.rand(2, 4, 768)\n",
    "\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "output = block(x)\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The full GPT model\n",
    "\n",
    "An overview of the GPT model architecture showing the flow of data through the GPT model. Starting from the bottom, tokenized text is first converted into token embeddings, which are then augmented with positional embeddings. This combined information forms a tensor that is passed through a series of transformer blocks shown in the center (each containing multi-head attention and feed forward neural network layers with dropout and layer normalization), which are stacked on top of each other and repeated 12 times.\n",
    "\n",
    "![GPT model](https://drek4537l1klr.cloudfront.net/raschka/Figures/4-15.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch:\n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[ 0.1381,  0.0077, -0.1963,  ..., -0.0222, -0.1060,  0.1717],\n",
      "         [ 0.3865, -0.8408, -0.6564,  ..., -0.5163,  0.2369, -0.3357],\n",
      "         [ 0.6989, -0.1829, -0.1631,  ...,  0.1472, -0.6504, -0.0056],\n",
      "         [-0.4290,  0.1669, -0.1258,  ...,  1.1579,  0.5303, -0.5549]],\n",
      "\n",
      "        [[ 0.1094, -0.2894, -0.1467,  ..., -0.0557,  0.2911, -0.2824],\n",
      "         [ 0.0882, -0.3552, -0.3527,  ...,  1.2930,  0.0053,  0.1898],\n",
      "         [ 0.6091,  0.4702, -0.4094,  ...,  0.7688,  0.3787, -0.1974],\n",
      "         [-0.0612, -0.0737,  0.4751,  ...,  1.2463, -0.3834,  0.0609]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg: GPTConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize the token embedding layer (including positional embeddings).\n",
    "        self.tok_emb = nn.Embedding(cfg.vocab_size, cfg.emb_dim)\n",
    "        self.pos_emb = nn.Embedding(cfg.context_length, cfg.emb_dim)\n",
    "\n",
    "        # Initialize the dropout layer.\n",
    "        self.drop_emb = nn.Dropout(cfg.dropout_rate)\n",
    "\n",
    "        # Initialize the transformer blocks.\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg.n_layers)]\n",
    "        )\n",
    "\n",
    "        # Initialize the final layer normalization.\n",
    "        self.final_norm = LayerNorm(cfg.emb_dim)\n",
    "        self.out_head = nn.Linear(cfg.emb_dim, cfg.vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, in_idx: torch.Tensor) -> torch.Tensor:\n",
    "        # Extract input dimensions.\n",
    "        # NOTE: The more standard notation for these dimensions is [B, T] where:\n",
    "        #\n",
    "        #   B = batch size\n",
    "        #   T = sequence length (num_tokens)\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "\n",
    "        # Embed the input tokens and add positional encodings.\n",
    "        # NOTE: The token embeddings are a tensor of shape [B, T, D] where:\n",
    "        #\n",
    "        #   B = batch size\n",
    "        #   T = sequence length (num_tokens)\n",
    "        #   D = model dimension (emb_dim)\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        # NOTE: The positional encodings are a tensor of shape [T, D] where:\n",
    "        #\n",
    "        #   T = sequence length (num_tokens)\n",
    "        #   D = model dimension (emb_dim)\n",
    "        #\n",
    "        # The positional encodings though are limited in length to the context length, which is\n",
    "        # set to 1024 for the GPT model.\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "\n",
    "        # Dropout the embedded tokens.\n",
    "        x = self.drop_emb(x)\n",
    "\n",
    "        # Pass through the transformer blocks.\n",
    "        x = self.trf_blocks(x)\n",
    "\n",
    "        # Final layer normalization.\n",
    "        x = self.final_norm(x)\n",
    "\n",
    "        # Project the output to the vocabulary size.\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# Test the GPT model.\n",
    "torch.manual_seed(123)\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "out = model(batch)\n",
    "\n",
    "print(\"Input batch:\\n\", batch)\n",
    "print(\"\\nOutput shape:\", out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 163,009,536\n",
      "Token embedding layer shape: torch.Size([50257, 768])\n",
      "Output layer shape: torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "# NOTE: The reason for the total number of parameters to be 163M instead of 124M is a concept called\n",
    "#       weight tying, which was used in the original GPT-2 architecture. It means that the original\n",
    "#       GPT-2 architecture reuses the weights from the token embedding layer in its output layer.\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")\n",
    "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
    "print(\"Output layer shape:\", model.out_head.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters considering weight tying: 124,412,160\n"
     ]
    }
   ],
   "source": [
    "total_params_gpt2 = total_params - sum(p.numel() for p in model.out_head.parameters())\n",
    "print(\n",
    "    f\"Number of trainable parameters \"\n",
    "    f\"considering weight tying: {total_params_gpt2:,}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the model: 621.83 MB\n"
     ]
    }
   ],
   "source": [
    "# Calculates the total size in bytes (assuming float32, 4 bytes per parameter).\n",
    "total_size_bytes = total_params * 4\n",
    "total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating text\n",
    "\n",
    "Starting with an initial input context (“Hello, I am”), the model predicts a subsequent token during each iteration, appending it to the input context for the next round of prediction.\n",
    "\n",
    "Steps:\n",
    "1. Decode the output tensors\n",
    "2. Select tokens based on a probability distribution\n",
    "3. Convert these tokens to human-readable text\n",
    "\n",
    "![Text generation process](https://drek4537l1klr.cloudfront.net/raschka/Figures/4-16.png)\n",
    "\n",
    "The detailed steps for generating text are shown in the figure below:\n",
    "\n",
    "1. In each step, the model outputs a matrix with vectors representing potential next tokens.\n",
    "2. The vector corresponding to the next token is extracted and converted into a probability distribution via the softmax function.\n",
    "3. Within the vector containing the resulting probability scores, the index of the highest value is located, which translates to the token ID. Selecting the highest probability is also called \"greedy decoding\" but other strategies are also possible (top-k sampling, top-p sampling, etc.).\n",
    "4. This token ID is then decoded back into text, producing the next token in the sequence.\n",
    "5. Finally, this token is appended to the previous inputs, forming a new input sequence for the subsequent iteration.\n",
    "\n",
    "This step-by-step process enables the model to generate text sequentially, building coherent phrases and sentences from the initial input context. In practice, we repeat this process over many iterations until we reach a user-specified number of generated tokens (or the model produces an EOS - end of sequence - token).\n",
    "\n",
    "![Text generation process](https://drek4537l1klr.cloudfront.net/raschka/Figures/4-17.png)\n",
    "\n",
    "<font color='red'>TODO: It seems wasteful to generate output tokens for each input token when we are really only interested in the last output token (i.e. the next token).</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded input tensor: [15496, 11, 314, 716]\n",
      "Encoded input tensor shape: torch.Size([1, 4])\n",
      "Output tensor: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267]])\n",
      "Output length: 10\n",
      "Decoded output text: Hello, I am Featureiman Byeswickattribute argue\n"
     ]
    }
   ],
   "source": [
    "def generate_text_simple(\n",
    "    model: GPTModel, idx: torch.Tensor, max_new_tokens: int, context_size: int\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Generate text using a simple greedy decoding strategy.\n",
    "\n",
    "    NOTE: This function requires the model to be put into eval mode. This has to be done by the\n",
    "          caller.\n",
    "\n",
    "    Args:\n",
    "        model: The GPT model.\n",
    "        idx: The input tensor of shape [B, T] containing the tokenized input text, i.e. a\n",
    "            (batch, n_tokens) array of indices in the current context.\n",
    "        max_new_tokens: The maximum number of new tokens to generate.\n",
    "        context_size: The size of the context window.\n",
    "\n",
    "    Returns:\n",
    "        A tensor of shape [B, T + max_new_tokens] containing the generated text.\n",
    "    \"\"\"\n",
    "    for _ in range(max_new_tokens):\n",
    "        # Crops current context if it exceeds the supported context size, e.g., if LLM supports\n",
    "        # only 5 tokens, and the context size is 10, then only the last 5 tokens are used as\n",
    "        # context.\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "\n",
    "        # Disable gradient calculation since they are not needed for inference.\n",
    "        with torch.no_grad():\n",
    "            # Forward pass through the model.\n",
    "            # NOTE: In more standard notation, this is [B, T, V] where:\n",
    "            #\n",
    "            #   B = batch size\n",
    "            #   T = sequence length (num_tokens)\n",
    "            #   V = vocabulary size\n",
    "            logits = model(idx_cond)  # shape [B, T, V]\n",
    "\n",
    "            # Focus only on the last time step, so that (batch, n_token, vocab_size) becomes\n",
    "            # (batch, vocab_size).\n",
    "            logits = logits[:, -1, :]  # shape [B, V]\n",
    "\n",
    "            # Convert the logits to probabilities.\n",
    "            # NOTE: The softmax function is applied over the vocabulary dimension (i.e. last\n",
    "            #       dimension). That results in a probability distribution over the vocabulary such\n",
    "            #       that the most likely token can be selected.\n",
    "            probas = torch.softmax(logits, dim=-1)  # shape [B, V]\n",
    "\n",
    "            # Select the token with the highest probability.\n",
    "            idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # shape [B, 1]\n",
    "\n",
    "            # Append the selected / sampled index / token to the input sequence.\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # shape [B, T + 1]\n",
    "\n",
    "    return idx\n",
    "\n",
    "\n",
    "# Test the greedy decoding strategy.\n",
    "start_context = \"Hello, I am\"\n",
    "encoded = tokenizer.encode(start_context)\n",
    "\n",
    "# NOTE: The unsqueeze function adds a new dimension at the specified index. In this case, we add a\n",
    "#       new dimension at index 0, which is the batch dimension.\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(f\"Encoded input tensor: {encoded}\")\n",
    "print(f\"Encoded input tensor shape: {encoded_tensor.shape}\")\n",
    "\n",
    "# Disables dropout since we are not training the model.\n",
    "model.eval()\n",
    "out = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=encoded_tensor,\n",
    "    max_new_tokens=6,\n",
    "    context_size=GPT_CONFIG_124M.context_length,\n",
    ")\n",
    "print(f\"Output tensor: {out}\")\n",
    "print(f\"Output length: {len(out.squeeze(0))}\")\n",
    "\n",
    "# Decodes the generated text.\n",
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(f\"Decoded output text: {decoded_text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avpc-off-vehicle-qa-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
