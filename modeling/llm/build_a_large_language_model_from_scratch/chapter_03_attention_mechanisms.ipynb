{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Mechanisms (chapter 3)\n",
    "\n",
    "This notebook explores attention mechanisms (including self-attention) based on Sebastian Raschka's\n",
    "book (Chapter 3), implementing basic self-attention, causal self-attention, and multi-headed\n",
    "self-attention (as shown in the figure below).\n",
    "\n",
    "## Acknowledgment\n",
    "\n",
    "All concepts, architectures, and implementation approaches are credited to Sebastian Raschka's work.\n",
    "This repository serves as my personal implementation and notes while working through the book's content.\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Sebastian Raschka's GitHub](https://github.com/rasbt)\n",
    "- [Book Information](https://www.manning.com/books/build-a-large-language-model-from-scratch)\n",
    "    - [Chapter 3](https://livebook.manning.com/book/build-a-large-language-model-from-scratch/chapter-3/9)\n",
    "\n",
    "![Attention mechanisms](https://drek4537l1klr.cloudfront.net/raschka/Figures/3-2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simplified self-attention\n",
    "\n",
    "This mechanism is inspired by the Bahdanau attention mechanism (named after the author of the paper\n",
    "that introduced this mechanism). When generating an output token, the decoder has access to all input\n",
    "tokens selectively. The importance of each input token is determined by an attention weight. More on\n",
    "the Bahdanau attention mechanism is shown in appendix B of the book.\n",
    "\n",
    "Note that self-attention refers to the fact that we are computing attention / importance weights with\n",
    "respect to a single input sequence. In other words, self-attention asseses and learns the relationships\n",
    "and dependencies between various parts of the input iteself.\n",
    "\n",
    "![Simplified self-attention](https://drek4537l1klr.cloudfront.net/raschka/Figures/3-7.png)\n",
    "\n",
    "In the figure above, we are computing the **context vector** z(2) for the query vector x(2). That context\n",
    "vector z(2) is based on other input elements $x^{(i)}$ in the input sequence $x$ (of length T) where the\n",
    "importance of each input element is determined by **attention weights** $\\alpha_{ij}$.\n",
    "\n",
    "Note that **attention weights $\\alpha_{ij}$** are the normalized version of\n",
    "**attention scores $\\omega_{ij}$** we'll see later on.\n",
    "\n",
    "A context vector can be thought of as an 'enriched' embedding vector that carries context from all\n",
    "other token embedding vectors in the input sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple example (single query vector)\n",
    "\n",
    "![Self-attention example](https://drek4537l1klr.cloudfront.net/raschka/Figures/3-8.png)\n",
    "\n",
    "These **attention scores $\\omega_{ij}$** are then normalized to arrive at **attention\n",
    "weights $\\alpha_{ij}$**. Normalization helps with interpretability and maintaining stability\n",
    "during the training process of LLMs.\n",
    "\n",
    "![Self-attention example continued](https://drek4537l1klr.cloudfront.net/raschka/Figures/3-9.png)\n",
    "\n",
    "Computing the context vector is simply the attention-weighted sum of all input elements.\n",
    "\n",
    "![Context vector computation](https://drek4537l1klr.cloudfront.net/raschka/Figures/3-10.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n",
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n",
      "Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656]) (sum: 1.0000001192092896)\n",
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581]) (sum: 1.0)\n",
      "Context vector: tensor([0.4419, 0.6515, 0.5683])\n",
      "Context vector: tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "# Define the input sequence (T = 6).\n",
    "inputs = torch.tensor(\n",
    "    [\n",
    "        [0.43, 0.15, 0.89],  # Your    (x^1)\n",
    "        [0.55, 0.87, 0.66],  # journey (x^2)\n",
    "        [0.57, 0.85, 0.64],  # starts  (x^3)\n",
    "        [0.22, 0.58, 0.33],  # with    (x^4)\n",
    "        [0.77, 0.25, 0.10],  # one     (x^5)\n",
    "        [0.05, 0.80, 0.55],  # step    (x^6)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Compute the attention scores (for the second element of the sequence x^2)\n",
    "# NOTE: The attention score computes similarity based on the dot product of the query and key vectors,\n",
    "#       which measures how aligned the query and key vectors are (a higher dot product indicates a\n",
    "#       greater degree of alignment, i.e. similarity between two vectors). A dot product is essentially\n",
    "#       a concise way of multiplying two vectors element-wise and summing the result.\n",
    "# NOTE: In the context of self-attention, the dot product determines the amount of attention the query\n",
    "#       should \"pay\" to each key in the input sequence.\n",
    "\n",
    "# 1. Via basic for-loops.\n",
    "query = inputs[1]  # Python uses 0-based indexing.\n",
    "attn_scores = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores[i] = torch.dot(x_i, query)\n",
    "print(attn_scores)\n",
    "\n",
    "\n",
    "# 2. Via matrix multiplication.\n",
    "attn_scores_mm = (\n",
    "    query @ inputs.T\n",
    ")  # The @ operator is syntactic sugar for matrix multiplication.\n",
    "print(attn_scores_mm)\n",
    "\n",
    "# Verify that the two methods yield the same attention scores.\n",
    "assert torch.allclose(attn_scores, attn_scores_mm)\n",
    "\n",
    "# Normalize the attention scores to get the attention weights.\n",
    "# 1. Via a naive approach.\n",
    "attn_weights = attn_scores / torch.sum(attn_scores)\n",
    "print(f\"Attention weights: {attn_weights} (sum: {torch.sum(attn_weights)})\")\n",
    "\n",
    "# 2. Via the softmax function.\n",
    "# NOTE: Softmax handles extreme values more gracefully and offers more favorable gradient properties\n",
    "#       during training.\n",
    "# NOTE: Since the softmax function ensures that attention weights are always positive and sum to 1,\n",
    "#       we can interpret the attention weights as probabilities.\n",
    "attn_weights_softmax = torch.nn.functional.softmax(attn_scores, dim=0)\n",
    "print(\n",
    "    f\"Attention weights: {attn_weights_softmax} (sum: {torch.sum(attn_weights_softmax)})\"\n",
    ")\n",
    "\n",
    "# Compute the context vector z(2) for the query vector x(2).\n",
    "# 1. Via a naive approach via a for-loop.\n",
    "context_vector_2 = torch.zeros(inputs.shape[1])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    context_vector_2 += attn_weights_softmax[i] * x_i\n",
    "print(f\"Context vector: {context_vector_2}\")\n",
    "\n",
    "# 2. Via matrix multiplication.\n",
    "context_vector_2_mm = attn_weights_softmax @ inputs\n",
    "print(f\"Context vector: {context_vector_2_mm}\")\n",
    "\n",
    "# Verify that the two methods yield the same attention scores.\n",
    "assert torch.allclose(context_vector_2, context_vector_2_mm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple example (batch query)\n",
    "\n",
    "![Batched attention weight computation](https://drek4537l1klr.cloudfront.net/raschka/Figures/3-11.png)\n",
    "\n",
    "A small side-note on tensor initialization.\n",
    "\n",
    "1. torch.empty\n",
    "    - Creates a tensor with uninitialized data - the tensor will be allocated in memory but the values are not initialized\n",
    "    - The tensor contains whatever values were already in the allocated memory block (garbage values)\n",
    "    - It's faster than torch.zeros because it skips the step of initializing all values\n",
    "2. torch.zeros\n",
    "    - Creates a tensor filled with the scalar value 0\n",
    "    - Explicitly initializes all elements of the tensor to zero\n",
    "    - Slightly slower than torch.empty because it needs to set all values to zero\n",
    "\n",
    "When to use which:\n",
    "- Use torch.zeros when you need a tensor initialized with zeros (most common use case)\n",
    "- Use torch.empty when:\n",
    "    - You'll immediately overwrite all values in the tensor\n",
    "    - Performance is critical and you don't care about initial values\n",
    "    - You're creating a buffer that will be filled later\n",
    "\n",
    "![Computation flow](https://drek4537l1klr.cloudfront.net/raschka/Figures/3-12.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: 'Your journey starts with one step'\n",
      "Inputs shape: torch.Size([6, 3])\n",
      "Unnormalized attention scores:\n",
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n",
      "\n",
      "Normalized attention scores:\n",
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n",
      "Sum of attention weights for each row: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "Context vectors:\n",
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "# Initialize the full attention weights matrix (a square matrix of shape (T, T)).\n",
    "print(f\"Inputs: 'Your journey starts with one step'\")\n",
    "print(f\"Inputs shape: {inputs.shape}\")\n",
    "\n",
    "# Compute the unnormalized attention scores.\n",
    "# 1. Via a naive approach via nested for-loops.\n",
    "attn_scores = torch.zeros(inputs.shape[0], inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):  # Iterate over the rows of the inputs tensor\n",
    "    for j, x_j in enumerate(inputs):  # Iterate over the columns of the inputs tensor\n",
    "        attn_scores[i, j] = torch.dot(\n",
    "            x_i, x_j\n",
    "        )  # Compute the dot product of the row and column vectors\n",
    "\n",
    "# 2. Via matrix multiplication.\n",
    "attn_scores_mm = inputs @ inputs.T\n",
    "assert torch.allclose(attn_scores, attn_scores_mm)\n",
    "print(f\"Unnormalized attention scores:\\n{attn_scores_mm}\\n\")\n",
    "\n",
    "# Normalize the attention scores to get the attention weights.\n",
    "attn_weights = torch.nn.functional.softmax(attn_scores_mm, dim=1)\n",
    "print(f\"Normalized attention scores:\\n{attn_weights}\")\n",
    "print(f\"Sum of attention weights for each row: {attn_weights.sum(dim=1)}\")\n",
    "\n",
    "# Compute the context vectors for all query vectors.\n",
    "context_vectors = attn_weights @ inputs\n",
    "print(f\"Context vectors:\\n{context_vectors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainable self-attention\n",
    "\n",
    "This attention mechanism was used in the original GPT implementation and is often referred to as\n",
    "**scaled dot-product attention**.\n",
    "\n",
    "The main difference to the simplified self-attention mechanism is the introduction of three trainable\n",
    "weight matrices $W_q$, $W_k$, $W_v$ (for query, key, and value respectively) that are used to project\n",
    "the embedded input tokens $x^{(i)}$ into query, key, and value vectors respectively.\n",
    "\n",
    "In the image below note that only the second token (\"journey\") is projected into a query vector since\n",
    "only the second token is \"being queried\". All T input tokens, however, are projected into key and value\n",
    "vectors (which will later be used to compute the full attention weight matrix).\n",
    "\n",
    "![Scaled dot-product attention](https://drek4537l1klr.cloudfront.net/raschka/Figures/3-14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple example (single query vector)\n",
    "\n",
    "Unlike in the simplified self-attention mechanism, scaled dot-product attention computes attention\n",
    "scores not on raw token embeddings but on the tokens projected into key and value space (via weight\n",
    "matrices $W_k$ and $W_v$).\n",
    "\n",
    "![Scaled dot-product attention example](https://drek4537l1klr.cloudfront.net/raschka/Figures/3-15.png)\n",
    "\n",
    "Computing the normalized attention weights $\\alpha_{ij}$ from unnormalized attention scores\n",
    "$\\omega_{ij}$ is done via the softmax function as before. This time, however, we scale the attention\n",
    "scores by dividing them by the square root of the embedding dimension of the keys (hence the name\n",
    "*scaled* dot-product attention).\n",
    "\n",
    "NOTE: The reason for the normalization by the embedding dimension size is to improve the training\n",
    "performance by avoiding small gradients. For instance, when scaling up the embedding dimension,\n",
    "which is typically greater than 1,000 for GPT-like LLMs, large dot products can result in very\n",
    "small gradients during backpropagation due to the softmax function applied to them. As dot products\n",
    "increase, the softmax function behaves more like a step function, resulting in gradients nearing\n",
    "zero. These small gradients can drastically slow down learning or cause training to stagnate (see \n",
    "page 69 in Sebastian Raschka's [book](https://www.manning.com/books/build-a-large-language-model-from-scratch)).\n",
    "\n",
    "![Scaled dot-product attention example continued](https://drek4537l1klr.cloudfront.net/raschka/Figures/3-16.png)\n",
    "\n",
    "The last step is to compute the context vector for $x^{(2)}$, which is the weighted sum of all value\n",
    "vectors of the input sequence (i.e. the input tokens embedded via the $W_v$ matrix).\n",
    "\n",
    "![Scaled dot-product attention example continued](https://drek4537l1klr.cloudfront.net/raschka/Figures/3-17.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input token / shape: tensor([0.5500, 0.8700, 0.6600]) (torch.Size([3]))\n",
      "Weight matrix shape: torch.Size([3, 2])\n",
      "Projected query vector: tensor([-1.1729, -0.0048])\n",
      "Keys shape: torch.Size([6, 2])\n",
      "Values shape: torch.Size([6, 2])\n",
      "Unnormalized attention score for x^2: 0.13763877749443054\n",
      "Unnormalized attention scores: tensor([ 0.2172,  0.1376,  0.1730, -0.0491,  0.7616, -0.3809])\n",
      "Attention weights: tensor([0.1704, 0.1611, 0.1652, 0.1412, 0.2505, 0.1117])\n",
      "Context vector: tensor([0.2854, 0.4081])\n"
     ]
    }
   ],
   "source": [
    "# Define input and output embedding size of the W_i embedding matrices.\n",
    "# NOTE: For GPT-like models, the input embedding size is typically equal to the output embedding\n",
    "#       size.\n",
    "x_2 = inputs[1]  # Python uses 0-based indexing.\n",
    "d_in = inputs.shape[1]  # The size of the input embedding dimension.\n",
    "d_out = 2  # The size of the output embedding dimension.\n",
    "print(f\"Input token / shape: {x_2} ({x_2.shape})\")\n",
    "\n",
    "# Instantiate the trainable weight matrices.\n",
    "# NOTE: requires_grad=False is done here to reduce visual clutter in the outputs. When training the\n",
    "#       model, requires_grad obviously has to be set to True to update the weights during training.\n",
    "torch.manual_seed(123)\n",
    "W_q = torch.nn.Parameter(torch.randn(d_in, d_out), requires_grad=False)\n",
    "W_k = torch.nn.Parameter(torch.randn(d_in, d_out), requires_grad=False)\n",
    "W_v = torch.nn.Parameter(torch.randn(d_in, d_out), requires_grad=False)\n",
    "\n",
    "# Project the query input token into query, key, and value vectors.\n",
    "query_2 = x_2 @ W_q\n",
    "key_2 = x_2 @ W_k\n",
    "value_2 = x_2 @ W_v\n",
    "print(f\"Weight matrix shape: {W_q.shape}\")\n",
    "print(f\"Projected query vector: {query_2}\")\n",
    "\n",
    "# Compute key and value vectors for all input tokens.\n",
    "# NOTE: Computing the context vector for the query vector x(2) requires the key and value vectors of\n",
    "#       all input tokens.\n",
    "keys = inputs @ W_k\n",
    "values = inputs @ W_v\n",
    "print(f\"Keys shape: {keys.shape}\")\n",
    "print(f\"Values shape: {values.shape}\")\n",
    "\n",
    "# Compute the unnormalized attention scores (for the query vector x(2) first).\n",
    "keys_2 = keys[1]\n",
    "attn_scores_2 = query_2.dot(keys_2)\n",
    "print(f\"Unnormalized attention score for x^2: {attn_scores_2}\")\n",
    "\n",
    "# Compute the unnormalized attention scores for all input tokens.\n",
    "attn_scores = query_2 @ keys.T\n",
    "print(f\"Unnormalized attention scores: {attn_scores}\")\n",
    "\n",
    "# Normalize the attention scores to get the attention weights.\n",
    "d_k = keys.shape[1]\n",
    "attn_weights = torch.nn.functional.softmax(attn_scores / d_k**0.5, dim=-1)\n",
    "print(f\"Attention weights: {attn_weights}\")\n",
    "\n",
    "# Compute the context vector for the query vector x(2).\n",
    "context_vector_2 = torch.zeros(d_out)\n",
    "for i, v_i in enumerate(values):\n",
    "    context_vector_2 += attn_weights[i] * v_i\n",
    "\n",
    "context_vector_2_mm = attn_weights @ values\n",
    "assert torch.allclose(context_vector_2, context_vector_2_mm)\n",
    "print(f\"Context vector: {context_vector_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A self-attention class\n",
    "\n",
    "In self-attention, we transform the input vectors in the input matrix X with the three weight\n",
    "matrices, $W_q$, $W_k$, and $W_v$. The new compute the attention weight matrix based on the resulting\n",
    "queries (Q) and keys (K). Using the attention weights and values (V), we then compute the context\n",
    "vectors (Z).\n",
    "\n",
    "![A Python class implementing self-attention](https://drek4537l1klr.cloudfront.net/raschka/Figures/3-18.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context vector 2 (from before): tensor([0.2854, 0.4081])\n",
      "Context vectors (V1):\n",
      "tensor([[0.2845, 0.4071],\n",
      "        [0.2854, 0.4081],\n",
      "        [0.2854, 0.4075],\n",
      "        [0.2864, 0.3974],\n",
      "        [0.2863, 0.3910],\n",
      "        [0.2860, 0.4039]], grad_fn=<MmBackward0>)\n",
      "Context vectors (V2):\n",
      "tensor([[0.5322, 0.2491],\n",
      "        [0.5316, 0.2488],\n",
      "        [0.5316, 0.2488],\n",
      "        [0.5340, 0.2501],\n",
      "        [0.5331, 0.2497],\n",
      "        [0.5337, 0.2499]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class SelfAttentionV1(nn.Module):\n",
    "    def __init__(self, d_in: int, d_out: int):\n",
    "        super().__init__()\n",
    "        self.W_q = nn.Parameter(torch.randn(d_in, d_out))\n",
    "        self.W_k = nn.Parameter(torch.randn(d_in, d_out))\n",
    "        self.W_v = nn.Parameter(torch.randn(d_in, d_out))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Project the input tokens into query, key, and value vectors.\n",
    "        query = x @ self.W_q\n",
    "        key = x @ self.W_k\n",
    "        value = x @ self.W_v\n",
    "\n",
    "        # Compute the unnormalized attention scores, i.e. the omegas.\n",
    "        attn_scores = query @ key.T\n",
    "\n",
    "        # Normalize the attention scores to get the attention weights, i.e. the alphas.\n",
    "        d_k = key.shape[-1]\n",
    "        attn_weights = torch.softmax(attn_scores / d_k**0.5, dim=-1)\n",
    "\n",
    "        # Compute the full set of context vectors.\n",
    "        context_vectors = attn_weights @ value\n",
    "\n",
    "        return context_vectors\n",
    "\n",
    "\n",
    "class SelfAttentionV2(nn.Module):\n",
    "    \"\"\"A Python class implementing self-attention.\n",
    "\n",
    "    V2 replaces the nn.Parameter objects with nn.Linear objects which effectively perform matrix\n",
    "    multiplication when the bias units are disabled.\n",
    "\n",
    "    One significant advantage of using nn.Linear objects is that nn.Linear has an optimized weight\n",
    "    initialization scheme that helps with stabilizing the training process and making it more\n",
    "    effective.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_in: int, d_out: int, qkv_bias: bool = False):\n",
    "        super().__init__()\n",
    "        self.W_q = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_k = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_v = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Project the input tokens into query, key, and value vectors.\n",
    "        query = self.W_q(x)\n",
    "        key = self.W_k(x)\n",
    "        value = self.W_v(x)\n",
    "\n",
    "        # Compute the unnormalized attention scores, i.e. the omegas.\n",
    "        attn_scores = query @ key.T\n",
    "\n",
    "        # Normalize the attention scores to get the attention weights, i.e. the alphas.\n",
    "        d_k = key.shape[-1]\n",
    "        attn_weights = torch.softmax(attn_scores / d_k**0.5, dim=-1)\n",
    "\n",
    "        # Compute the full set of context vectors.\n",
    "        context_vectors = attn_weights @ value\n",
    "\n",
    "        return context_vectors\n",
    "\n",
    "\n",
    "# Test the self-attention class.\n",
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttentionV1(d_in, d_out)\n",
    "sa_v2 = SelfAttentionV2(d_in, d_out)\n",
    "\n",
    "# NOTE: SelfAttentionV1 and SelfAttentionV2 give different outputs because they use different\n",
    "#       initial weights for the weight matrices since nn.Linear uses a more sophisticated weight\n",
    "#       initialization scheme.\n",
    "print(f\"Context vector 2 (from before): {context_vector_2_mm}\")\n",
    "print(f\"Context vectors (V1):\\n{sa_v1(inputs)}\")\n",
    "print(f\"Context vectors (V2):\\n{sa_v2(inputs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Causal self-attention\n",
    "\n",
    "Causal attention or *masked attention* restricts a model to only consider previous and current\n",
    "inputs in a sequence when processing a given query token when computing attention scores (compare\n",
    "that to standard self-attention that considers the entire sequence as seen above).\n",
    "\n",
    "Causal self-attention masks out future tokens such that the are not taken into account when\n",
    "computing context vectors. In the diagram below, any token above the diagonal of the attention\n",
    "matrix is a future token that should not be taken into account.\n",
    "\n",
    "For example, the token \"Your\" can only attend to the first token in the sequence (i.e. \"Your\") while\n",
    "the third token \"starts\" can attend to all prior tokens as well, i.e. \"Your\", \"journey\", and \"starts\".\n",
    "\n",
    "![Causal self-attention](https://drek4537l1klr.cloudfront.net/raschka/Figures/3-19.png)\n",
    "\n",
    "The causal self-attention implementation will modify our previous self-attention implementation by\n",
    "introducing a mask to modify the attention weight matrix.\n",
    "\n",
    "![Causal self-attention implementation sketch](https://drek4537l1klr.cloudfront.net/raschka/Figures/3-20.png)\n",
    "\n",
    "The above implementation results in wasted computation since we still compute the full attention\n",
    "matrix and normalize twice (once in the softmax operation on the full attention matrix and once\n",
    "after masking out upper triangular entries). A more efficient implementation below relies on a\n",
    "property of the softmax function (where negative infinity entries in the attention matrix are\n",
    "essentially zero probability entries). Mathematically, this occurs because $e^{-\\infty}  \\rightarrow 0$.\n",
    "\n",
    "![Causal self-attention implementation sketch efficient](https://drek4537l1klr.cloudfront.net/raschka/Figures/3-21.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights:\n",
      "tensor([[0.1825, 0.1568, 0.1576, 0.1657, 0.1796, 0.1577],\n",
      "        [0.1852, 0.1554, 0.1562, 0.1655, 0.1814, 0.1563],\n",
      "        [0.1852, 0.1554, 0.1562, 0.1654, 0.1814, 0.1563],\n",
      "        [0.1756, 0.1611, 0.1615, 0.1662, 0.1740, 0.1616],\n",
      "        [0.1804, 0.1589, 0.1594, 0.1655, 0.1765, 0.1593],\n",
      "        [0.1760, 0.1605, 0.1610, 0.1664, 0.1749, 0.1612]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "\n",
      "Simple mask:\n",
      "tensor([[1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1]])\n",
      "\n",
      "Mask (for softmax):\n",
      "tensor([[0, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 0, 0]])\n",
      "\n",
      "Attention weights masked:\n",
      "tensor([[0.1825,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.1852, 0.1554,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.1852, 0.1554, 0.1562,   -inf,   -inf,   -inf],\n",
      "        [0.1756, 0.1611, 0.1615, 0.1662,   -inf,   -inf],\n",
      "        [0.1804, 0.1589, 0.1594, 0.1655, 0.1765,   -inf],\n",
      "        [0.1760, 0.1605, 0.1610, 0.1664, 0.1749, 0.1612]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "\n",
      "Attention weights masked and normalized:\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5053, 0.4947, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3380, 0.3309, 0.3311, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2517, 0.2491, 0.2492, 0.2500, 0.0000, 0.0000],\n",
      "        [0.2017, 0.1987, 0.1988, 0.1996, 0.2012, 0.0000],\n",
      "        [0.1678, 0.1659, 0.1660, 0.1666, 0.1676, 0.1660]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Compute the attention scores.\n",
    "# NOTE: Reuses the query and key weight matrices of the SelfAttention_v2 object from the previous\n",
    "#       section for convenience\n",
    "queries = sa_v2.W_q(inputs)\n",
    "keys = sa_v2.W_k(inputs)\n",
    "attn_scores = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=-1)\n",
    "print(f\"Attention weights:\\n{attn_weights}\")\n",
    "\n",
    "# Simple mask with 0s above the main diagonal\n",
    "# NOTE: torch.tril creates a lower triangular matrix with ones on and below the diagonal.\n",
    "context_length = attn_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length)).type(torch.int64)\n",
    "print(f\"\\nSimple mask:\\n{mask_simple}\")\n",
    "\n",
    "# Create a mask with negative infinity entries for the upper triangular entries.\n",
    "# NOTE: torch.triu creates an upper triangular matrix with ones on and above the diagonal.\n",
    "context_length = attn_weights.shape[0]\n",
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1).type(\n",
    "    torch.int64\n",
    ")\n",
    "print(f\"\\nMask (for softmax):\\n{mask}\")\n",
    "\n",
    "# Apply the mask to the attention scores.\n",
    "attn_weights_masked = attn_weights.masked_fill(mask.bool(), -torch.inf)\n",
    "print(f\"\\nAttention weights masked:\\n{attn_weights_masked}\")\n",
    "\n",
    "# Normalize the masked attention weights.\n",
    "attn_weights_masked_normalized = torch.softmax(\n",
    "    attn_weights_masked / keys.shape[-1] ** 0.5, dim=1\n",
    ")\n",
    "print(f\"\\nAttention weights masked and normalized:\\n{attn_weights_masked_normalized}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout: Masking additional attention weights\n",
    "\n",
    "Dropout is a technique where randomly selected hidden layer units are ignored (or dropped out) which\n",
    "helps prevent overfitting during training because the model is not allowed to become overly reliant\n",
    "on any specific set of hidden layer units. Note that dropout is only used *during training* and\n",
    "disabled afterwards.\n",
    "\n",
    "Dropout in self-attention is most commonly applied at two specific times:\n",
    "1. after calculating the attention weights\n",
    "2. after applying the attention weights to the value vectors\n",
    "\n",
    "Here we'll apply dropout after applying the attention weights to the value vectors (which is the more\n",
    "common variant in practice).\n",
    "\n",
    "![Dropout in causal self-attention](https://drek4537l1klr.cloudfront.net/raschka/Figures/3-22.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example:\n",
      "tensor([[1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n",
      "Dropout:\n",
      "tensor([[2., 2., 2., 2., 2., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.],\n",
      "        [0., 0., 2., 0., 2., 0.],\n",
      "        [2., 2., 0., 0., 0., 2.],\n",
      "        [2., 0., 0., 0., 0., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.]])\n",
      "Dropout:\n",
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.6622, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4982, 0.0000, 0.5000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3974, 0.3975, 0.3993, 0.4024, 0.0000],\n",
      "        [0.3355, 0.3319, 0.0000, 0.0000, 0.3353, 0.3320]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "# Instantiate the dropout module (choose a dropout probability of 50%)\n",
    "dropout = torch.nn.Dropout(p=0.5)\n",
    "\n",
    "# Create some example data (a matrix of ones).\n",
    "example = torch.ones(6, 6)\n",
    "print(f\"Example:\\n{example}\")\n",
    "\n",
    "# NOTE: Applying dropout scales the outputs by a factor of 1/(1-p) during training. This means that\n",
    "#       during evaluation the module simply computes an identity function. This is done to compensate\n",
    "#       for the reduction of active elements and is crucial to maintain the overall balance of the\n",
    "#       attention weights as it ensures that the average influence of the attention mechanism remains\n",
    "#       consistent during both the training and inference phases.\n",
    "# See https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html\n",
    "print(f\"Dropout:\\n{dropout(example)}\")\n",
    "\n",
    "# Apply dropout to the attention weights.\n",
    "print(f\"Dropout:\\n{dropout(attn_weights_masked_normalized)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A compact causal attention class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: torch.Size([2, 6, 3])\n",
      "Context vector shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "# We want to ensure that our implementation works with batches of data (as produced by the\n",
    "# dataloader implemented in chapter 2).\n",
    "batch = torch.stack([inputs, inputs], dim=0)\n",
    "print(f\"Batch shape: {batch.shape}\")\n",
    "\n",
    "\n",
    "class CausalAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_in: int,\n",
    "        d_out: int,\n",
    "        context_length: int,\n",
    "        dropout_prob: float = 0.1,\n",
    "        qkv_bias: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Cache d_out for later use.\n",
    "        self.d_out = d_out\n",
    "\n",
    "        # Initialize the weight matrices.\n",
    "        self.W_q = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_k = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_v = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "        # Initialize the dropout module.\n",
    "        # Compared to the previous implementation, we now use a dropout layer.\n",
    "        self.dropout = torch.nn.Dropout(p=dropout_prob)\n",
    "\n",
    "        # Register a buffer for the mask.\n",
    "        # NOTE: Buffers are not trained and are not subject to gradient descent.\n",
    "        # NOTE: The use of register_buffer in PyTorch is not strictly necessary for all use cases\n",
    "        #       but offers several advantages here. For instance, when we use the CausalAttention\n",
    "        #       class in our LLM, buffers are automatically moved to the appropriate device (CPU or\n",
    "        #       GPU) along with our model, which will be relevant when training our LLM. This means\n",
    "        #       we don’t need to manually ensure these tensors are on the same device as your model\n",
    "        #       parameters, avoiding device mismatch errors.\n",
    "        self.register_buffer(\n",
    "            \"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, verbose: bool = False) -> torch.Tensor:\n",
    "        # Extract input dimensions.\n",
    "        batch_size, num_tokens, d_in = x.shape\n",
    "\n",
    "        # Project input into query, key, and value vectors.\n",
    "        query = self.W_q(x)\n",
    "        key = self.W_k(x)\n",
    "        value = self.W_v(x)\n",
    "\n",
    "        # Compute the unnormalized attention scores.\n",
    "        # NOTE: We transpose dimensions 1 and 2, keeping the batch dimension at the first position (0).\n",
    "        attn_scores = query @ key.transpose(-2, -1)\n",
    "\n",
    "        # Apply the mask to the attention scores.\n",
    "        # NOTE: In PyTorch, operations with a trailing underscore are performed in-place, avoiding\n",
    "        #       unnecessary memory copies.\n",
    "        attn_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"Unnormalized causal attention scores (shape: {attn_scores.shape}):\\n{attn_scores}\"\n",
    "            )\n",
    "\n",
    "        # Normalize the attention scores.\n",
    "        attn_weights = torch.softmax(attn_scores / self.d_out**0.5, dim=-1)\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"Normalized causal attention weights (shape: {attn_weights.shape}):\\n{attn_weights}\"\n",
    "            )\n",
    "\n",
    "        # Apply dropout to the attention weights.\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Compute the context vectors.\n",
    "        context_vectors = attn_weights @ value\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"Context vectors (shape: {context_vectors.shape}):\\n{context_vectors}\"\n",
    "            )\n",
    "\n",
    "        return context_vectors\n",
    "\n",
    "\n",
    "# Test the causal attention class.\n",
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(f\"Context vector shape: {context_vecs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-head self-attention (naive)\n",
    "\n",
    "The term “multi-head” refers to dividing the attention mechanism into multiple\n",
    "“heads,” each operating independently. In this context, a single causal attention module can be considered single-head attention, where there is only one set of attention weights processing the input sequentially.\n",
    "\n",
    "In a basic implementation of multi-head self-attention, one could just stack multiple\n",
    "causal self-attention modules as is done in the following figure and the MultiHeadAttentionWrapper below. Each head has its own weights and all heads' outputs are combined by stacking the output tensors. This is a rather inefficient implementation since the individual heads are processed sequentially.\n",
    "\n",
    "Using multiple instances of the self-attention mechanism can be computationally intensive, but it’s crucial for the kind of complex pattern recognition that models like transformer-based LLMs are known for.\n",
    "\n",
    "As mentioned before, the main idea behind multi-head attention is to run the attention mechanism multiple times (in parallel) with different, learned linear projections—the results of multiplying the input data (like the query, key, and value vectors in attention mechanisms) by a weight matrix. \n",
    "\n",
    "![Naive multi-head self-attention](https://drek4537l1klr.cloudfront.net/raschka/Figures/3-24.png)\n",
    "\n",
    "The example below shows how the individual heads compute their output and how the output tensors are stacked.\n",
    "\n",
    "![Naive multi-head self-attention example](https://drek4537l1klr.cloudfront.net/raschka/Figures/3-25.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context vectors:\n",
      "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
      "\n",
      "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n",
      "Batch shape: torch.Size([2, 6, 3])\n",
      "Context vectors shape: torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_in: int,\n",
    "        d_out: int,\n",
    "        context_length: int,\n",
    "        dropout: float,\n",
    "        num_heads: int,\n",
    "        qkv_bias: bool = False,\n",
    "    ):\n",
    "        \"\"\"Initialize the multi-head attention wrapper.\n",
    "\n",
    "        Args:\n",
    "            d_in: The dimension of the input.\n",
    "            d_out: The dimension of the output.\n",
    "            context_length: The length of the context. This argument sets the length of the causal\n",
    "                mask, i.e. the maximum supported sequence length.\n",
    "            dropout: The dropout probability.\n",
    "            num_heads: The number of attention heads.\n",
    "            qkv_bias: Whether to use a bias in the query, key, and value projections.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [\n",
    "                CausalAttention(d_in, d_out, context_length, dropout, qkv_bias)\n",
    "                for _ in range(num_heads)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # NOTE: All heads are processed sequentially which is inefficient.\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "\n",
    "\n",
    "# Test with a simple example.\n",
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]  # This is the number of tokens\n",
    "d_in, d_out = 3, 2\n",
    "\n",
    "mha = MultiHeadAttentionWrapper(\n",
    "    d_in=d_in, d_out=d_out, context_length=context_length, dropout=0.0, num_heads=2\n",
    ")\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "# NOTE: The shapes below indicate that the input batch (shape [2,6,3]) is transformed into a tensor\n",
    "#       of shape [2,6,4] where the last dimension is the concatenation of the outputs of the two heads.\n",
    "# Input size:                       [2,6,3] (last dim is d_in)\n",
    "# Individual head output size:      [2,6,2] (last dim is d_out)\n",
    "# Concatenated head output size:    [2,6,4] (last dim is d_out * num_heads)\n",
    "print(f\"Context vectors:\\n{context_vecs}\")\n",
    "print(f\"Batch shape: {batch.shape}\")\n",
    "print(f\"Context vectors shape: {context_vecs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-head self-attention (parallel)\n",
    "\n",
    "One way to improve the efficiency of multi-head self-attention (MHA) is by computing the outputs for all attention heads simultaneously via matrix multiplication.\n",
    "\n",
    "The CausalAttention class independently performs the attention mechanism, and the results from each head are concatenated. In contrast, the following MultiHeadAttention class integrates the multi-head functionality within a single class. It splits the input into multiple heads by reshaping the projected query, key, and value tensors and then combines the results from these heads after computing attention.\n",
    "\n",
    "The below image compares the two implementations. In the MultiHeadAttentionWrapper class with two attention heads, we initialized two weight matrices, $W_{q_1}$ and $W_{q_2}$, and computed two query matrices, $Q_1$ and $Q_2$ (top). In the MultiheadAttention class, we initialize one larger weight matrix $W_q$, only perform one matrix multiplication with the inputs to obtain a query matrix Q, and then split the query matrix into $Q_1$ and $Q_2$ (bottom).\n",
    "\n",
    "The splitting of the query, key, and value tensors is achieved through tensor reshaping and transposing operations using PyTorch’s .view and .transpose methods. The input is first transformed (via linear layers for queries, keys, and values) and then reshaped to represent multiple heads.\n",
    "\n",
    "![Multi-head self-attention](https://drek4537l1klr.cloudfront.net/raschka/Figures/3-26.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([2, 6, 2])\n",
      "queries.shape: torch.Size([2, 6, 2])\n",
      "values.shape: torch.Size([2, 6, 2])\n",
      "head_dim: 1\n",
      "keys.shape: torch.Size([2, 6, 2, 1])\n",
      "values.shape: torch.Size([2, 6, 2, 1])\n",
      "queries.shape: torch.Size([2, 6, 2, 1])\n",
      "keys.shape: torch.Size([2, 2, 6, 1])\n",
      "values.shape: torch.Size([2, 2, 6, 1])\n",
      "queries.shape: torch.Size([2, 2, 6, 1])\n",
      "attn_scores.shape: torch.Size([2, 2, 6, 6])\n",
      "mask_bool.shape: torch.Size([6, 6])\n",
      "attn_weights.shape: torch.Size([2, 2, 6, 6])\n",
      "context_vec.shape: torch.Size([2, 6, 2, 1])\n",
      "context_vec.shape: torch.Size([2, 6, 2])\n",
      "tensor([[[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]],\n",
      "\n",
      "        [[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_in: int,\n",
    "        d_out: int,\n",
    "        context_length: int,\n",
    "        dropout: float,\n",
    "        num_heads: int,\n",
    "        qkv_bias: bool = False,\n",
    "    ):\n",
    "        \"\"\"Initialize the multi-head attention class.\n",
    "\n",
    "        Args:\n",
    "            d_in: The dimension of the input.\n",
    "            d_out: The dimension of the output.\n",
    "            context_length: The length of the context. This argument sets the length of the causal\n",
    "                mask, i.e. the maximum supported sequence length.\n",
    "            dropout: The dropout probability.\n",
    "            num_heads: The number of attention heads.\n",
    "            qkv_bias: Whether to use a bias in the query, key, and value projections.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Verify that the output dimension is divisible by the number of heads, which is required\n",
    "        # for splitting the output into the specified number of heads.\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        # Cache the output dimension and the number of heads for later use.\n",
    "        # NOTE: This reduces the projection dim to match the desired output dim.\n",
    "        # NOTE: The key operation is to split the d_out dimension into num_heads and head_dim,\n",
    "        #       where head_dim = d_out / num_heads. This splitting is then achieved using the .view\n",
    "        #       method: a tensor of dimensions (b, num_tokens, d_out) is reshaped to dimension\n",
    "        #       (b, num_tokens, num_heads, head_dim).\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        # Initialize the weight matrices.\n",
    "        # NOTE: Only a single weight matrix is initialized for the query, key, and value projections\n",
    "        #       since we will split the output into the specified number of heads.\n",
    "        self.W_q = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_k = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_v = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "        # Initialize the output projection layer.\n",
    "        # NOTE: This implementation uses a Linear layer to combine all head outputs.\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "\n",
    "        # Initialize the dropout layer.\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Register a buffer for the mask.\n",
    "        self.register_buffer(\n",
    "            \"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Extract input dimensions.\n",
    "        # NOTE: The customary notation for these dimensions is [B, T, D] where:\n",
    "        #\n",
    "        #   B = batch size\n",
    "        #   T = sequence length (num_tokens)\n",
    "        #   D = model dimension (d_in)\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        # Project the input into query, key, and value vectors.\n",
    "        # NOTE: The shape of the projected query, key, and value vectors is [B, T, D]\n",
    "        keys = self.W_k(x)  # [B, T, D]\n",
    "        queries = self.W_q(x)  # [B, T, D]\n",
    "        values = self.W_v(x)  # [B, T, D]\n",
    "        print(f\"keys.shape: {keys.shape}\")\n",
    "        print(f\"queries.shape: {queries.shape}\")\n",
    "        print(f\"values.shape: {values.shape}\")\n",
    "\n",
    "        # Split the query, key, and value vectors into the specified number of heads.\n",
    "        # NOTE: We implicitly split the matrix by adding a num_heads dimension. Then we unroll the\n",
    "        #       last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim).\n",
    "        # Compute head dimension\n",
    "        head_dim = self.d_out // self.num_heads\n",
    "        print(f\"head_dim: {head_dim}\")\n",
    "\n",
    "        # Reshape to [B, T, H, D_h] where:\n",
    "        #\n",
    "        #   B = batch size\n",
    "        #   T = sequence length (num_tokens)\n",
    "        #   H = number of heads\n",
    "        #   D_h = dimension per head (head_dim)\n",
    "        #\n",
    "        # NOTE: We implicitly split the matrix by adding a num_heads dimension. Then we unroll the\n",
    "        #       last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim).\n",
    "        # NOTE: See section \"A note on views\" for more details.\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, head_dim)\n",
    "        print(f\"keys.shape: {keys.shape}\")\n",
    "        print(f\"values.shape: {values.shape}\")\n",
    "        print(f\"queries.shape: {queries.shape}\")\n",
    "\n",
    "        # Transpose from [B, T, H, D_h] to [B, H, T, D_h], i.e. from shape\n",
    "        # (b, num_tokens, num_heads, head_dim) to (b, num_heads, num_tokens, head_dim)\n",
    "        #\n",
    "        # NOTE: This transposition is crucial for correctly aligning the queries, keys, and values\n",
    "        #       across the different heads and performing batched matrix multiplications\n",
    "        #       efficiently.\n",
    "        # NOTE: This reshaping results in each head having access to the full sequence of tokens\n",
    "        #       (i.e. a tensor of shape T x D_h).\n",
    "        keys = keys.transpose(1, 2)  # shape [B, H, T, D_h]\n",
    "        queries = queries.transpose(1, 2)  # shape [B, H, T, D_h]\n",
    "        values = values.transpose(1, 2)  # shape [B, H, T, D_h]\n",
    "        print(f\"keys.shape: {keys.shape}\")\n",
    "        print(f\"values.shape: {values.shape}\")\n",
    "        print(f\"queries.shape: {queries.shape}\")\n",
    "\n",
    "        # Compute the unnormalized attention scores via a dot product for each head.\n",
    "        # NOTE: We transpose the T and D_h dimension (i.e. num_tokens and head_dim) just like we\n",
    "        #       have already done in the \"Trainable self-attention\" section.\n",
    "        # NOTE: Change key shape from [B, H, T, D_h] to [B, H, D_h, T]\n",
    "        # NOTE: The output shape of attn_scores is [B, H, T, T], i.e. a square matrix of size T\n",
    "        #       (i.e. sequence length) for each head.\n",
    "        # NOTE: The following operation does a batched matrix multiplication between queries and\n",
    "        #       keys. In this case, the matrix multiplication implementation in PyTorch handles the\n",
    "        #       four-dimensional input tensor so that the matrix multiplication is carried out\n",
    "        #       between the two last dimensions (num_tokens, head_dim) and then repeated for the\n",
    "        #       individual heads.\n",
    "        # NOTE: See section \"A note on batched matrix multiplications\" for more details.\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # shape [B, H, T, T]\n",
    "        print(f\"attn_scores.shape: {attn_scores.shape}\")\n",
    "\n",
    "        # The mask is truncated to the number of tokens in the input sequence (i.e. sequence\n",
    "        # length T)\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]  # shape [T, T]\n",
    "        print(f\"mask_bool.shape: {mask_bool.shape}\")\n",
    "\n",
    "        # Apply the mask to the attention scores.\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)  # shape [B, H, T, T]\n",
    "\n",
    "        # Compute the normalized attention weights (as before)\n",
    "        # NOTE: The scaling factor is the last dimension of the keys tensor (i.e. head_dim, see\n",
    "        #       line 88).\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)  # shape [B, H, T, T]\n",
    "        print(f\"attn_weights.shape: {attn_weights.shape}\")\n",
    "\n",
    "        # Compute the context vectors.\n",
    "        # NOTE: The shapes of the individual tensors are:\n",
    "        #       - attn_weights: [B, H, T, T]\n",
    "        #       - values: [B, H, T, D_h]\n",
    "        #       - context_vec: [B, H, T, D_h]\n",
    "        #       - context_vec.transposed: [B, T, H, D_h]\n",
    "        # NOTE: The context vectors from all heads are transposed back to the shape\n",
    "        #       (b, num_tokens, num_heads, head_dim).\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)  # shape [B, T, H, D_h]\n",
    "        print(f\"context_vec.shape: {context_vec.shape}\")\n",
    "\n",
    "        # Reshape/flatten the context vectors from [B, T, H, D_h] to [B, T, D_out], i.e. combine all\n",
    "        # individual attention heads (d_out = H * D_h).\n",
    "        # NOTE: Combines heads, where self.d_out = self.num_heads * self.head_dim (see line 32).\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        print(f\"context_vec.shape: {context_vec.shape}\")\n",
    "\n",
    "        # Optionally project the context vectors to the output dimension.\n",
    "        # NOTE: This output projection layer is not strictly necessary (see appendix B of the book\n",
    "        #       for more details), but it is commonly used in many LLM architectures.\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "        return context_vec\n",
    "\n",
    "\n",
    "# Test with a simple example.\n",
    "torch.manual_seed(123)\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 2\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A note on views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n",
      "torch.Size([2, 3, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "# Example for reshaping a tensor from [2, 3, 4] to [2, 3, 2, 2] (via views)\n",
    "B, T, D = 2, 3, 4\n",
    "x = torch.randn((B, T, D))\n",
    "print(x.shape)\n",
    "x_view = x.view(B, T, 2, 2)\n",
    "print(x_view.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A note on batched matrix multiplications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of aat: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[1.3208, 1.1631, 1.2879],\n",
      "          [1.1631, 2.2150, 1.8424],\n",
      "          [1.2879, 1.8424, 2.0402]],\n",
      "\n",
      "         [[0.4391, 0.7003, 0.5903],\n",
      "          [0.7003, 1.3737, 1.0620],\n",
      "          [0.5903, 1.0620, 0.9912]]]])\n"
     ]
    }
   ],
   "source": [
    "# The shape of this tensor is (b, num_heads, num_tokens, head_dim) = (1, 2, 3, 4).\n",
    "a = torch.tensor(\n",
    "    [\n",
    "        [\n",
    "            [\n",
    "                [0.2745, 0.6584, 0.2775, 0.8573],\n",
    "                [0.8993, 0.0390, 0.9268, 0.7388],\n",
    "                [0.7179, 0.7058, 0.9156, 0.4340],\n",
    "            ],\n",
    "            [\n",
    "                [0.0772, 0.3565, 0.1479, 0.5331],\n",
    "                [0.4066, 0.2318, 0.4545, 0.9737],\n",
    "                [0.4606, 0.5159, 0.4220, 0.5786],\n",
    "            ],\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Perform a batched matrix multiplication between a and a.transpose(2, 3), i.e. num_tokens and\n",
    "# head_dim are transposed.\n",
    "# NOTE: [1, 2, 3, 4] @ [1, 2, 4, 3] = [1, 2, 3, 3]\n",
    "# NOTE: In this case, the matrix multiplication implementation in PyTorch handles the\n",
    "#       four-dimensional input tensor so that the matrix multiplication is carried out between the\n",
    "#       two last dimensions (num_tokens, head_dim) and then repeated for the individual heads (as\n",
    "#       well as for each batch separately, i.e. the first dimension which here is just one element).\n",
    "aat = a @ a.transpose(2, 3)\n",
    "print(f\"Shape of aat: {aat.shape}\")\n",
    "print(aat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First head:\n",
      " tensor([[1.3208, 1.1631, 1.2879],\n",
      "        [1.1631, 2.2150, 1.8424],\n",
      "        [1.2879, 1.8424, 2.0402]])\n",
      "\n",
      "Second head:\n",
      " tensor([[0.4391, 0.7003, 0.5903],\n",
      "        [0.7003, 1.3737, 1.0620],\n",
      "        [0.5903, 1.0620, 0.9912]])\n"
     ]
    }
   ],
   "source": [
    "# A less compact version of the above operation is as follows:\n",
    "first_head = a[0, 0, :, :]\n",
    "first_res = first_head @ first_head.T\n",
    "print(\"First head:\\n\", first_res)\n",
    "\n",
    "second_head = a[0, 1, :, :]\n",
    "second_res = second_head @ second_head.T\n",
    "print(\"\\nSecond head:\\n\", second_res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avpc-off-vehicle-qa-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
