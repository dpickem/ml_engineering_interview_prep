{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader Creation (chapter 2)\n",
    "\n",
    "This notebook explores dataset/dataloader creation techniques based on Sebastian Raschka's book\n",
    "(Chapter 2), implementing data sampling, batching, and other techniques.\n",
    "\n",
    "## Dataset\n",
    "Borrowed from [Manning's Live Books](https://livebook.manning.com/wiki/categories/llm/dataset)\n",
    "\n",
    "## Acknowledgment\n",
    "\n",
    "All concepts, architectures, and implementation approaches are credited to Sebastian Raschka's work. This repository serves as my personal implementation and notes while working through the book's content.\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Sebastian Raschka's GitHub](https://github.com/rasbt)\n",
    "- [Book Information](https://www.manning.com/books/build-a-large-language-model-from-scratch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Any, Dict, List, Tuple\n",
    "import urllib.request\n",
    "\n",
    "from importlib.metadata import version\n",
    "import tiktoken\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.7.0\n"
     ]
    }
   ],
   "source": [
    "# Verify library versions.\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))  # expected: 0.7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20479\n",
      "Total number of tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "# Download sample data to a text file.\n",
    "local_filename = \"data/the_verdict.txt\"\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "    \"the-verdict.txt\"\n",
    ")\n",
    "urllib.request.urlretrieve(url, local_filename)\n",
    "\n",
    "# Read the text file into a string.\n",
    "with open(local_filename, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "print(\"Total number of character:\", len(raw_text))\n",
    "\n",
    "# Encode the text.\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(\"Total number of tokens:\", len(enc_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample some text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [290, 4920, 2241, 287]\n",
      "y:      [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "# Skip the first 50 tokens (for a slightly more interesting example).\n",
    "enc_sample = enc_text[50:]\n",
    "\n",
    "# Show tokens in a context window of size 4.\n",
    "# NOTE: The context size determines how many tokens are included in the input.\n",
    "context_size = 4\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1 : context_size + 1]\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and ---->  established\n",
      " and established ---->  himself\n",
      " and established himself ---->  in\n",
      " and established himself in ---->  a\n"
     ]
    }
   ],
   "source": [
    "# Show input-output pairs for a given context size.\n",
    "show_text = True\n",
    "for i in range(1, context_size + 1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    if show_text:\n",
    "        print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))\n",
    "    else:\n",
    "        print(context, \"---->\", desired)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "Datasets are created using a sliding window approach as illustrated below.\n",
    "\n",
    "Note that each sample (input tensor x and target tensor y) contains multiple training samples. For\n",
    "example the first row:\n",
    "\n",
    "- x[0][0] = [\"In\", \"the\", \"heart\", \"of\"]\n",
    "- y[0][0] = [\"the\", \"heart\", \"of\", \"the\"]\n",
    "\n",
    "would map to multiple training samples x -> y as follows:\n",
    "\n",
    "- \"In\" --> \"the\"\n",
    "- \"In the\" --> \"heart\"\n",
    "- \"In the heart\" --> \"of\"\n",
    "- \"In the heart of\" --> \"the\"\n",
    "\n",
    "![Dataset](https://drek4537l1klr.cloudfront.net/raschka/Figures/2-13.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   40,   367,  2885,  1464,  1807,  3619,   402,   271, 10899,  2138]),\n",
       " tensor([  367,  2885,  1464,  1807,  3619,   402,   271, 10899,  2138,   257]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, text: str, tokenizer: Any, max_length: int, stride: int):\n",
    "        # Initialize inputs / outputs.\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Argument validation.\n",
    "        assert stride > 0, \"Stride must be greater than 0.\"\n",
    "        assert max_length > 0, \"max_length must be greater than 0.\"\n",
    "        assert len(text) > 0, \"text must be a non-empty string.\"\n",
    "        assert stride <= max_length, (\n",
    "            \"Stride cannot be larger than max_length, otherwise some input text will \"\n",
    "            \"be skipped during tokenization.\"\n",
    "        )\n",
    "\n",
    "        # Tokenize the entire input text.\n",
    "        token_ids = tokenizer.encode(text)\n",
    "\n",
    "        # Use a sliding window approach to create input/output pairs.\n",
    "        # NOTE: The maximum sequence length produced is determined by 'max_length'.\n",
    "        # NOTE: If stride < max_length, the window (chunks) will overlap.\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i : i + max_length]\n",
    "            target_chunk = token_ids[i + 1 : i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Return the total length of the dataset.\n",
    "\n",
    "        NOTE: This is a required method for PyTorch Datasets.\n",
    "        \"\"\"\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.tensor, torch.tensor]:\n",
    "        \"\"\"Access an item at a given index.\n",
    "\n",
    "        NOTE: This is a required method for PyTorch Datasets.\n",
    "        \"\"\"\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "\n",
    "dataset = GPTDatasetV1(text=raw_text, tokenizer=tokenizer, max_length=10, stride=5)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader\n",
    "\n",
    "Before being able to turn tokens into embeddings, we need to implement an efficient dataloader.\n",
    "\n",
    "A dataloader provides the following functionality:\n",
    "\n",
    "- Efficient iteration over the dataset\n",
    "- Batching and shuffling functionality\n",
    "\n",
    "Internally, the dataloader will operate on tensors that contain tokens (and not the raw text).\n",
    "A sample returned from the dataloader will contain an input tensor (a sequence of tokens) and a\n",
    "target tensor (the next word in the sequence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(\n",
    "    text: str,\n",
    "    batch_size: int = 4,\n",
    "    max_length: int = 256,\n",
    "    stride: int = 128,\n",
    "    shuffle: bool = True,\n",
    "    drop_last: bool = True,\n",
    "    num_workers: bool = 0,\n",
    "):\n",
    "    \"\"\"Create a dataloader from a given text.\n",
    "\n",
    "    Args:\n",
    "        text: The input data to create a dataset from.\n",
    "        batch_size: The number of samples to group into a batch.\n",
    "        max_length: The maximum number of tokens to store in each sample.\n",
    "        stride: The stride determines how far we slide the sliding window at each sampling step. A\n",
    "                stride of 1 slides the window by one token. A stride < max_length results in\n",
    "                overlapping input samples while a stride > max_length leads to input tokens being\n",
    "                skipped (i.e. not be contained in any input sample). A stride == max_length utilizes\n",
    "                the data fully without any overlap or skipping (overlapping samples can lead to\n",
    "                increased overfitting during model training).\n",
    "        shuffle:\n",
    "        drop_last: If set, the last batch of data is dropped if it is shorter than the specified\n",
    "                   batch size (to prevent loss spikes during training).\n",
    "        num_workers: The number of CPU processes to use for data preprocessing.\n",
    "    \"\"\"\n",
    "    # Instantiate a tokenizer.\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create the dataset from the input text.\n",
    "    dataset = GPTDatasetV1(\n",
    "        text=text, tokenizer=tokenizer, max_length=max_length, stride=stride\n",
    "    )\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
     ]
    }
   ],
   "source": [
    "# Test dataloader creation.\n",
    "with open(local_filename, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "dataloader = create_dataloader_v1(\n",
    "    text=raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token embeddings\n",
    "\n",
    "Converting tokens to token embeddings requires the initialization of embedding weights. Most commonly,\n",
    "these embedding weights are just initialized with random values. These weights are optimized during\n",
    "LLM training.\n",
    "\n",
    "A token embedding layer can be thought of as a trainable lookup table that maps a vocabulary of\n",
    "size N to vectors of embedding dimension T. Each token that is part of the vocabulary is mapped to\n",
    "a vector of dimension T.\n",
    "\n",
    "For reference, the vocabulary of the BPE tokenizer is of size 50,257. The embedding dimension of\n",
    "GPT-3 is of size 12,288.\n",
    "\n",
    "![Token embedding](https://drek4537l1klr.cloudfront.net/raschka/Figures/2-15.png)\n",
    "\n",
    "Note that embedding layers can be thought of as a more efficient implementation of one-hot encoding\n",
    "followed by matrix multiplication in a fully connected layer (see [supplementary code on \"Understanding the Difference Between Embedding Layers and Linear Layers\"](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/03_bonus_embedding-vs-matmul/embeddings-and-linear-layers.ipynb))\n",
    "\n",
    "![Token embedding example](https://drek4537l1klr.cloudfront.net/raschka/Figures/2-16.png)\n",
    "\n",
    "Embedding layers perform a lookup operation (as shown above), retrieving the embedding vector\n",
    "corresponding to the token ID from the embedding layer’s weight matrix. For instance, the embedding\n",
    "vector of the token ID 5 is the sixth row of the embedding layer weight matrix (it is the sixth\n",
    "instead of the fifth row because Python starts counting at 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n",
      "torch.Size([6, 3])\n",
      "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n",
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Sample embedding.\n",
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "\n",
    "# The embedding layer is of shape 6 x 3 which maps a vocabulary of size 6 to an embedding space of\n",
    "# dimension 3. The weight matrix, therefore, has six rows and three columns which means there is one\n",
    "# row in the matrix for each token in the vocabulary.\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "print(embedding_layer.weight)\n",
    "print(embedding_layer.weight.shape)\n",
    "\n",
    "# Apply the embedding layer to a single token ID.\n",
    "# NOTE: This is essentially a lookup operation that retrieves a given row from the embedding\n",
    "#       layer's weight matrix.\n",
    "print(embedding_layer(torch.tensor([3])))\n",
    "\n",
    "# Apply the embedding layer to multiple token IDs in batch.\n",
    "print(embedding_layer(torch.tensor([2, 3, 5, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Position embeddings\n",
    "\n",
    "The embedding layer has no notion of position or order of tokens within a sequence. The same token ID\n",
    "always gets mapped to the same vector representation regardless of position in the sequence. That\n",
    "shortcoming is commonly addressed via position embeddings that add are added to the token embedding.\n",
    "\n",
    "![Issues with token embeddings](https://drek4537l1klr.cloudfront.net/raschka/Figures/2-17.png)\n",
    "\n",
    "The embedding layer converts a token ID into the same vector representation regardless of where it\n",
    "is located in the input sequence. For example, the token ID 5, whether it’s in the first or fourth\n",
    "position in the token ID input vector, will result in the same embedding vector.\n",
    "\n",
    "Position embeddings fall into one of two categories:\n",
    "\n",
    "1. **relative position embeddings**: these encode the relative distance between tokens (or \"how far apart\" tokens are). The advantage is that models can generalize better to sequences of varying lengths.\n",
    "2. **absolute position embeddings**: associated with a specific position in the sequence. For each position in the input sequence, a unique embedding is added to the token embedding.\n",
    "\n",
    "![Absolute position embeddings](https://drek4537l1klr.cloudfront.net/raschka/Figures/2-18.png)\n",
    "\n",
    "In the above example of absolute positional embeddings are added to the token embedding vector to\n",
    "create the input embeddings for an LLM. The positional vectors have the same dimension as the original\n",
    "token embeddings.\n",
    "\n",
    "For reference, OpenAI's GPT uses absolute positional encodings that are optimized during training\n",
    "rather than defined in a fixed manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "Inputs shape: torch.Size([8, 4])\n",
      "Embedding shape: torch.Size([8, 4, 256])\n",
      "Position embedding layer shape: torch.Size([4, 256])\n",
      "Input embeddings shape: torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "# A full example of token embedding + absolute positional embedding.\n",
    "\n",
    "# Create an embedding layer.\n",
    "# NOTE: The vocab size is that of the BPE tokenizer.\n",
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "\n",
    "# Create a dataloader.\n",
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=max_length, stride=max_length, shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(f\"Token IDs: {inputs}\")\n",
    "print(f\"Inputs shape: {inputs.shape}\")\n",
    "\n",
    "# Embed inputs.\n",
    "# NOTE: Each input token is mapped to an embedding vector of dimension 256.\n",
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(f\"Embedding shape: {token_embeddings.shape}\")\n",
    "\n",
    "# Create an absolute position embedding layer.\n",
    "# NOTE: The embedding layer has shape 4 x 256, i.e. the same embeddings will be added to each\n",
    "#       batch of data.\n",
    "# NOTE: The input to the pos_embeddings is usually a placeholder vector torch.arange(context_length),\n",
    "#       which contains a sequence of numbers 0, 1, ..., up to the maximum input length –1. The\n",
    "#       context_length is a variable that represents the supported input size of the LLM.\n",
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "print(f\"Position embedding layer shape: {pos_embeddings.shape}\")\n",
    "\n",
    "# Add position embeddings to token embeddings.\n",
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(f\"Input embeddings shape: {input_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of a full input embedding pipeline\n",
    "\n",
    "As part of the input processing pipeline, input text is first broken\n",
    "up into individual tokens. These tokens are then converted into token IDs using a\n",
    "vocabulary. The token IDs are converted into embedding vectors to which positional\n",
    "embeddings of a similar size are added, resulting in input embeddings that are used\n",
    "as input for the main LLM layers.\n",
    "\n",
    "![Full embedding pipeline](https://drek4537l1klr.cloudfront.net/raschka/Figures/2-19.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
