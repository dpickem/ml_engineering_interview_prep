{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning to follow instructions (chapter 7)\n",
    "\n",
    "This notebook explores the fine-tuning process of LLMs with the purpose of creating instruction fine-tuned model based on Sebastian Raschka's book (Chapter 7). In particular, it discusses the following:\n",
    "\n",
    "- The instruction fine-tuning process of LLMs\n",
    "- Preparing a dataset for supervised instruction fine-tuning\n",
    "- Organizing instruction data in training batches\n",
    "- Loading a pretrained LLM and fine-tuning it to follow human instructions\n",
    "- Extracting LLM-generated instruction responses for evaluation\n",
    "- Evaluating an instruction-fine-tuned LLM\n",
    "\n",
    "## Acknowledgment\n",
    "\n",
    "All concepts, architectures, and implementation approaches are credited to Sebastian Raschka's work.  This repository serves as my personal implementation and notes while working through the book's content.\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Sebastian Raschka's GitHub](https://github.com/rasbt)\n",
    "- [Book Information](https://www.manning.com/books/build-a-large-language-model-from-scratch)\n",
    "    - [Chapter 7](https://livebook.manning.com/book/build-a-large-language-model-from-scratch/chapter-7)\n",
    "\n",
    "![Topic overview](https://drek4537l1klr.cloudfront.net/raschka/Figures/7-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "import json\n",
    "import functools\n",
    "import os\n",
    "import pathlib\n",
    "import psutil\n",
    "from pprint import pprint\n",
    "import time\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "import urllib.request\n",
    "import urllib\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import tiktoken\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Import previous chapter dependencies.\n",
    "# See https://stackoverflow.com/questions/44116194/import-a-function-from-another-ipynb-file\n",
    "# NOTE: Importing these functions seems to run the entire cell the symbol is defined in, which would\n",
    "#       suggest that symbols should be defined in separate cells from the test code.\n",
    "# NOTE: Importing another ipynb file basically runs the entire imported notebook.\n",
    "import import_ipynb\n",
    "from gpt_download import download_and_load_gpt2\n",
    "\n",
    "# Chapter 4 dependencies.\n",
    "from chapter_04_gpt_from_scratch import (\n",
    "    GPTConfig,\n",
    "    GPTModel,\n",
    ")\n",
    "\n",
    "# Chapter 5 dependencies.\n",
    "from chapter_05_pretraining_on_unlabeled_data import (\n",
    "    generate,\n",
    "    token_ids_to_text,\n",
    "    text_to_token_ids,\n",
    "    load_weights_into_gpt,\n",
    "    calc_loss_loader,\n",
    "    train_model_simple,\n",
    "    plot_losses,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base config.\n",
    "GPT_CONFIG_124M = GPTConfig(\n",
    "    vocab_size=50257,  # as used by the BPE tokenizer for GPT-2.\n",
    "    context_length=1024,\n",
    "    emb_dim=768,\n",
    "    n_heads=12,\n",
    "    n_layers=12,\n",
    "    dropout_rate=0.0,  # disable dropout for inference\n",
    "    qkv_bias=False,\n",
    ")\n",
    "\n",
    "# Determine the device to run the model on.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1: Preparing the dataset\n",
    "\n",
    "We now know that pretraining an LLM involves a training procedure where it learns to generate one word at a time. The resulting pretrained LLM is capable of text completion, meaning it can finish sentences or write text paragraphs given a fragment as input.\n",
    "\n",
    "Here, we focus on improving the LLM’s ability to follow such instructions and generate a desired response. Preparing the dataset is a key aspect of instruction fine-tuning.\n",
    "\n",
    "![Dataset preparation](https://drek4537l1klr.cloudfront.net/raschka/Figures/7-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and load the dataset\n",
    "\n",
    "The dataset consists of 1,100 instruction–response pairs. This dataset was created specifically for this book. The following code implements and executes a function to download this dataset, which is a relatively small file (only 204 KB) in JSON forma.\n",
    "\n",
    "As we can see, the example entries are Python dictionary objects containing an ```instruction```, ```input```, and ```output```.\n",
    "\n",
    "The ```input``` field may occasionally be empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_load_file(file_path: pathlib.Path, url: str) -> Dict[str, Any]:\n",
    "    \"\"\"Download and load a file from a URL.\n",
    "\n",
    "    Args:\n",
    "        file_path: The path to the file to download.\n",
    "        url: The URL to download the file from.\n",
    "\n",
    "    Returns:\n",
    "        The loaded data.\n",
    "    \"\"\"\n",
    "    # Skips download if file was already downloaded\n",
    "    if not os.path.exists(file_path):\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            text_data = response.read().decode(\"utf-8\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(text_data)\n",
    "    else:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text_data = file.read()\n",
    "\n",
    "    # Load and decode the data from the file.\n",
    "    with open(file_path, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "file_path = pathlib.Path(\"data/instruction-data.json\")\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch\"\n",
    "    \"/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
    ")\n",
    "data = download_and_load_file(file_path, url)\n",
    "print(\"Number of entries:\", len(data))\n",
    "pprint(data[50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt formatting\n",
    "\n",
    "Instruction fine-tuning involves training a model on a dataset where the input-output pairs, like those we extracted from the JSON file, are explicitly provided. There are various methods to format these entries for LLMs.\n",
    "\n",
    "There are various methods to format these entries for LLMs, often referred to as prompt styles. The most commonly used ones are the following:\n",
    "\n",
    "- **Alpaca** prompt style\n",
    "- **Phi-3** prompt style\n",
    "\n",
    "Alpaca was one of the early LLMs to publicly detail its instruction fine-tuning process. Phi-3, developed by Microsoft, is included to demonstrate the diversity in prompt styles. The rest of this notebook uses the Alpaca prompt style since it is one of the most popular ones, largely because it helped define the original approach to fine-tuning.\n",
    "\n",
    "![Prompt styles](https://drek4537l1klr.cloudfront.net/raschka/Figures/7-4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input(entry: Dict[str, Any], add_output: bool = False) -> str:\n",
    "    \"\"\"Format an entry for the Alpaca prompt style.\n",
    "\n",
    "    Args:\n",
    "        entry: A dictionary containing an `instruction` and `input` key.\n",
    "        add_output: Whether to add the `output` key to the formatted string.\n",
    "\n",
    "    Returns:\n",
    "        The formatted string.\n",
    "    \"\"\"\n",
    "    # Add the 'system' prompt and the entry's instruction.\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    # Add the entry's input if it exists.\n",
    "    # NOTE: The 'input' section is skipped if the field is empty.\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "\n",
    "    # Optionally add the desired response.\n",
    "    desired_response = f\"\\n\\n### Response:\\n{entry['output']}\" if add_output else \"\"\n",
    "\n",
    "    return instruction_text + input_text + desired_response\n",
    "\n",
    "\n",
    "# Format the example entry.\n",
    "model_input = format_input(data[50], add_output=True)\n",
    "print(model_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the datast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: This section should reuse functions from chapter 6.\n",
    "\n",
    "# Use 85% of the data for training, 10% for testing, and 5% for validation.\n",
    "train_portion = int(len(data) * 0.85)\n",
    "test_portion = int(len(data) * 0.1)\n",
    "val_portion = len(data) - train_portion - test_portion\n",
    "\n",
    "train_data = data[:train_portion]\n",
    "test_data = data[train_portion : train_portion + test_portion]\n",
    "val_data = data[train_portion + test_portion :]\n",
    "\n",
    "print(\"Training set length:\", len(train_data))\n",
    "print(\"Validation set length:\", len(val_data))\n",
    "print(\"Test set length:\", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organizing data into training batches\n",
    "\n",
    "In this section, we learn how to efficiently pad the data samples to equal lengths so we can assemble multiple instruction examples in a batch.\n",
    "\n",
    "In the previous chapter, the training batches were created automatically by the PyTorch DataLoader class, which employs a default collate function to combine lists of samples into batches. A collate function is responsible for taking a list of individual data samples and merging them into a single batch that can be processed efficiently by the model during training. Here, we create our own custom collate function to handle specific requirements and formatting (pre-tokenization and formatting of inputs) of our instruction dataset.\n",
    "\n",
    "![Batching overview](https://drek4537l1klr.cloudfront.net/raschka/Figures/7-6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataset class\n",
    "\n",
    "Similar to the approach used for classification fine-tuning, we want to accelerate training by collecting multiple training examples in a batch, which necessitates padding all inputs to a similar length. As with classification fine-tuning, we use the <|endoftext|> token as a padding token.\n",
    "\n",
    "![Prompt formatting and tokenization](https://drek4537l1klr.cloudfront.net/raschka/Figures/7-7.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstructionDataset(Dataset):\n",
    "    \"\"\"Dataset class for instruction fine-tuning.\"\"\"\n",
    "\n",
    "    def __init__(self, data: List[Dict[str, Any]], tokenizer: tiktoken.Encoding):\n",
    "        # Cache the raw and encoded texts.\n",
    "        self.data = data\n",
    "        self.encoded_texts = []\n",
    "\n",
    "        # Pretokenizes texts\n",
    "        for entry in data:\n",
    "            full_text = format_input(entry=entry, add_output=True)\n",
    "            self.encoded_texts.append(tokenizer.encode(full_text))\n",
    "\n",
    "    def __getitem__(self, index: int) -> List[int]:\n",
    "        return self.encoded_texts[index]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "dataset = InstructionDataset(train_data, tokenizer)\n",
    "print(f\"Length of dataset: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of appending the <|endoftext|> tokens to the text inputs, we can append the token ID\n",
    "# corresponding to <|endoftext|> to the pretokenized inputs directly. We can use the tokenizer’s\n",
    "# .encode method on an <|endoftext|> token to remind us which token ID we should use:\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom collate function\n",
    "\n",
    "This custom collate function pads the training examples in each batch to the same length while allowing different batches to have different lengths, as demonstrated in the figure below. This approach minimizes unnecessary padding by only extending sequences to match the longest one in each batch, not the whole dataset.\n",
    "\n",
    "![Custom collate function](https://drek4537l1klr.cloudfront.net/raschka/Figures/7-8.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_draft_1(\n",
    "    batch: List[List[int]], pad_token_id: int = 50256, device: str = \"cpu\"\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Custom collate function for instruction fine-tuning.\n",
    "\n",
    "    Args:\n",
    "        batch: A list of lists of integers representing the training examples.\n",
    "        pad_token_id: The token ID to use for padding.\n",
    "        device: The device to move the resulting tensor to.\n",
    "\n",
    "    Returns:\n",
    "        A tensor of the padded inputs.\n",
    "    \"\"\"\n",
    "    # Find the longest sequence in the batch.\n",
    "    batch_max_length = max(len(item) + 1 for item in batch)\n",
    "\n",
    "    # Pad and prepare the inputs.\n",
    "    inputs_lst = []\n",
    "    for item in batch:\n",
    "        # Copy the item and append a single padding token.\n",
    "        new_item = item.copy()\n",
    "        new_item += [pad_token_id]\n",
    "\n",
    "        # Pad the sequence to the longest sequence in the batch.\n",
    "        padded = new_item + [pad_token_id] * (batch_max_length - len(new_item))\n",
    "\n",
    "        # Convert the padded sequence to a tensor, remove the extra padded token added earlier, and\n",
    "        # add it to the list of inputs.\n",
    "        # NOTE: The purpose of this will become clear later in the second draft of this collate\n",
    "        #       function.\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        inputs_lst.append(inputs)\n",
    "\n",
    "    # Stack the inputs into a single tensor and move it to the specified device.\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    return inputs_tensor\n",
    "\n",
    "\n",
    "# Test the custom collate function.\n",
    "# NOTE: This output shows all inputs have been padded to the length of the longest input list,\n",
    "#       inputs_1, containing five token IDs.\n",
    "inputs_1 = [0, 1, 2, 3, 4]\n",
    "inputs_2 = [5, 6]\n",
    "inputs_3 = [7, 8, 9]\n",
    "batch = (\n",
    "    inputs_1,\n",
    "    inputs_2,\n",
    "    inputs_3,\n",
    ")\n",
    "print(custom_collate_draft_1(batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding target tokens to the collate function\n",
    "\n",
    "We also need to create batches with the target token IDs corresponding to the batch of input IDs. These target IDs, as shown in the figure below, are crucial because they represent what we want the model to generate and what we need during training to calculate the loss for the weight updates. That is, we modify our custom collate function to return the target token IDs in addition to the input token IDs.\n",
    "\n",
    "![Target tokens in custom collate function](https://drek4537l1klr.cloudfront.net/raschka/Figures/7-9.png)\n",
    "\n",
    "Similar to the process we used to pretrain an LLM, the target token IDs match the input token IDs but are shifted one position to the right. \n",
    "\n",
    "![Target tokens in custom collate function](https://drek4537l1klr.cloudfront.net/raschka/Figures/7-10.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_draft_2(\n",
    "    batch: List[List[int]], pad_token_id: int = 50256, device: str = \"cpu\"\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Custom collate function for instruction fine-tuning.\n",
    "\n",
    "    Args:\n",
    "        batch: A list of lists of integers representing the training examples.\n",
    "        pad_token_id: The token ID to use for padding.\n",
    "        device: The device to move the resulting tensor to.\n",
    "\n",
    "    Returns:\n",
    "        A tuple of tensors of the padded inputs and targets.\n",
    "    \"\"\"\n",
    "    # Find the longest sequence in the batch.\n",
    "    batch_max_length = max(len(item) + 1 for item in batch)\n",
    "\n",
    "    # Initialize the outputs.\n",
    "    inputs_lst, targets_lst = [], []\n",
    "\n",
    "    # Pad and prepare the inputs.\n",
    "    for item in batch:\n",
    "        # Copy the item and append a single padding token.\n",
    "        new_item = item.copy()\n",
    "        new_item += [pad_token_id]\n",
    "\n",
    "        # Pad the sequence to the longest sequence in the batch.\n",
    "        padded = new_item + [pad_token_id] * (batch_max_length - len(new_item))\n",
    "\n",
    "        # Truncates the last token for inputs.\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        # Shifts +1 to the right for targets.\n",
    "        targets = torch.tensor(padded[1:])\n",
    "\n",
    "        # Append the inputs and targets to the lists.\n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "\n",
    "    # Stack the inputs into a single tensor and move it to the specified device.\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "    return inputs_tensor, targets_tensor\n",
    "\n",
    "\n",
    "# Test the custom collate function.\n",
    "inputs, targets = custom_collate_draft_2(batch)\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace padding tokens with -100\n",
    "\n",
    "We assign a -100 placeholder value to all padding tokens. This special value allows us to exclude these padding tokens from contributing to the training loss calculation, ensuring that only meaningful data influences model learning.\n",
    "\n",
    "The default setting of the cross entropy function in PyTorch is ```cross_entropy(..., ignore_index=-100)```. This means that it ignores targets labeled with -100.\n",
    "\n",
    "However, note that we retain one end-of-text token, ID 50256, in the target list. Retaining it allows the LLM to learn when to generate an end-of-text token in response to instructions, which we use as an indicator that the generated response is complete.\n",
    "\n",
    "![Padding tokens](https://drek4537l1klr.cloudfront.net/raschka/Figures/7-12.png)\n",
    "\n",
    "In addition to masking out padding tokens, it is also common to mask out the target token IDs that correspond to the instruction, as illustrated in figure 7.13. By masking out the LLM’s target token IDs corresponding to the instruction, the cross entropy loss is only computed for the generated response target IDs. Thus, the model is trained to focus on generating accurate responses rather than memorizing instructions, which can help reduce overfitting.\n",
    "\n",
    "![Masking out instructions](https://drek4537l1klr.cloudfront.net/raschka/Figures/7-13.png)\n",
    "\n",
    "As of this writing, researchers are divided on whether masking the instructions is universally beneficial during instruction fine-tuning. For instance, the 2024 paper by Shi et al., [Instruction Tuning With Loss Over Instructions](https://arxiv.org/abs/2405.14394), demonstrated that not masking the instructions benefits the LLM performance (see appendix B for more details). Here, we will not apply instruction masking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(\n",
    "    batch: List[List[int]],\n",
    "    pad_token_id: int = 50256,\n",
    "    ignore_index=-100,\n",
    "    allowed_max_length=None,\n",
    "    device: str = \"cpu\",\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Custom collate function for instruction fine-tuning.\n",
    "\n",
    "    Args:\n",
    "        batch: A list of lists of integers representing the training examples.\n",
    "        pad_token_id: The token ID to use for padding.\n",
    "        ignore_index: The value to use for padding tokens.\n",
    "        allowed_max_length: The maximum length of the input sequences.\n",
    "        device: The device to move the resulting tensor to.\n",
    "\n",
    "    Returns:\n",
    "        A tuple of tensors of the padded inputs and targets.\n",
    "    \"\"\"\n",
    "    # Find the longest sequence in the batch.\n",
    "    batch_max_length = max(len(item) + 1 for item in batch)\n",
    "\n",
    "    # Initialize the outputs.\n",
    "    inputs_lst, targets_lst = [], []\n",
    "\n",
    "    # Pad and prepare the inputs.\n",
    "    for item in batch:\n",
    "        # Copy the item and append a single padding token.\n",
    "        new_item = item.copy()\n",
    "        new_item += [pad_token_id]\n",
    "\n",
    "        # Pad the sequence to the longest sequence in the batch.\n",
    "        padded = new_item + [pad_token_id] * (batch_max_length - len(new_item))\n",
    "\n",
    "        # Truncates the last token for inputs.\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        # Shifts +1 to the right for targets.\n",
    "        targets = torch.tensor(padded[1:])\n",
    "\n",
    "        # Replaces all but the first padding tokens in targets by ignore_index.\n",
    "        mask = targets == pad_token_id\n",
    "        indices = torch.nonzero(mask).squeeze()\n",
    "        if indices.numel() > 1:\n",
    "            targets[indices[1:]] = ignore_index\n",
    "\n",
    "        # Optionally truncates to the maximum sequence length.\n",
    "        if allowed_max_length is not None:\n",
    "            inputs = inputs[:allowed_max_length]\n",
    "            targets = targets[:allowed_max_length]\n",
    "\n",
    "        # Append the inputs and targets to the lists.\n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "\n",
    "    # Stack the inputs into a single tensor and move it to the specified device.\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "    return inputs_tensor, targets_tensor\n",
    "\n",
    "\n",
    "# Test the custom collate function.\n",
    "inputs, targets = custom_collate_fn(batch)\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "customized_collate_fn = functools.partial(\n",
    "    custom_collate_fn, device=device, allowed_max_length=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can try to increase this number if parallel Python processes are supported by your operating\n",
    "# system.\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "# Set the seed for reproducibility.\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# Create the datasets.\n",
    "train_dataset = InstructionDataset(train_data, tokenizer)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "val_dataset = InstructionDataset(val_data, tokenizer)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "test_dataset = InstructionDataset(test_data, tokenizer)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "\n",
    "# Print the shape of all batches in the train loader.\n",
    "# NOTE: Each batch contains 8 examples but the length of the sequences can vary from batch to batch.\n",
    "print(f\"Train loader (length: {len(train_loader)}):\")\n",
    "for inputs, targets in train_loader:\n",
    "    print(inputs.shape, targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2 - Fine-tuning the LLM\n",
    "\n",
    "Before beginning instruction fine-tuning, we must first load a pretrained GPT model that we want to fine-tune, a process we have undertaken previously. However, instead of using the smallest 124-million-parameter model as before, we load the medium-sized model with 355 million parameters. The reason for this choice is that the 124-million-parameter model is too limited in capacity to achieve satisfactory results via instruction fine-tuning. Specifically, smaller models lack the necessary capacity to learn and retain the intricate patterns and nuanced behaviors required for high-quality instruction-following tasks.\n",
    "\n",
    "The following supplementary section in this book’s code repository lists several options for using cloud GPUs: https://mng.bz/EOEq.\n",
    "\n",
    "![Model loading](https://drek4537l1klr.cloudfront.net/raschka/Figures/7-15.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a pre-trained LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the base config.\n",
    "GPT_CONFIG = GPTConfig(\n",
    "    vocab_size=50257,  # as used by the BPE tokenizer for GPT-2.\n",
    "    context_length=1024,\n",
    "    emb_dim=768,\n",
    "    n_heads=12,\n",
    "    n_layers=12,\n",
    "    dropout_rate=0.0,  # disable dropout for inference\n",
    "    qkv_bias=False,\n",
    ")\n",
    "\n",
    "# Update the model configuration to conform to the model size.\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "# Instantiate a base config.\n",
    "tmp_config = dataclasses.asdict(GPT_CONFIG)\n",
    "\n",
    "# Load the overlay parameters.\n",
    "model_name = \"gpt2-medium (355M)\"\n",
    "tmp_config.update(model_configs[model_name])\n",
    "\n",
    "# Update the context length to match OpenAI's GPT-2 models.\n",
    "tmp_config.update({\"context_length\": 1024})\n",
    "\n",
    "# OpenAI used bias vectors in the multi-head attention module’s linear layers to implement the\n",
    "# query, key, and value matrix computations. Bias vectors are not commonly used in LLMs anymore as\n",
    "# they don’t improve the modeling performance and are thus unnecessary. However, since we are\n",
    "# working with pretrained weights, we need to match the settings for consistency and enable these\n",
    "# bias vectors.\n",
    "tmp_config.update({\"qkv_bias\": True})\n",
    "\n",
    "# Instantiate the new configuration.\n",
    "NEW_CONFIG = GPTConfig(**tmp_config)\n",
    "\n",
    "# Download the pretrained weights.\n",
    "model_size = model_name.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "print(f\"Downloading pretrained weights for {model_size} model...\")\n",
    "settings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"gpt2\")\n",
    "\n",
    "# Initialize the model with the new configuration.\n",
    "model = GPTModel(NEW_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check the model outputs on a random example.\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# Print an example from the validation set.\n",
    "input_text = format_input(val_data[0])\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a response from the model.\n",
    "# NOTE: The generate function returns the combined input and output text. This behavior was\n",
    "#       previously convenient since pretrained LLMs are primarily designed as text-completion\n",
    "#       models, where the input and output are concatenated to create coherent and legible\n",
    "#       text. However, when evaluating the model’s performance on a specific task, we often\n",
    "#       want to focus solely on the model’s generated response.\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(input_text, tokenizer),\n",
    "    max_new_tokens=35,\n",
    "    context_size=NEW_CONFIG.context_length,\n",
    "    eos_id=50256,\n",
    ")\n",
    "\n",
    "# Print the model's response without the input text (i.e. the instruction).\n",
    "# NOTE: To isolate the model’s response text, we need to subtract the length of the input\n",
    "#       instruction from the start of the generated_text.\n",
    "generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "response_text = generated_text[len(input_text) :].strip()\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruction fine-tuning the LLM\n",
    "\n",
    "\n",
    "- **Exercise 7.3** Fine-tuning on the original Alpaca dataset\n",
    "\n",
    "The Alpaca dataset, by researchers at Stanford, is one of the earliest and most popular openly shared instruction datasets, consisting of 52,002 entries. As an alternative to the instruction-data.json file we use here, consider fine-tuning an LLM on this dataset. The dataset is available at https://mng.bz/NBnE. This dataset contains 52,002 entries, which is approximately 50 times more than those we used here, and most entries are longer. Thus, I highly recommend using a GPU to conduct the training, which will accelerate the fine-tuning process. If you encounter out-of-memory errors, consider reducing the batch_size from 8 to 4, 2, or even 1. Lowering the allowed_max_length from 1,024 to 512 or 256 can also help manage memory problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate baseline train and validation loss (before any fine-tuning).\n",
    "model.to(device)\n",
    "torch.manual_seed(123)\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(\n",
    "        data_loader=train_loader, model=model, device=device, num_batches=5\n",
    "    )\n",
    "    val_loss = calc_loss_loader(\n",
    "        data_loader=val_loader, model=model, device=device, num_batches=5\n",
    "    )\n",
    "\n",
    "print(f\"Training loss: {train_loss}\")\n",
    "print(f\"Validation loss: {val_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note on weight decay\n",
    "\n",
    "AdamW implements weight decay by subtracting a scaled version of the weights from the parameter update, rather than modifying the loss function like L2 regularization. This decoupling of weight decay from the gradient calculation ensures that momentum and adaptive learning rates in Adam are not affected by weight decay, leading to more consistent and effective regularization (see [this post](https://medium.com/@aisagescribe/understanding-adam-and-adamw-advanced-optimization-techniques-in-deep-learning-b2b27ba2b63b) or [this post](https://benihime91.github.io/blog/machinelearning/deeplearning/python3.x/tensorflow2.x/2020/10/08/adamW.html) or [this post](https://www.datacamp.com/tutorial/adamw-optimizer-in-pytorch))\n",
    "\n",
    "1. Weight Decay vs. L2 Regularization:\n",
    "    - **Weight Decay**: Modifies the parameter update step to penalize large weights. It directly subtracts a portion of the weights from the update, effectively shrinking them towards zero. \n",
    "    - **L2 Regularization**: Modifies the loss function by adding a penalty term proportional to the squared magnitude of the weights. This penalty term increases the loss for large weights, making it more difficult for the model to learn large values. \n",
    "2. AdamW's Approach:\n",
    "    - AdamW implements weight decay by directly subtracting a scaled version of the weights from the parameter update, without changing the loss function. \n",
    "    - This ensures that the momentum and adaptive learning rates in Adam, which are crucial for efficient training, are not affected by the weight decay process. \n",
    "3. Mathematical Representation:\n",
    "Let's consider the following:\n",
    "- $\\theta_t$: The weights at iteration $t$. \n",
    "- $\\nabla L(\\theta_t)$: The gradient of the loss function with respect to the weights at iteration $t$. \n",
    "- $\\eta_t$: The learning rate at iteration $t$. \n",
    "- $\\lambda$: The weight decay hyperparameter. \n",
    "\n",
    "**AdamW Update Rule:**\n",
    "$$\\theta_{t+1} = \\theta_t - \\eta_t \\cdot \\nabla L(\\theta_t) - \\eta_t \\cdot \\lambda \\cdot \\theta_t$$\n",
    "\n",
    "**Explanation:**\n",
    "- The first term ($- \\eta_t \\cdot \\nabla L(\\theta_t)$) is the standard gradient descent update. \n",
    "- The second term ($- \\eta_t \\cdot \\lambda \\cdot \\theta_t$) is the weight decay term. It subtracts a portion of the weights ($\\lambda \\cdot \\theta_t$) from the update, scaled by the learning rate ($\\eta_t$). \n",
    "- This direct subtraction ensures that the weights are gradually shrunk towards zero during training.\n",
    "\n",
    "4. Advantages of AdamW:\n",
    "    - Consistent Regularization: AdamW applies weight decay directly to the parameters, ensuring consistent regularization regardless of the magnitude of the gradients. \n",
    "    - Improved Generalization: By effectively shrinking weights towards zero, AdamW can help prevent overfitting and improve the model's ability to generalize to unseen data. \n",
    "    - Better Convergence: AdamW can lead to faster and more stable convergence during training, especially when dealing with large models and datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility.\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# Initialize the optimizer.\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00005, weight_decay=0.1)\n",
    "\n",
    "# Set the number of epochs.\n",
    "num_epochs = 2\n",
    "\n",
    "# Start the timer.\n",
    "start_time = time.time()\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    device,\n",
    "    num_epochs=num_epochs,\n",
    "    eval_freq=5,\n",
    "    eval_iter=5,\n",
    "    start_context=format_input(val_data[0]),\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Calculate the execution time.\n",
    "# NOTE: With an NVIDIA NVIDIA RTX 5000 GPU (32GB VRAM), this should take about 0.70 minutes.\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training/validation loss curves\n",
    "\n",
    "From the loss plot shown in figure 7.17, we can see that the model’s performance on both the training and validation sets improves substantially over the course of training. The rapid decrease in losses during the initial phase indicates that the model quickly learns meaningful patterns and representations from the data. Then, as training progresses to the second epoch, the losses continue to decrease but at a slower rate, suggesting that the model is fine-tuning its learned representations and converging to a stable solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 3 - Evaluating the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting and saving responses\n",
    "\n",
    "This is for evaluating the model's performance on the hold-out dataset which requires generating a response for each input in the test dataset.\n",
    "\n",
    "![Evaluation](https://drek4537l1klr.cloudfront.net/raschka/Figures/7-18.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spot-checking examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spot check a few examples.\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# Iterates over the first three test set samples\n",
    "for entry in test_data[:3]:\n",
    "    # Format the input instructions.\n",
    "    input_text = format_input(entry)\n",
    "\n",
    "    # Generate a response from the model.\n",
    "    token_ids = generate(\n",
    "        model=model,\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=NEW_CONFIG.context_length,\n",
    "        eos_id=50256,\n",
    "    )\n",
    "\n",
    "    # Decode the generated token IDs into text.\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "\n",
    "    # Remove the \"### Response:\" prefix and strip any leading or trailing whitespace.\n",
    "    response_text = (\n",
    "        generated_text[len(input_text) :].replace(\"### Response:\", \"\").strip()\n",
    "    )\n",
    "\n",
    "    print(\"---------- INPUT ----------------------\")\n",
    "    print(input_text)\n",
    "    print(\"---------- CORRECT RESPONSE ------------\")\n",
    "    print(f\"\\nCorrect response:\\n>> {entry['output']}\")\n",
    "    print(\"---------- MODEL RESPONSE --------------\")\n",
    "    print(f\"\\nModel response:\\n>> {response_text.strip()}\")\n",
    "    print(\"-------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate responses for the entire test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate responses for the entire test set.\n",
    "for i, entry in tqdm(enumerate(test_data), total=len(test_data)):\n",
    "    # Generate the response.\n",
    "    input_text = format_input(entry)\n",
    "    token_ids = generate(\n",
    "        model=model,\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=NEW_CONFIG.context_length,\n",
    "        eos_id=50256,\n",
    "    )\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    response_text = (\n",
    "        generated_text[len(input_text) :].replace(\"### Response:\", \"\").strip()\n",
    "    )\n",
    "\n",
    "    # Update the test data with the model's response.\n",
    "    test_data[i][\"model_response\"] = response_text\n",
    "\n",
    "# Save the test data with the model's responses for later use.\n",
    "with open(\"instruction-data-with-response.json\", \"w\") as file:\n",
    "    json.dump(test_data, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Removes white spaces and parentheses from file name.\n",
    "file_name = f\"{re.sub(r'[ ()]', '', model_name) }-sft.pth\"\n",
    "torch.save(model.state_dict(), file_name)\n",
    "print(f\"Model saved as {file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"gpt2-medium355M-sft.pth\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the fine-tuned model\n",
    "\n",
    "This section details the implementation of a method to automate the response evaluation of the fine-tuned LLM using another, larger LLM. To evaluate test set responses in an automated fashion, we utilize an existing instruction-fine-tuned 8-billion-parameter Llama 3 model developed by Meta AI. This model can be run locally using the open source Ollama application (https://ollama.com).\n",
    "\n",
    "**NOTE**: Ollama is an efficient application for running LLMs on a laptop. It serves as a wrapper around the open source llama.cpp library (https://github.com/ggerganov/llama.cpp), which implements LLMs in pure C/C++ to maximize efficiency. However, Ollama is only a tool for generating text using LLMs (inference) and does not support training or fine-tuning LLMs.\n",
    "\n",
    "The 8-billion-parameter Llama 3 model is a very capable LLM that runs locally. However, it’s not as capable as large proprietary LLMs such as GPT-4 offered by OpenAI. For readers interested in exploring how to utilize GPT-4 through the OpenAI API to assess generated model responses, an optional code notebook is available within the supplementary materials accompanying this book at https://mng.bz/BgEv.\n",
    "\n",
    "![Running Ollama](https://drek4537l1klr.cloudfront.net/raschka/Figures/7-20.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions for Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama running: True\n"
     ]
    }
   ],
   "source": [
    "# Utility function to verify Ollama is running.\n",
    "def check_if_running(process_name: str) -> bool:\n",
    "    \"\"\"Check if the specified process is running.\"\"\"\n",
    "    running = False\n",
    "    for proc in psutil.process_iter([\"name\"]):\n",
    "        if process_name in proc.info[\"name\"]:\n",
    "            running = True\n",
    "            break\n",
    "\n",
    "    return running\n",
    "\n",
    "\n",
    "ollama_running = check_if_running(\"ollama\")\n",
    "if not ollama_running:\n",
    "    raise RuntimeError(\"Ollama not running. Launch ollama before proceeding.\")\n",
    "\n",
    "print(\"Ollama running:\", check_if_running(\"ollama\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REST API-based query function for Ollama.\n",
    "LOCAL_HOST_OLLAMA_URL = \"http://localhost:11434/api/chat\"\n",
    "\n",
    "\n",
    "def query_model(prompt: str, model: str = \"llama3\", url: str = LOCAL_HOST_OLLAMA_URL):\n",
    "    \"\"\"Query the Ollama model using the REST API.\n",
    "\n",
    "    Args:\n",
    "        prompt: The prompt to query the model with.\n",
    "        model: The model to query.\n",
    "        url: The URL of the Ollama server.\n",
    "\n",
    "    Returns:\n",
    "        The response from the model.\n",
    "    \"\"\"\n",
    "    # Creates the data payload as a dictionary\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        # Settings for deterministic responses\n",
    "        \"options\": {\n",
    "            \"seed\": 123,\n",
    "            \"temperature\": 0,\n",
    "            \"num_ctx\": 2048,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Converts the dictionary to a JSON-formatted string and encodes it to bytes.\n",
    "    payload = json.dumps(data).encode(\"utf-8\")\n",
    "\n",
    "    # Creates a request object, setting the method to POST and adding necessary headers.\n",
    "    request = urllib.request.Request(url, data=payload, method=\"POST\")\n",
    "    request.add_header(\"Content-Type\", \"application/json\")\n",
    "\n",
    "    # Sends the request and captures the response.\n",
    "    response_data = \"\"\n",
    "    with urllib.request.urlopen(request) as response:\n",
    "        while True:\n",
    "            # Reads the response line by line.\n",
    "            line = response.readline().decode(\"utf-8\")\n",
    "            if not line:\n",
    "                break\n",
    "\n",
    "            # Parses the JSON-formatted line into a dictionary.\n",
    "            response_json = json.loads(line)\n",
    "\n",
    "            # Appends the response content to the response data.\n",
    "            response_data += response_json[\"message\"][\"content\"]\n",
    "\n",
    "    return response_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the REST API call to Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llamas are herbivores, which means they primarily eat plants and plant-based foods. Their diet typically consists of:\n",
      "\n",
      "1. Grasses: Llamas love to graze on various types of grasses, including tall grasses, short grasses, and even weeds.\n",
      "2. Hay: Hay is a staple in a llama's diet. They enjoy eating timothy hay, alfalfa hay, or other types of hay as a source of fiber and nutrients.\n",
      "3. Grains: Llamas may be fed grains like oats, barley, or corn as an energy-rich snack or supplement.\n",
      "4. Fruits and vegetables: Llamas can eat fruits and veggies like apples, carrots, sweet potatoes, and leafy greens like kale or spinach.\n",
      "5. Minerals: Llamas need access to minerals like calcium, phosphorus, and salt to maintain strong bones and overall health.\n",
      "\n",
      "In the wild, llamas might also eat:\n",
      "\n",
      "1. Leaves: They'll munch on leaves from trees and shrubs, like willow or cedar.\n",
      "2. Bark: In some cases, they may eat the bark of certain trees, like aspen or birch.\n",
      "3. Mushrooms: Some species of mushrooms are safe for llamas to eat.\n",
      "\n",
      "In captivity, llama owners typically provide a balanced diet that includes a mix of hay, grains, and fruits/vegetables. It's essential to consult with a veterinarian or experienced llama breeder to determine the best diet for your llama based on its age, size, and health status.\n"
     ]
    }
   ],
   "source": [
    "model = \"llama3\"\n",
    "result = query_model(\"What do Llamas eat?\", model)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score the instruction fine-tuned responses via Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset response:\n",
      ">> The car is as fast as lightning.\n",
      "\n",
      "Model response:\n",
      ">> The car is as fast as a bullet.\n",
      "\n",
      "Score:\n",
      ">> I'd rate the model response \"The car is as fast as a bullet.\" an 85 out of 100.\n",
      "\n",
      "Here's why:\n",
      "\n",
      "* The response uses a simile correctly, comparing the speed of the car to something else (in this case, a bullet).\n",
      "* The comparison is relevant and makes sense, as bullets are known for their high velocity.\n",
      "* The phrase \"as fast as\" is used correctly to introduce the simile.\n",
      "\n",
      "The only reason I wouldn't give it a perfect score is that some people might not immediately think of bullets when they hear \"fast\", whereas lightning is often associated with speed. However, \"as fast as a bullet\" is still a strong and effective simile that effectively conveys the idea that the car is very quick!\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Dataset response:\n",
      ">> The type of cloud typically associated with thunderstorms is cumulonimbus.\n",
      "\n",
      "Model response:\n",
      ">> The type of cloud associated with thunderstorms is a cumulus cloud.\n",
      "\n",
      "Score:\n",
      ">> I'd score this model response as 40 out of 100.\n",
      "\n",
      "Here's why:\n",
      "\n",
      "* The model correctly identifies that thunderstorms are related to clouds (correctly identifying the type of phenomenon).\n",
      "* However, it incorrectly specifies the type of cloud associated with thunderstorms. Cumulus clouds are not typically associated with thunderstorms; cumulonimbus clouds are.\n",
      "* The response lacks precision and accuracy in its description.\n",
      "\n",
      "Overall, while the model attempts to address the question, it provides an incorrect answer, which is why I'd score it as 40 out of 100.\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Dataset response:\n",
      ">> Jane Austen.\n",
      "\n",
      "Model response:\n",
      ">> The author of 'Pride and Prejudice' is Jane Austen.\n",
      "\n",
      "Score:\n",
      ">> I'd rate my own response as 95 out of 100. Here's why:\n",
      "\n",
      "* The response accurately answers the question by naming the author of 'Pride and Prejudice' as Jane Austen.\n",
      "* The response is concise and to the point, providing a clear and direct answer to the question.\n",
      "* There are no grammatical errors or inaccuracies in the response.\n",
      "\n",
      "The only reason I wouldn't give myself a perfect score is that the response is slightly redundant - it's not necessary to rephrase the instruction (\"Name the author...\") in the response itself. A more concise response would simply be \"Jane Austen.\"\n",
      "\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the fine-tuned model and score some examples.\n",
    "\n",
    "for entry in test_data[:3]:\n",
    "    prompt = (\n",
    "        f\"Given the input `{format_input(entry)}` \"\n",
    "        f\"and correct output `{entry['output']}`, \"\n",
    "        f\"score the model response `{entry['model_response']}`\"\n",
    "        f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
    "    )\n",
    "    print(\"\\nDataset response:\")\n",
    "    print(\">>\", entry[\"output\"])\n",
    "    print(\"\\nModel response:\")\n",
    "    print(\">>\", entry[\"model_response\"])\n",
    "    print(\"\\nScore:\")\n",
    "    print(\">>\", query_model(prompt))\n",
    "    print(\"\\n-------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numeric model scoring\n",
    "\n",
    "It’s worth noting that Ollama is not entirely deterministic across operating systems at the time of this writing, which means that the scores you obtain might vary slightly from the previous scores. To obtain more robust results, you can repeat the evaluation multiple times and average the resulting scores.\n",
    "\n",
    "To further improve our model’s performance, we can explore various strategies, such as:\n",
    "- **Adjusting the hyperparameters** during fine-tuning, such as the learning rate, batch size, or number of epochs\n",
    "- **Increasing the size of the training dataset** or diversifying the examples to cover a broader range of topics and styles\n",
    "- **Experimenting with different prompts** or instruction formats to guide the model’s responses more effectively\n",
    "- **Using a larger pretrained model**, which may have greater capacity to capture complex patterns and generate more accurate responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_scores(\n",
    "    json_data: List[Dict[str, Any]], json_key: str, model: str = \"llama3\"\n",
    ") -> List[int]:\n",
    "    \"\"\"Generate scores for a model based on a JSON dataset.\n",
    "\n",
    "    Args:\n",
    "        json_data: The JSON dataset to score.\n",
    "        json_key: The key in the JSON dataset to score.\n",
    "        model: The model to score with.\n",
    "\n",
    "    Returns:\n",
    "        A list of scores.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for entry in tqdm(json_data, desc=\"Scoring entries\"):\n",
    "        # Modified instruction line to only return the numeric score (without any explanation).\n",
    "        prompt = (\n",
    "            f\"Given the input `{format_input(entry)}` \"\n",
    "            f\"and correct output `{entry['output']}`, \"\n",
    "            f\"score the model response `{entry[json_key]}`\"\n",
    "            f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
    "            f\"Respond with the integer number only.\"\n",
    "        )\n",
    "\n",
    "        # Query the model and get the score.\n",
    "        score = query_model(prompt, model)\n",
    "\n",
    "        # Try to convert the score to an integer.\n",
    "        try:\n",
    "            scores.append(int(score))\n",
    "        except ValueError:\n",
    "            print(f\"Could not convert score: {score}\")\n",
    "            continue\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c68ead22146548b4ab2bf5c18a69f642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scoring entries:   0%|          | 0/110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of scores: 110 of 110\n",
      "Average score: 47.81\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores = generate_model_scores(test_data, \"model_response\")\n",
    "print(f\"Number of scores: {len(scores)} of {len(test_data)}\")\n",
    "print(f\"Average score: {sum(scores)/len(scores):.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 4 - Preference fine-tuning with DPO\n",
    "\n",
    "While we covered the most essential steps, there is an optional step that can be performed after instruction fine-tuning: preference fine-tuning. Preference fine-tuning is particularly useful for customizing a model to better align with specific user preferences. If you are interested in exploring this further, see the **04_preference-tuning- with-dpo** folder in this book’s supplementary GitHub repository at https://mng.bz/dZwD.\n",
    "\n",
    "In addition to the main content covered in this book, the GitHub repository also contains a large selection of bonus material that you may find valuable. To learn more about these additional resources, visit the Bonus Material section on the repository’s README page: https://mng.bz/r12g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dd5924b57ae4295b004bd05f74d5019",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26b58e5bac284ba9a0bfae9af5567429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/41 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89d62842aa9a4a56a339b4f98642c833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/6407814 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e09c9da60d574aa1b54a324f95c70d63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\n",
    "    path=\"wikimedia/wikipedia\",\n",
    "    name=\"20231101.en\",\n",
    "    cache_dir=\"~/data/datasets/wikipedia_en\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anarchism is a political philosophy and movement that is skeptical of all justifications for authority and seeks to abolish the institutions it claims maintain unnecessary coercion and hierarchy, typically including nation-states, and capitalism. Anarchism advocates for the replacement of the state with stateless societies and voluntary free associations. As a historically left-wing movement, this reading of anarchism is placed on the farthest left of the political spectrum, usually described as the libertarian wing of the socialist movement (libertarian socialism).\n",
      "\n",
      "Humans have lived in societies without formal hierarchies long before the establishment of states, realms, or empires. With the rise of organised hierarchical bodies, scepticism toward authority also rose. Although traces of anarchist ideas are found all throughout history, modern anarchism emerged from the Enlightenment. During the latter half of the 19th and the first decades of the 20th century, the anarchist movement flourished in most parts of the world and had a significant role in workers' struggles for emancipation. Various anarchist schools of thought formed during this period. Anarchists have taken part in several revolutions, most notably in the Paris Commune, the Russian Civil War and the Spanish Civil War, whose end marked the end of the classical era of anarchism. In the last decades of the 20th and into the 21st century, the anarchist movement has been resurgent once more, growing in popularity and influence within anti-capitalist, anti-war and anti-globalisation movements.\n",
      "\n",
      "Anarchists employ diverse approaches, which may be generally divided into revolutionary and evolutionary strategies; there is significant overlap between the two. Evolutionary methods try to simulate what an anarchist society might be like, but revolutionary tactics, which have historically taken a violent turn, aim to overthrow authority and the state. Many facets of human civilization have been influenced by anarchist theory, critique, and praxis.\n",
      "\n",
      "Etymology, terminology, and definition \n",
      "\n",
      "The etymological origin of anarchism is from the Ancient Greek anarkhia, meaning \"without a ruler\", composed of the prefix an- (\"without\") and the word arkhos (\"leader\" or \"ruler\"). The suffix -ism denotes the ideological current that favours anarchy. Anarchism appears in English from 1642 as anarchisme and anarchy from 1539; early English usages emphasised a sense of disorder. Various factions within the French Revolution labelled their opponents as anarchists, although few such accused shared many views with later anarchists. Many revolutionaries of the 19th century such as William Godwin (1756–1836) and Wilhelm Weitling (1808–1871) would contribute to the anarchist doctrines of the next generation but did not use anarchist or anarchism in describing themselves or their beliefs.\n",
      "\n",
      "The first political philosopher to call himself an anarchist () was Pierre-Joseph Proudhon (1809–1865), marking the formal birth of anarchism in the mid-19th century. Since the 1890s and beginning in France, libertarianism has often been used as a synonym for anarchism and its use as a synonym is still common outside the United States. Some usages of libertarianism refer to individualistic free-market philosophy only, and free-market anarchism in particular is termed libertarian anarchism.\n",
      "\n",
      "While the term libertarian has been largely synonymous with anarchism, its meaning has more recently been diluted by wider adoption from ideologically disparate groups, including both the New Left and libertarian Marxists, who do not associate themselves with authoritarian socialists or a vanguard party, and extreme cultural liberals, who are primarily concerned with civil liberties. Additionally, some anarchists use libertarian socialist to avoid anarchism's negative connotations and emphasise its connections with socialism. Anarchism is broadly used to describe the anti-authoritarian wing of the socialist movement. Anarchism is contrasted to socialist forms which are state-oriented or from above. Scholars of anarchism generally highlight anarchism's socialist credentials and criticise attempts at creating dichotomies between the two. Some scholars describe anarchism as having many influences from liberalism, and being both liberal and socialist but more so. Many scholars reject anarcho-capitalism as a misunderstanding of anarchist principles.\n",
      "\n",
      "While opposition to the state is central to anarchist thought, defining anarchism is not an easy task for scholars, as there is a lot of discussion among scholars and anarchists on the matter, and various currents perceive anarchism slightly differently. Major definitional elements include the will for a non-coercive society, the rejection of the state apparatus, the belief that human nature allows humans to exist in or progress toward such a non-coercive society, and a suggestion on how to act to pursue the ideal of anarchy.\n",
      "\n",
      "History\n",
      "\n",
      "Pre-modern era \n",
      "\n",
      "Before the creation of towns and cities, established authority did not exist. It was after the institution of authority that anarchistic ideas were espoused as a reaction. The most notable precursors to anarchism in the ancient world were in China and Greece. In China, philosophical anarchism (the discussion on the legitimacy of the state) was delineated by Taoist philosophers Zhuang Zhou and Laozi. Alongside Stoicism, Taoism has been said to have had \"significant anticipations\" of anarchism.\n",
      "\n",
      "Anarchic attitudes were also articulated by tragedians and philosophers in Greece. Aeschylus and Sophocles used the myth of Antigone to illustrate the conflict between laws imposed by the state and personal autonomy. Socrates questioned Athenian authorities constantly and insisted on the right of individual freedom of conscience. Cynics dismissed human law (nomos) and associated authorities while trying to live according to nature (physis). Stoics were supportive of a society based on unofficial and friendly relations among its citizens without the presence of a state.\n",
      "\n",
      "In medieval Europe, there was no anarchistic activity except some ascetic religious movements. These, and other Muslim movements, later gave birth to religious anarchism. In the Sasanian Empire, Mazdak called for an egalitarian society and the abolition of monarchy, only to be soon executed by Emperor Kavad I.\n",
      "\n",
      "In Basra, religious sects preached against the state. In Europe, various sects developed anti-state and libertarian tendencies. Renewed interest in antiquity during the Renaissance and in private judgment during the Reformation restored elements of anti-authoritarian secularism, particularly in France. Enlightenment challenges to intellectual authority (secular and religious) and the revolutions of the 1790s and 1848 all spurred the ideological development of what became the era of classical anarchism.\n",
      "\n",
      "Modern era \n",
      "During the French Revolution, partisan groups such as the Enragés and the  saw a turning point in the fermentation of anti-state and federalist sentiments. The first anarchist currents developed throughout the 18th century as William Godwin espoused philosophical anarchism in England, morally delegitimising the state, Max Stirner's thinking paved the way to individualism and Pierre-Joseph Proudhon's theory of mutualism found fertile soil in France. By the late 1870s, various anarchist schools of thought had become well-defined and a wave of then unprecedented globalisation occurred from 1880 to 1914. This era of classical anarchism lasted until the end of the Spanish Civil War and is considered the golden age of anarchism.\n",
      "\n",
      "Drawing from mutualism, Mikhail Bakunin founded collectivist anarchism and entered the International Workingmen's Association, a class worker union later known as the First International that formed in 1864 to unite diverse revolutionary currents. The International became a significant political force, with Karl Marx being a leading figure and a member of its General Council. Bakunin's faction (the Jura Federation) and Proudhon's followers (the mutualists) opposed state socialism, advocating political abstentionism and small property holdings. After bitter disputes, the Bakuninists were expelled from the International by the Marxists at the 1872 Hague Congress. Anarchists were treated similarly in the Second International, being ultimately expelled in 1896. Bakunin famously predicted that if revolutionaries gained power by Marx's terms, they would end up the new tyrants of workers. In response to their expulsion from the First International, anarchists formed the St. Imier International. Under the influence of Peter Kropotkin, a Russian philosopher and scientist, anarcho-communism overlapped with collectivism. Anarcho-communists, who drew inspiration from the 1871 Paris Commune, advocated for free federation and for the distribution of goods according to one's needs.\n",
      "\n",
      "By the turn of the 20th century, anarchism had spread all over the world. It was a notable feature of the international syndicalist movement. In China, small groups of students imported the humanistic pro-science version of anarcho-communism. Tokyo was a hotspot for rebellious youth from East Asian countries, who moved to the Japanese capital to study. In Latin America, Argentina was a stronghold for anarcho-syndicalism, where it became the most prominent left-wing ideology. During this time, a minority of anarchists adopted tactics of revolutionary political violence, known as propaganda of the deed. The dismemberment of the French socialist movement into many groups and the execution and exile of many Communards to penal colonies following the suppression of the Paris Commune favoured individualist political expression and acts. Even though many anarchists distanced themselves from these terrorist acts, infamy came upon the movement and attempts were made to prevent anarchists immigrating to the US, including the Immigration Act of 1903, also called the Anarchist Exclusion Act. Illegalism was another strategy which some anarchists adopted during this period.\n",
      "\n",
      "Despite concerns, anarchists enthusiastically participated in the Russian Revolution in opposition to the White movement, especially in the Makhnovshchina; however, they met harsh suppression after the Bolshevik government had stabilised, including during the Kronstadt rebellion. Several anarchists from Petrograd and Moscow fled to Ukraine, before the Bolsheviks crushed the anarchist movement there too. With the anarchists being repressed in Russia, two new antithetical currents emerged, namely platformism and synthesis anarchism. The former sought to create a coherent group that would push for revolution while the latter were against anything that would resemble a political party. Seeing the victories of the Bolsheviks in the October Revolution and the resulting Russian Civil War, many workers and activists turned to communist parties which grew at the expense of anarchism and other socialist movements. In France and the United States, members of major syndicalist movements such as the General Confederation of Labour and the Industrial Workers of the World left their organisations and joined the Communist International.\n",
      "\n",
      "In the Spanish Civil War of 1936–39, anarchists and syndicalists (CNT and FAI) once again allied themselves with various currents of leftists. A long tradition of Spanish anarchism led to anarchists playing a pivotal role in the war, and particularly in the Spanish Revolution of 1936. In response to the army rebellion, an anarchist-inspired movement of peasants and workers, supported by armed militias, took control of Barcelona and of large areas of rural Spain, where they collectivised the land. The Soviet Union provided some limited assistance at the beginning of the war, but the result was a bitter fight between communists and other leftists in a series of events known as the May Days, as Joseph Stalin asserted Soviet control of the Republican government, ending in another defeat of anarchists at the hands of the communists.\n",
      "\n",
      "Post-WWII \n",
      "\n",
      "By the end of World War II, the anarchist movement had been severely weakened. The 1960s witnessed a revival of anarchism, likely caused by a perceived failure of Marxism–Leninism and tensions built by the Cold War. During this time, anarchism found a presence in other movements critical towards both capitalism and the state such as the anti-nuclear, environmental, and peace movements, the counterculture of the 1960s, and the New Left. It also saw a transition from its previous revolutionary nature to provocative anti-capitalist reformism. Anarchism became associated with punk subculture as exemplified by bands such as Crass and the Sex Pistols. The established feminist tendencies of anarcha-feminism returned with vigour during the second wave of feminism. Black anarchism began to take form at this time and influenced anarchism's move from a Eurocentric demographic. This coincided with its failure to gain traction in Northern Europe and its unprecedented height in Latin America.\n",
      "\n",
      "Around the turn of the 21st century, anarchism grew in popularity and influence within anti-capitalist, anti-war and anti-globalisation movements. Anarchists became known for their involvement in protests against the World Trade Organization (WTO), the Group of Eight and the World Economic Forum. During the protests, ad hoc leaderless anonymous cadres known as black blocs engaged in rioting, property destruction and violent confrontations with the police. Other organisational tactics pioneered at this time include affinity groups, security culture and the use of decentralised technologies such as the Internet. A significant event of this period was the confrontations at the 1999 Seattle WTO conference. Anarchist ideas have been influential in the development of the Zapatistas in Mexico and the Democratic Federation of Northern Syria, more commonly known as Rojava, a de facto autonomous region in northern Syria.\n",
      "\n",
      "While having revolutionary aspirations, many forms of anarchism are not confrontational nowadays. Instead, they are trying to build an alternative way of social organization, based on mutual interdependence and voluntary cooperation. Scholar Carissa Honeywell takes the example of Food not Bombs group of collectives, to highlight some features of how anarchist groups work: direct action, working together and in solidarity with those left behind. While doing so, they inform about the rising rates of world hunger suggest a policy to tackle hunger, ranging from de-funding the arms industry to addressing Monsanto seed-saving policies and patents, helping farmers and commodification of food and housing. Honeywell also emphasizes that contemporary anarchists are interested in the flourishing not only of humans, but non-humans and the environment as well. Honeywell argues that escalation of problems such as continuous wars and world poverty show that the current framework not only cannot solve those pressing problems for humanity, but are causal factors as well, resulting in the rejection of representative democracy and the state as a whole.\n",
      "\n",
      "Thought \n",
      "Anarchist schools of thought have been generally grouped into two main historical traditions, social anarchism and individualist anarchism, owing to their different origins, values and evolution. The individualist current emphasises negative liberty in opposing restraints upon the free individual, while the social current emphasises positive liberty in aiming to achieve the free potential of society through equality and social ownership. In a chronological sense, anarchism can be segmented by the classical currents of the late 19th century and the post-classical currents (anarcha-feminism, green anarchism, and post-anarchism) developed thereafter.\n",
      "\n",
      "Beyond the specific factions of anarchist movements which constitute political anarchism lies philosophical anarchism which holds that the state lacks moral legitimacy, without necessarily accepting the imperative of revolution to eliminate it. A component especially of individualist anarchism, philosophical anarchism may tolerate the existence of a minimal state but claims that citizens have no moral obligation to obey government when it conflicts with individual autonomy. Anarchism pays significant attention to moral arguments since ethics have a central role in anarchist philosophy. Anarchism's emphasis on anti-capitalism, egalitarianism, and for the extension of community and individuality sets it apart from anarcho-capitalism and other types of economic libertarianism.\n",
      "\n",
      "Anarchism is usually placed on the far-left of the political spectrum. Much of its economics and legal philosophy reflect anti-authoritarian, anti-statist, libertarian, and radical interpretations of left-wing and socialist politics such as collectivism, communism, individualism, mutualism, and syndicalism, among other libertarian socialist economic theories. As anarchism does not offer a fixed body of doctrine from a single particular worldview, many anarchist types and traditions exist and varieties of anarchy diverge widely. One reaction against sectarianism within the anarchist milieu was anarchism without adjectives, a call for toleration and unity among anarchists first adopted by Fernando Tarrida del Mármol in 1889 in response to the bitter debates of anarchist theory at the time. Belief in political nihilism has been espoused by anarchists. Despite separation, the various anarchist schools of thought are not seen as distinct entities but rather as tendencies that intermingle and are connected through a set of uniform principles such as individual and local autonomy, mutual aid, network organisation, communal democracy, justified authority and decentralisation.\n",
      "\n",
      "Classical \n",
      "\n",
      "Inceptive currents among classical anarchist currents were mutualism and individualism. They were followed by the major currents of social anarchism (collectivist, communist and syndicalist). They differ on organisational and economic aspects of their ideal society.\n",
      "\n",
      "Mutualism is an 18th-century economic theory that was developed into anarchist theory by Pierre-Joseph Proudhon. Its aims include \"abolishing the state\", reciprocity, free association, voluntary contract, federation and monetary reform of both credit and currency that would be regulated by a bank of the people. Mutualism has been retrospectively characterised as ideologically situated between individualist and collectivist forms of anarchism. In What Is Property? (1840), Proudhon first characterised his goal as a \"third form of society, the synthesis of communism and property.\" Collectivist anarchism is a revolutionary socialist form of anarchism commonly associated with Mikhail Bakunin. Collectivist anarchists advocate collective ownership of the means of production which is theorised to be achieved through violent revolution and that workers be paid according to time worked, rather than goods being distributed according to need as in communism. Collectivist anarchism arose alongside Marxism but rejected the dictatorship of the proletariat despite the stated Marxist goal of a collectivist stateless society.\n",
      "\n",
      "Anarcho-communism is a theory of anarchism that advocates a communist society with common ownership of the means of production, held by a federal network of voluntary associations, with production and consumption based on the guiding principle \"From each according to his ability, to each according to his need.\" Anarcho-communism developed from radical socialist currents after the French Revolution but was first formulated as such in the Italian section of the First International. It was later expanded upon in the theoretical work of Peter Kropotkin, whose specific style would go onto become the dominating view of anarchists by the late 19th century. Anarcho-syndicalism is a branch of anarchism that views labour syndicates as a potential force for revolutionary social change, replacing capitalism and the state with a new society democratically self-managed by workers. The basic principles of anarcho-syndicalism are direct action, workers' solidarity and workers' self-management.\n",
      "\n",
      "Individualist anarchism is a set of several traditions of thought within the anarchist movement that emphasise the individual and their will over any kinds of external determinants. Early influences on individualist forms of anarchism include William Godwin, Max Stirner, and Henry David Thoreau. Through many countries, individualist anarchism attracted a small yet diverse following of Bohemian artists and intellectuals as well as young anarchist outlaws in what became known as illegalism and individual reclamation.\n",
      "\n",
      "Post-classical and contemporary \n",
      "\n",
      "Anarchist principles undergird contemporary radical social movements of the left. Interest in the anarchist movement developed alongside momentum in the anti-globalisation movement, whose leading activist networks were anarchist in orientation. As the movement shaped 21st century radicalism, wider embrace of anarchist principles signaled a revival of interest. Anarchism has continued to generate many philosophies and movements, at times eclectic, drawing upon various sources and combining disparate concepts to create new philosophical approaches. The anti-capitalist tradition of classical anarchism has remained prominent within contemporary currents.\n",
      "\n",
      "Contemporary news coverage which emphasizes black bloc demonstrations has reinforced anarchism's historical association with chaos and violence. Its publicity has also led more scholars in fields such as anthropology and history to engage with the anarchist movement, although contemporary anarchism favours actions over academic theory. Various anarchist groups, tendencies, and schools of thought exist today, making it difficult to describe the contemporary anarchist movement. While theorists and activists have established \"relatively stable constellations of anarchist principles\", there is no consensus on which principles are core and commentators describe multiple anarchisms, rather than a singular anarchism, in which common principles are shared between schools of anarchism while each group prioritizes those principles differently. Gender equality can be a common principle, although it ranks as a higher priority to anarcha-feminists than anarcho-communists.\n",
      "\n",
      "Anarchists are generally committed against coercive authority in all forms, namely \"all centralized and hierarchical forms of government (e.g., monarchy, representative democracy, state socialism, etc.), economic class systems (e.g., capitalism, Bolshevism, feudalism, slavery, etc.), autocratic religions (e.g., fundamentalist Islam, Roman Catholicism, etc.), patriarchy, heterosexism, white supremacy, and imperialism.\" Anarchist schools disagree on the methods by which these forms should be opposed. The principle of equal liberty is closer to anarchist political ethics in that it transcends both the liberal and socialist traditions. This entails that liberty and equality cannot be implemented within the state, resulting in the questioning of all forms of domination and hierarchy.\n",
      "\n",
      "Tactics \n",
      "Anarchists' tactics take various forms but in general serve two major goals, namely, to first oppose the Establishment and secondly to promote anarchist ethics and reflect an anarchist vision of society, illustrating the unity of means and ends. A broad categorisation can be made between aims to destroy oppressive states and institutions by revolutionary means on one hand and aims to change society through evolutionary means on the other. Evolutionary tactics embrace nonviolence, reject violence and take a gradual approach to anarchist aims, although there is significant overlap between the two.\n",
      "\n",
      "Anarchist tactics have shifted during the course of the last century. Anarchists during the early 20th century focused more on strikes and militancy while contemporary anarchists use a broader array of approaches.\n",
      "\n",
      "Classical era \n",
      "\n",
      "During the classical era, anarchists had a militant tendency. Not only did they confront state armed forces, as in Spain and Ukraine, but some of them also employed terrorism as propaganda of the deed. Assassination attempts were carried out against heads of state, some of which were successful. Anarchists also took part in revolutions. Many anarchists, especially the Galleanists, believed that these attempts would be the impetus for a revolution against capitalism and the state. Many of these attacks were done by individual assailants and the majority took place in the late 1870s, the early 1880s and the 1890s, with some still occurring in the early 1900s. Their decrease in prevalence was the result of further judicial power and targeting and cataloging by state institutions.\n",
      "\n",
      "Anarchist perspectives towards violence have always been controversial. Anarcho-pacifists advocate for non-violence means to achieve their stateless, nonviolent ends. Other anarchist groups advocate direct action, a tactic which can include acts of sabotage or terrorism. This attitude was quite prominent a century ago when seeing the state as a tyrant and some anarchists believing that they had every right to oppose its oppression by any means possible. Emma Goldman and Errico Malatesta, who were proponents of limited use of violence, stated that violence is merely a reaction to state violence as a necessary evil.\n",
      "\n",
      "Anarchists took an active role in strike actions, although they tended to be antipathetic to formal syndicalism, seeing it as reformist. They saw it as a part of the movement which sought to overthrow the state and capitalism. Anarchists also reinforced their propaganda within the arts, some of whom practiced naturism and nudism. Those anarchists also built communities which were based on friendship and were involved in the news media.\n",
      "\n",
      "Revolutionary \n",
      "\n",
      "In the current era, Italian anarchist Alfredo Bonanno, a proponent of insurrectionary anarchism, has reinstated the debate on violence by rejecting the nonviolence tactic adopted since the late 19th century by Kropotkin and other prominent anarchists afterwards. Both Bonanno and the French group The Invisible Committee advocate for small, informal affiliation groups, where each member is responsible for their own actions but works together to bring down oppression utilizing sabotage and other violent means against state, capitalism, and other enemies. Members of The Invisible Committee were arrested in 2008 on various charges, terrorism included.\n",
      "\n",
      "Overall, contemporary anarchists are much less violent and militant than their ideological ancestors. They mostly engage in confronting the police during demonstrations and riots, especially in countries such as Canada, Greece, and Mexico. Militant black bloc protest groups are known for clashing with the police; however, anarchists not only clash with state operators, they also engage in the struggle against fascists and racists, taking anti-fascist action and mobilizing to prevent hate rallies from happening.\n",
      "\n",
      "Evolutionary \n",
      "Anarchists commonly employ direct action. This can take the form of disrupting and protesting against unjust hierarchy, or the form of self-managing their lives through the creation of counter-institutions such as communes and non-hierarchical collectives. Decision-making is often handled in an anti-authoritarian way, with everyone having equal say in each decision, an approach known as horizontalism. Contemporary-era anarchists have been engaging with various grassroots movements that are more or less based on horizontalism, although not explicitly anarchist, respecting personal autonomy and participating in mass activism such as strikes and demonstrations. In contrast with the big-A anarchism of the classical era, the newly coined term small-a anarchism signals their tendency not to base their thoughts and actions on classical-era anarchism or to refer to classical anarchists such as Peter Kropotkin and Pierre-Joseph Proudhon to justify their opinions. Those anarchists would rather base their thought and praxis on their own experience which they will later theorize.\n",
      "\n",
      "The decision-making process of small anarchist affinity groups plays a significant tactical role. Anarchists have employed various methods in order to build a rough consensus among members of their group without the need of a leader or a leading group. One way is for an individual from the group to play the role of facilitator to help achieve a consensus without taking part in the discussion themselves or promoting a specific point. Minorities usually accept rough consensus, except when they feel the proposal contradicts anarchist ethics, goals and values. Anarchists usually form small groups (5–20 individuals) to enhance autonomy and friendships among their members. These kinds of groups more often than not interconnect with each other, forming larger networks. Anarchists still support and participate in strikes, especially wildcat strikes as these are leaderless strikes not organised centrally by a syndicate.\n",
      "\n",
      "As in the past, newspapers and journals are used, and anarchists have gone online in the World Wide Web to spread their message. Anarchists have found it easier to create websites because of distributional and other difficulties, hosting electronic libraries and other portals. Anarchists were also involved in developing various software that are available for free. The way these hacktivists work to develop and distribute resembles the anarchist ideals, especially when it comes to preserving users' privacy from state surveillance.\n",
      "\n",
      "Anarchists organize themselves to squat and reclaim public spaces. During important events such as protests and when spaces are being occupied, they are often called Temporary Autonomous Zones (TAZ), spaces where art, poetry, and surrealism are blended to display the anarchist ideal. As seen by anarchists, squatting is a way to regain urban space from the capitalist market, serving pragmatical needs and also being an exemplary direct action. Acquiring space enables anarchists to experiment with their ideas and build social bonds. Adding up these tactics while having in mind that not all anarchists share the same attitudes towards them, along with various forms of protesting at highly symbolic events, make up a carnivalesque atmosphere that is part of contemporary anarchist vividity.\n",
      "\n",
      "Key issues \n",
      "\n",
      "As anarchism is a philosophy that embodies many diverse attitudes, tendencies, and schools of thought; disagreement over questions of values, ideology, and tactics is common. Its diversity has led to widely different uses of identical terms among different anarchist traditions which has created a number of definitional concerns in anarchist theory. The compatibility of capitalism, nationalism, and religion with anarchism is widely disputed, and anarchism enjoys complex relationships with ideologies such as communism, collectivism, Marxism, and trade unionism. Anarchists may be motivated by humanism, divine authority, enlightened self-interest, veganism, or any number of alternative ethical doctrines. Phenomena such as civilisation, technology (e.g. within anarcho-primitivism), and the democratic process may be sharply criticised within some anarchist tendencies and simultaneously lauded in others.\n",
      "\n",
      "The state \n",
      "\n",
      "Objection to the state and its institutions is a sine qua non of anarchism. Anarchists consider the state as a tool of domination and believe it to be illegitimate regardless of its political tendencies. Instead of people being able to control the aspects of their life, major decisions are taken by a small elite. Authority ultimately rests solely on power, regardless of whether that power is open or transparent, as it still has the ability to coerce people. Another anarchist argument against states is that the people constituting a government, even the most altruistic among officials, will unavoidably seek to gain more power, leading to corruption. Anarchists consider the idea that the state is the collective will of the people to be an unachievable fiction due to the fact that the ruling class is distinct from the rest of society.\n",
      "\n",
      "Specific anarchist attitudes towards the state vary. Robert Paul Wolff believed that the tension between authority and autonomy would mean the state could never be legitimate. Bakunin saw the state as meaning \"coercion, domination by means of coercion, camouflaged if possible but unceremonious and overt if need be.\" A. John Simmons and Leslie Green, who leaned toward philosophical anarchism, believed that the state could be legitimate if it is governed by consensus, although they saw this as highly unlikely. Beliefs on how to abolish the state also differ.\n",
      "\n",
      "Gender, sexuality, and free love \n",
      "\n",
      "As gender and sexuality carry along them dynamics of hierarchy, many anarchists address, analyse, and oppose the suppression of one's autonomy imposed by gender roles.\n",
      "\n",
      "Sexuality was not often discussed by classical anarchists but the few that did felt that an anarchist society would lead to sexuality naturally developing. Sexual violence was a concern for anarchists such as Benjamin Tucker, who opposed age of consent laws, believing they would benefit predatory men. A historical current that arose and flourished during 1890 and 1920 within anarchism was free love. In contemporary anarchism, this current survives as a tendency to support polyamory, relationship anarchy, and queer anarchism. Free love advocates were against marriage, which they saw as a way of men imposing authority over women, largely because marriage law greatly favoured the power of men. The notion of free love was much broader and included a critique of the established order that limited women's sexual freedom and pleasure. Those free love movements contributed to the establishment of communal houses, where large groups of travelers, anarchists and other activists slept in beds together. Free love had roots both in Europe and the United States; however, some anarchists struggled with the jealousy that arose from free love. Anarchist feminists were advocates of free love, against marriage, and pro-choice (utilising a contemporary term), and had a similar agenda. Anarchist and non-anarchist feminists differed on suffrage but were supportive of one another.\n",
      "\n",
      "During the second half of the 20th century, anarchism intermingled with the second wave of feminism, radicalising some currents of the feminist movement and being influenced as well. By the latest decades of the 20th century, anarchists and feminists were advocating for the rights and autonomy of women, gays, queers and other marginalised groups, with some feminist thinkers suggesting a fusion of the two currents. With the third wave of feminism, sexual identity and compulsory heterosexuality became a subject of study for anarchists, yielding a post-structuralist critique of sexual normality. Some anarchists distanced themselves from this line of thinking, suggesting that it leaned towards an individualism that was dropping the cause of social liberation.\n",
      "\n",
      "Education \n",
      "\n",
      "The interest of anarchists in education stretches back to the first emergence of classical anarchism. Anarchists consider proper education, one which sets the foundations of the future autonomy of the individual and the society, to be an act of mutual aid. Anarchist writers such as William Godwin (Political Justice) and Max Stirner (\"The False Principle of Our Education\") attacked both state education and private education as another means by which the ruling class replicate their privileges.\n",
      "\n",
      "In 1901, Catalan anarchist and free thinker Francisco Ferrer established the Escuela Moderna in Barcelona as an opposition to the established education system which was dictated largely by the Catholic Church. Ferrer's approach was secular, rejecting both state and church involvement in the educational process whilst giving pupils large amounts of autonomy in planning their work and attendance. Ferrer aimed to educate the working class and explicitly sought to foster class consciousness among students. The school closed after constant harassment by the state and Ferrer was later arrested. Nonetheless, his ideas formed the inspiration for a series of modern schools around the world. Christian anarchist Leo Tolstoy, who published the essay Education and Culture, also established a similar school with its founding principle being that \"for education to be effective it had to be free.\" In a similar token, A. S. Neill founded what became the Summerhill School in 1921, also declaring being free from coercion.\n",
      "\n",
      "Anarchist education is based largely on the idea that a child's right to develop freely and without manipulation ought to be respected and that rationality would lead children to morally good conclusions; however, there has been little consensus among anarchist figures as to what constitutes manipulation. Ferrer believed that moral indoctrination was necessary and explicitly taught pupils that equality, liberty and social justice were not possible under capitalism, along with other critiques of government and nationalism.\n",
      "\n",
      "Late 20th century and contemporary anarchist writers (Paul Goodman, Herbert Read, and Colin Ward) intensified and expanded the anarchist critique of state education, largely focusing on the need for a system that focuses on children's creativity rather than on their ability to attain a career or participate in consumerism as part of a consumer society. Contemporary anarchists such as Ward claim that state education serves to perpetuate socioeconomic inequality.\n",
      "\n",
      "While few anarchist education institutions have survived to the modern-day, major tenets of anarchist schools, among them respect for child autonomy and relying on reasoning rather than indoctrination as a teaching method, have spread among mainstream educational institutions. Judith Suissa names three schools as explicitly anarchists' schools, namely the Free Skool Santa Cruz in the United States which is part of a wider American-Canadian network of schools, the Self-Managed Learning College in Brighton, England, and the Paideia School in Spain.\n",
      "\n",
      "The arts \n",
      "\n",
      "The connection between anarchism and art was quite profound during the classical era of anarchism, especially among artistic currents that were developing during that era such as futurists, surrealists and others. In literature, anarchism was mostly associated with the New Apocalyptics and the neo-romanticism movement. In music, anarchism has been associated with music scenes such as punk. Anarchists such as Leo Tolstoy and Herbert Read stated that the border between the artist and the non-artist, what separates art from a daily act, is a construct produced by the alienation caused by capitalism and it prevents humans from living a joyful life.\n",
      "\n",
      "Other anarchists advocated for or used art as a means to achieve anarchist ends. In his book Breaking the Spell: A History of Anarchist Filmmakers, Videotape Guerrillas, and Digital Ninjas, Chris Robé claims that \"anarchist-inflected practices have increasingly structured movement-based video activism.\" Throughout the 20th century, many prominent anarchists (Peter Kropotkin, Emma Goldman, Gustav Landauer and Camillo Berneri) and publications such as Anarchy wrote about matters pertaining to the arts.\n",
      "\n",
      "Three overlapping properties made art useful to anarchists. It could depict a critique of existing society and hierarchies, serve as a prefigurative tool to reflect the anarchist ideal society and even turn into a means of direct action such as in protests. As it appeals to both emotion and reason, art could appeal to the whole human and have a powerful effect. The 19th-century neo-impressionist movement had an ecological aesthetic and offered an example of an anarchist perception of the road towards socialism. In Les chataigniers a Osny by anarchist painter Camille Pissarro, the blending of aesthetic and social harmony is prefiguring an ideal anarchistic agrarian community.\n",
      "\n",
      "Criticism \n",
      "The most common critique of anarchism is the assertion that humans cannot self-govern and so a state is necessary for human survival. Philosopher Bertrand Russell supported this critique, stating that \"[p]eace and war, tariffs, regulations of sanitary conditions and the sale of noxious drugs, the preservation of a just system of distribution: these, among others, are functions which could hardly be performed in a community in which there was no central government.\" Another common criticism of anarchism is that it fits a world of isolation in which only the small enough entities can be self-governing; a response would be that major anarchist thinkers advocated anarchist federalism.\n",
      "\n",
      "Another criticism of anarchism is the belief that it is inherently unstable: that an anarchist society would inevitably evolve back into a state. Thomas Hobbes and other early social contract theorists argued that the state emerges in response to natural anarchy in order to protect the people's interests and keep order. Philosopher Robert Nozick argued that a \"night-watchman state\", or minarchy, would emerge from anarchy through the process of an invisible hand, in which people would exercise their liberty and buy protection from protection agencies, evolving into a minimal state. Anarchists reject these criticisms by arguing that humans in a state of nature would not just be in a state of war. Anarcho-primitivists in particular argue that humans were better off in a state of nature in small tribes living close to the land, while anarchists in general argue that the negatives of state organization, such as hierarchies, monopolies and inequality, outweigh the benefits.\n",
      "\n",
      "Philosophy lecturer Andrew G. Fiala composed a list of common arguments against anarchism which includes critiques such as that anarchism is innately related to violence and destruction, not only in the pragmatic world, such as at protests, but in the world of ethics as well. Secondly, anarchism is evaluated as unfeasible or utopian since the state cannot be defeated practically. This line of arguments most often calls for political action within the system to reform it. The third argument is that anarchism is self-contradictory as a ruling theory that has no ruling theory. Anarchism also calls for collective action whilst endorsing the autonomy of the individual, hence no collective action can be taken. Lastly, Fiala mentions a critique towards philosophical anarchism of being ineffective (all talk and thoughts) and in the meantime capitalism and bourgeois class remains strong.\n",
      "\n",
      "Philosophical anarchism has met the criticism of members of academia following the release of pro-anarchist books such as A. John Simmons' Moral Principles and Political Obligations. Law professor William A. Edmundson authored an essay to argue against three major philosophical anarchist principles which he finds fallacious. Edmundson says that while the individual does not owe the state a duty of obedience, this does not imply that anarchism is the inevitable conclusion and the state is still morally legitimate. In The Problem of Political Authority, Michael Huemer defends philosophical anarchism, claiming that \"political authority is a moral illusion.\"\n",
      "\n",
      "One of the earliest criticisms is that anarchism defies and fails to understand the biological inclination to authority. Joseph Raz states that the acceptance of authority implies the belief that following their instructions will afford more success. Raz believes that this argument is true in following both authorities' successful and mistaken instruction. Anarchists reject this criticism because challenging or disobeying authority does not entail the disappearance of its advantages by acknowledging authority such as doctors or lawyers as reliable, nor does it involve a complete surrender of independent judgment. Anarchist perception of human nature, rejection of the state, and commitment to social revolution has been criticised by academics as naive, overly simplistic, and unrealistic, respectively. Classical anarchism has been criticised for relying too heavily on the belief that the abolition of the state will lead to human cooperation prospering.\n",
      "\n",
      "Friedrich Engels, considered to be one of the principal founders of Marxism, criticised anarchism's anti-authoritarianism as inherently counter-revolutionary because in his view a revolution is by itself authoritarian. Academic John Molyneux writes in his book Anarchism: A Marxist Criticism that \"anarchism cannot win\", believing that it lacks the ability to properly implement its ideas. The Marxist criticism of anarchism is that it has a utopian character because all individuals should have anarchist views and values. According to the Marxist view, that a social idea would follow directly from this human ideal and out of the free will of every individual formed its essence. Marxists state that this contradiction was responsible for their inability to act. In the anarchist vision, the conflict between liberty and equality was resolved through coexistence and intertwining.\n",
      "\n",
      "See also \n",
      "\n",
      " Anarchism by country\n",
      " Governance without government\n",
      " List of anarchist political ideologies\n",
      " List of books about anarchism\n",
      "\n",
      "References\n",
      "\n",
      "Explanatory notes\n",
      "\n",
      "Citations\n",
      "\n",
      "General and cited sources\n",
      "\n",
      "Primary sources\n",
      "\n",
      "Secondary sources\n",
      "\n",
      "Tertiary sources\n",
      "\n",
      "Further reading \n",
      " \n",
      "  \n",
      "  Criticism of philosophical anarchism.\n",
      " \n",
      "  A defence of philosophical anarchism, stating that \"both kinds of 'anarchism' [i.e. philosophical and political anarchism] are philosophical and political claims.\" (p. 137)\n",
      "  Anarchistic popular fiction novel.\n",
      " \n",
      " \n",
      " \n",
      "  An argument for philosophical anarchism.\n",
      "\n",
      "External links \n",
      "\n",
      " \n",
      " Anarchy Archives – an online research center on the history and theory of anarchism.\n",
      "\n",
      " \n",
      "Anti-capitalism\n",
      "Anti-fascism\n",
      "Economic ideologies\n",
      "Far-left politics\n",
      "Left-wing politics\n",
      "Libertarian socialism\n",
      "Libertarianism\n",
      "Political culture\n",
      "Political ideologies\n",
      "Political movements\n",
      "Social theories\n",
      "Socialism\n"
     ]
    }
   ],
   "source": [
    "print(ds[\"train\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avpc-off-vehicle-qa-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
