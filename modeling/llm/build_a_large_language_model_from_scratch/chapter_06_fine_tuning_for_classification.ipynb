{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning for classification (chapter 6)\n",
    "\n",
    "This notebook explores the fine-tuning process of LLMs with the purpose of creating a classification model based on Sebastian Raschka's book (Chapter 6). In particular, it discusses the following:\n",
    "\n",
    "- Introducing different LLM fine-tuning approaches\n",
    "- Preparing a dataset for text classification\n",
    "- Modifying a pretrained LLM for fine-tuning\n",
    "- Fine-tuning an LLM to identify spam messages\n",
    "- Evaluating the accuracy of a fine-tuned LLM classifier\n",
    "- Using a fine-tuned LLM to classify new data\n",
    "\n",
    "## Instruction fine-tuning\n",
    "\n",
    "- Instruction-tuned models can typically handle a broader range of tasks\n",
    "- More general approach that can handle multiple tasks\n",
    "- Best suited for models that need to handle a variety of tasks based on complex user instructions\n",
    "- These models improve flexibility and interaction quality\n",
    "- Instruction fine-tuning requires larger datasets and greater computational resources\n",
    "\n",
    "## Classification fine-tuning\n",
    "\n",
    "- Ideal for projects requiring precise categorization into predefined classes (e.g. sentiment analysis or spam detection)\n",
    "- Specialized approach targeted at outputting a specific set of labels\n",
    "- The model is restricted to only the labels encountered during training\n",
    "- Requires less data and compute power\n",
    "\n",
    "## Acknowledgment\n",
    "\n",
    "All concepts, architectures, and implementation approaches are credited to Sebastian Raschka's work.  This repository serves as my personal implementation and notes while working through the book's content.\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Sebastian Raschka's GitHub](https://github.com/rasbt)\n",
    "- [Book Information](https://www.manning.com/books/build-a-large-language-model-from-scratch)\n",
    "    - [Chapter 6](https://livebook.manning.com/book/build-a-large-language-model-from-scratch/chapter-6)\n",
    "\n",
    "![Topic overview](https://drek4537l1klr.cloudfront.net/raschka/Figures/6-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install import-ipynb for importing ipynb files.\n",
    "# %pip install import-ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import tiktoken\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Import previous chapter dependencies.\n",
    "# See https://stackoverflow.com/questions/44116194/import-a-function-from-another-ipynb-file\n",
    "# NOTE: Importing these functions seems to run the entire cell the symbol is defined in, which would\n",
    "#       suggest that symbols should be defined in separate cells from the test code.\n",
    "import import_ipynb\n",
    "from gpt_download import download_and_load_gpt2\n",
    "from chapter_02_dataset_creation import create_dataloader_v1\n",
    "from chapter_04_gpt_from_scratch import (\n",
    "    GPTConfig,\n",
    "    GPTModel,\n",
    "    generate_text_simple,\n",
    ")\n",
    "\n",
    "# NOTE: Importing another ipynb file basically runs the entire imported notebook.\n",
    "from chapter_05_pretraining_on_unlabeled_data import (\n",
    "    generate,\n",
    "    token_ids_to_text,\n",
    "    text_to_token_ids,\n",
    ")\n",
    "\n",
    "# Define the base config.\n",
    "GPT_CONFIG_124M = GPTConfig(\n",
    "    vocab_size=50257,  # as used by the BPE tokenizer for GPT-2.\n",
    "    context_length=1024,\n",
    "    emb_dim=768,\n",
    "    n_heads=12,\n",
    "    n_layers=12,\n",
    "    dropout_rate=0.0,  # disable dropout for inference\n",
    "    qkv_bias=False,\n",
    ")\n",
    "\n",
    "# Determine the device to run the model on.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1: Preparing the dataset\n",
    "\n",
    "This section follows stage 1 in the following figure:\n",
    "\n",
    "![Dataset preparation](https://drek4537l1klr.cloudfront.net/raschka/Figures/6-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
    "zip_path = Path(\"data/sms_spam_collection.zip\")\n",
    "extracted_path = Path(\"data/sms_spam_collection\")\n",
    "data_file_path = extracted_path / \"SMSSpamCollection.tsv\"\n",
    "\n",
    "\n",
    "def download_and_unzip_spam_data(\n",
    "    url: str, zip_path: Path, extracted_path: Path, data_file_path: Path\n",
    "):\n",
    "    \"\"\"Download and unzip the spam data from the UCI repository.\n",
    "\n",
    "    Args:\n",
    "        url: The URL of the zip file.\n",
    "        zip_path: The path to save the zip file.\n",
    "        extracted_path: The path to save the extracted files.\n",
    "        data_file_path: The path to save the data file.\n",
    "    \"\"\"\n",
    "    # Check if the file already exists.\n",
    "    if data_file_path.exists():\n",
    "        print(f\"{data_file_path} already exists. Skipping download \" \"and extraction.\")\n",
    "        return\n",
    "\n",
    "    # Download the zip file.\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        with open(zip_path, \"wb\") as out_file:\n",
    "            out_file.write(response.read())\n",
    "\n",
    "    # Extract the zip file.\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(extracted_path)\n",
    "\n",
    "    # Add a .tsv extension to the file (tab-separated values).\n",
    "    original_file_path = Path(extracted_path) / \"SMSSpamCollection\"\n",
    "    os.rename(original_file_path, data_file_path)\n",
    "    print(f\"File downloaded and saved as {data_file_path}\")\n",
    "\n",
    "\n",
    "download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into a pandas DataFrame.\n",
    "df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"])\n",
    "\n",
    "# Show the label count.\n",
    "print(df[\"Label\"].value_counts())\n",
    "\n",
    "# Show a few examples.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balancing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a balanced dataset by undersampling the majority class.\n",
    "def create_balanced_dataset(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Create a balanced dataset by undersampling the majority class.\n",
    "\n",
    "    NOTE: This function can quite significantly reduce the size of the dataset.\n",
    "\n",
    "    Args:\n",
    "        df: The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        A balanced DataFrame.\n",
    "    \"\"\"\n",
    "    num_spam = len(df[df[\"Label\"] == \"spam\"])\n",
    "    ham_subset = df[df[\"Label\"] == \"ham\"].sample(num_spam, random_state=123)\n",
    "    balanced_df = pd.concat([ham_subset, df[df[\"Label\"] == \"spam\"]])\n",
    "    return balanced_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Create a balanced dataset.\n",
    "balanced_df = create_balanced_dataset(df)\n",
    "print(balanced_df[\"Label\"].value_counts())\n",
    "\n",
    "# Convert string labels to integers.\n",
    "balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})\n",
    "balanced_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the datast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_split(\n",
    "    df: pd.DataFrame, train_frac: float, validation_frac: float\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Split a DataFrame into train, validation, and test sets.\n",
    "\n",
    "    NOTE: The size of the test set is implied to be the remainder of train and validation fraction\n",
    "          (all fractions should add up to 1).\n",
    "\n",
    "    Args:\n",
    "        df: The input DataFrame.\n",
    "        train_frac: The fraction of the dataset to use for training.\n",
    "        validation_frac: The fraction of the dataset to use for validation.\n",
    "\n",
    "    Returns:\n",
    "        A tuple of DataFrames for train, validation, and test sets.\n",
    "    \"\"\"\n",
    "    # Shuffle the entire DataFrame\n",
    "    df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "\n",
    "    # Calculate split indices (for train and validation explicitly.)\n",
    "    train_end = int(len(df) * train_frac)\n",
    "    validation_end = train_end + int(len(df) * validation_frac)\n",
    "\n",
    "    # Split the DataFrame.\n",
    "    train_df = df[:train_end]\n",
    "    validation_df = df[train_end:validation_end]\n",
    "    test_df = df[validation_end:]\n",
    "\n",
    "    return train_df, validation_df, test_df\n",
    "\n",
    "\n",
    "# Test size is implied to be 0.2 as the remainder.\n",
    "train_df, validation_df, test_df = random_split(\n",
    "    df=balanced_df, train_frac=0.7, validation_frac=0.1\n",
    ")\n",
    "print(f\"Train set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(validation_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")\n",
    "\n",
    "# Save the DataFrames to CSV files.\n",
    "train_df.to_csv(extracted_path / \"train.csv\", index=None)\n",
    "validation_df.to_csv(extracted_path / \"validation.csv\", index=None)\n",
    "test_df.to_csv(extracted_path / \"test.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the datasets\n",
    "\n",
    "Previously, we utilized a sliding window technique to generate uniformly sized text chunks, which we then grouped into batches for more efficient model training. Each chunk functioned as an individual training instance. However, we are now working with a spam dataset that contains text messages of varying lengths. To batch these messages as we did with the text chunks, we have two primary options:\n",
    "\n",
    "- Truncate all messages to the length of the shortest message in the dataset or batch.\n",
    "- Pad all messages to the length of the longest message in the dataset or batch.\n",
    "\n",
    "The first option is computationally cheaper, but it may result in significant information loss if shorter messages are much smaller than the average or longest messages, potentially reducing model performance. So, we opt for the second option, which preserves the entire content of all messages.\n",
    "\n",
    "To implement batching, where all messages are padded to the length of the longest message in the dataset, we add padding tokens to all shorter messages. For this purpose, we use \"<|endoftext|>\" as a padding token. However, instead of appending the string \"<|endoftext|>\" to each of the text messages directly, we can add the token ID corresponding to \"<|endoftext|>\" to the encoded text messages\n",
    "\n",
    "![Padding approach](https://drek4537l1klr.cloudfront.net/raschka/Figures/6-6.png)\n",
    "\n",
    "The example below shows what a training batch looks like. A single training batch consisting of eight text messages represented as token IDs. Each text message consists of 120 token IDs. A class label array stores the eight class labels corresponding to the text messages, which can be either 0 (“not spam”) or 1 (“spam”).\n",
    "\n",
    "![Training batch example](https://drek4537l1klr.cloudfront.net/raschka/Figures/6-7.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Dataset class.\n",
    "class SpamDataset(Dataset):\n",
    "    \"\"\"Dataset class for the spam dataset.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        csv_file: Path,\n",
    "        tokenizer: tiktoken.Encoding,\n",
    "        max_length: Optional[int] = None,\n",
    "        pad_token_id: int = 50256,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the SpamDataset class.\n",
    "\n",
    "        Args:\n",
    "            csv_file: The path to the CSV file containing the data.\n",
    "            tokenizer: The tokenizer to use.\n",
    "            max_length: The maximum length of the encoded texts.\n",
    "            pad_token_id: The ID of the padding token.\n",
    "        \"\"\"\n",
    "        # Load the data from the CSV file.\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "\n",
    "        # Pretokenize all texts.\n",
    "        self.encoded_texts = [tokenizer.encode(text) for text in self.data[\"Text\"]]\n",
    "\n",
    "        if max_length is None:\n",
    "            # If no maximum length is provided, use the longest encoded text.\n",
    "            self.max_length = self._longest_encoded_length()\n",
    "        else:\n",
    "            # Truncate sequences if they are longer than max_length.\n",
    "            self.max_length = max_length\n",
    "            self.encoded_texts = [\n",
    "                encoded_text[: self.max_length] for encoded_text in self.encoded_texts\n",
    "            ]\n",
    "\n",
    "        # Pads sequences to the longest sequence\n",
    "        self.encoded_texts = [\n",
    "            encoded_text + [pad_token_id] * (self.max_length - len(encoded_text))\n",
    "            for encoded_text in self.encoded_texts\n",
    "        ]\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        encoded = self.encoded_texts[index]\n",
    "        label = self.data.iloc[index][\"Label\"]\n",
    "        return (\n",
    "            torch.tensor(encoded, dtype=torch.long),\n",
    "            torch.tensor(label, dtype=torch.long),\n",
    "        )\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def _longest_encoded_length(self) -> int:\n",
    "        \"\"\"Determine the longest encoded text length.\"\"\"\n",
    "        return max(len(encoded_text) for encoded_text in self.encoded_texts)\n",
    "\n",
    "\n",
    "# Load the training dataset.\n",
    "train_dataset = SpamDataset(\n",
    "    csv_file=extracted_path / \"train.csv\", max_length=None, tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Load the validation and test sets and limit the max length to the same value as the training set.\n",
    "# NOTE: Importantly, any validation and test set samples exceeding the length of the longest\n",
    "#       training example are truncated using encoded_text[:self.max_length] in the SpamDataset code\n",
    "#       we defined earlier. This truncation is optional; you can set max_length=None for both\n",
    "#       validation and test sets, provided there are no sequences exceeding 1,024 tokens in these\n",
    "#       sets.\n",
    "val_dataset = SpamDataset(\n",
    "    csv_file=extracted_path / \"validation.csv\",\n",
    "    max_length=None,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "test_dataset = SpamDataset(\n",
    "    csv_file=extracted_path / \"test.csv\",\n",
    "    max_length=None,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Show the maximum length of the encoded texts.\n",
    "print(f\"Maximum length of the encoded texts: {train_dataset.max_length}\")\n",
    "print(f\"Maximum length of the encoded texts: {val_dataset.max_length}\")\n",
    "print(f\"Maximum length of the encoded texts: {test_dataset.max_length}\")\n",
    "\n",
    "# Verify that the maximum length does not exceed the context length.\n",
    "assert train_dataset.max_length <= GPT_CONFIG_124M.context_length\n",
    "assert val_dataset.max_length <= GPT_CONFIG_124M.context_length\n",
    "assert test_dataset.max_length <= GPT_CONFIG_124M.context_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This num_worker setting ensures compatibility with most computers.\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "# Show the size of the data loaders.\n",
    "print(f\"Train set size: {len(train_loader)}\")\n",
    "print(f\"Validation set size: {len(val_loader)}\")\n",
    "print(f\"Test set size: {len(test_loader)}\")\n",
    "\n",
    "# Show the first batch of the training set.\n",
    "# NOTE: As we can see, the input batches consist of eight training examples with 120 tokens each,\n",
    "#       as expected. The label tensor stores the class labels corresponding to the eight training\n",
    "#       examples.\n",
    "print(\"\\nFirst training batch:\")\n",
    "for input_batch, target_batch in train_loader:\n",
    "    print(\"Input batch dimensions:\", input_batch.shape)\n",
    "    print(\"Label batch dimensions\", target_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2: Model Setup\n",
    "\n",
    "![Model initialization](https://drek4537l1klr.cloudfront.net/raschka/Figures/6-8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing a model with pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "\n",
    "# Load the base config.\n",
    "GPT_CONFIG_124M = GPTConfig(\n",
    "    vocab_size=50257,  # as used by the BPE tokenizer for GPT-2.\n",
    "    context_length=1024,\n",
    "    emb_dim=768,\n",
    "    n_heads=12,\n",
    "    n_layers=12,\n",
    "    dropout_rate=0.0,  # disable dropout for inference\n",
    "    qkv_bias=False,\n",
    ")\n",
    "\n",
    "# Update the model configuration to conform to the model size.\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "# Instantiate a base config.\n",
    "tmp_config = dataclasses.asdict(GPT_CONFIG_124M)\n",
    "\n",
    "# Load the overlay parameters.\n",
    "model_name = \"gpt2-small (124M)\"\n",
    "tmp_config.update(model_configs[model_name])\n",
    "\n",
    "# Update the context length to match OpenAI's GPT-2 models.\n",
    "tmp_config.update({\"context_length\": 1024})\n",
    "\n",
    "# OpenAI used bias vectors in the multi-head attention module’s linear layers to implement the\n",
    "# query, key, and value matrix computations. Bias vectors are not commonly used in LLMs anymore as\n",
    "# they don’t improve the modeling performance and are thus unnecessary. However, since we are\n",
    "# working with pretrained weights, we need to match the settings for consistency and enable these\n",
    "# bias vectors.\n",
    "tmp_config.update({\"qkv_bias\": True})\n",
    "\n",
    "# Instantiate the new configuration.\n",
    "NEW_CONFIG = GPTConfig(**tmp_config)\n",
    "\n",
    "# Initialize the model with the new configuration.\n",
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This code is copied from chapter_05_pretraining_on_unlabeled_data.ipynb because the import\n",
    "#       from load_weights_into_gpt.py is not working.\n",
    "\n",
    "\n",
    "def assign(left, right):\n",
    "    \"\"\"Safely assign the right weight tensor to the left layer.\n",
    "\n",
    "    Checks whether two tensors or arrays (left and right) have the same dimensions or shape and\n",
    "    returns the right tensor as trainable PyTorch parameters.\n",
    "    \"\"\"\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, \" \"Right: {right.shape}\")\n",
    "\n",
    "    return torch.nn.Parameter(torch.tensor(right))\n",
    "\n",
    "\n",
    "def load_weights_into_gpt(gpt: GPTModel, params: dict):\n",
    "    # Sets the model’s positional and token embedding weights to those specified in params.\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params[\"wpe\"])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params[\"wte\"])\n",
    "\n",
    "    # Iterates over each transformer block in the model.\n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        # The np.split function is used to divide the attention and bias weights into three equal\n",
    "        # parts for the query, key, and value components.\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1\n",
    "        )\n",
    "        gpt.trf_blocks[b].mha.W_q.weight = assign(\n",
    "            gpt.trf_blocks[b].mha.W_q.weight, q_w.T\n",
    "        )\n",
    "        gpt.trf_blocks[b].mha.W_k.weight = assign(\n",
    "            gpt.trf_blocks[b].mha.W_k.weight, k_w.T\n",
    "        )\n",
    "        gpt.trf_blocks[b].mha.W_v.weight = assign(\n",
    "            gpt.trf_blocks[b].mha.W_v.weight, v_w.T\n",
    "        )\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1\n",
    "        )\n",
    "        gpt.trf_blocks[b].mha.W_q.bias = assign(gpt.trf_blocks[b].mha.W_q.bias, q_b)\n",
    "        gpt.trf_blocks[b].mha.W_k.bias = assign(gpt.trf_blocks[b].mha.W_k.bias, k_b)\n",
    "        gpt.trf_blocks[b].mha.W_v.bias = assign(gpt.trf_blocks[b].mha.W_v.bias, v_b)\n",
    "        gpt.trf_blocks[b].mha.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].mha.out_proj.weight,\n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T,\n",
    "        )\n",
    "        gpt.trf_blocks[b].mha.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].mha.out_proj.bias,\n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"],\n",
    "        )\n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].weight,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T,\n",
    "        )\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].bias, params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"]\n",
    "        )\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].weight,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T,\n",
    "        )\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].bias,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"],\n",
    "        )\n",
    "        gpt.trf_blocks[b].pre_attention_norm.scale = assign(\n",
    "            gpt.trf_blocks[b].pre_attention_norm.scale, params[\"blocks\"][b][\"ln_1\"][\"g\"]\n",
    "        )\n",
    "        gpt.trf_blocks[b].pre_attention_norm.shift = assign(\n",
    "            gpt.trf_blocks[b].pre_attention_norm.shift, params[\"blocks\"][b][\"ln_1\"][\"b\"]\n",
    "        )\n",
    "        gpt.trf_blocks[b].pre_ff_norm.scale = assign(\n",
    "            gpt.trf_blocks[b].pre_ff_norm.scale, params[\"blocks\"][b][\"ln_2\"][\"g\"]\n",
    "        )\n",
    "        gpt.trf_blocks[b].pre_ff_norm.shift = assign(\n",
    "            gpt.trf_blocks[b].pre_ff_norm.shift, params[\"blocks\"][b][\"ln_2\"][\"b\"]\n",
    "        )\n",
    "\n",
    "        gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "        gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "\n",
    "        # The original GPT-2 model by OpenAI reused the token embedding weights in the output layer\n",
    "        # to reduce the total number of parameters, which is a concept known as weight tying.\n",
    "        gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the GPT-2 weights.\n",
    "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")\n",
    "\n",
    "# Load the weights into the model.\n",
    "load_weights_into_gpt(gpt, params)\n",
    "gpt.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model to verify that it can generate coherent text.\n",
    "text_1 = \"Every effort moves you\"\n",
    "token_ids = generate_text_simple(\n",
    "    model=gpt.to(device),\n",
    "    idx=text_to_token_ids(text_1, tokenizer).to(device),\n",
    "    max_new_tokens=15,\n",
    "    context_size=NEW_CONFIG.context_length,\n",
    ")\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model is already capable of classifying spam and ham messages via instruction\n",
    "# examples.\n",
    "text_2 = (\n",
    "    \"Is the following text 'spam'? Answer with 'yes' or 'no':\"\n",
    "    \" 'You are a winner you have been specially\"\n",
    "    \" selected to receive $1000 cash or a $2000 award.'\"\n",
    ")\n",
    "token_ids = generate_text_simple(\n",
    "    model=gpt.to(device),\n",
    "    idx=text_to_token_ids(text_2, tokenizer).to(device),\n",
    "    max_new_tokens=23,\n",
    "    context_size=NEW_CONFIG.context_length,\n",
    ")\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify model for fine-tuning (adding a classification head)\n",
    "\n",
    "Adapting a GPT model for spam classification by altering its architecture. Initially, the model’s linear output layer mapped 768 hidden units to a vocabulary of 50,257 tokens. To detect spam, we replace this layer with a new output layer that maps the same 768 hidden units to just two classes, representing “spam” and “not spam.”\n",
    "\n",
    "### Fine-tuning selected layers vs. all layers\n",
    "\n",
    "Since we start with a pretrained model, it’s not necessary to fine-tune all model layers. In neural network-based language models, the lower layers generally capture basic language structures and semantics applicable across a wide range of tasks and datasets. So, fine-tuning only the last layers (i.e., layers near the output), which are more specific to nuanced linguistic patterns and task-specific features, is often sufficient to adapt the model to new tasks. A nice side effect is that it is computationally more efficient to fine-tune only a small number of layers.\n",
    "\n",
    "![Model modification](https://drek4537l1klr.cloudfront.net/raschka/Figures/6-9.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the model for fine-tuning.\n",
    "\n",
    "# 1. Freeze all parameters in the model.\n",
    "for param in gpt.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 2. Replace the final linear layer with a new one for the two classes.\n",
    "torch.manual_seed(123)\n",
    "num_classes = 2\n",
    "gpt.out_head = nn.Linear(GPT_CONFIG_124M.emb_dim, num_classes)\n",
    "\n",
    "# Mark additional layers as trainable, in particular the last transformer block as well as the\n",
    "# final layer norm.\n",
    "for param in gpt.trf_blocks[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "for param in gpt.final_norm.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try running the model with a random input to see that it is working.\n",
    "inputs_str = \"Do you have time\"\n",
    "inputs = tokenizer.encode(inputs_str)\n",
    "inputs = torch.tensor(inputs).unsqueeze(0)\n",
    "print(\"Inputs:\", inputs_str)\n",
    "print(\"Inputs dimensions:\", inputs.shape)  # B x T, i.e. batch size x sequence length\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = gpt.to(device)(inputs.to(device))\n",
    "\n",
    "# NOTE: The output shape is B x T x 2, i.e. batch size x sequence length x number of classes.\n",
    "#       The model produces logits for each class and for each token in the input sequence.\n",
    "# NOTE: We are interested in fine-tuning this model to return a class label indicating whether a\n",
    "#       model input is “spam” or “not spam.” We don’t need to fine-tune all four output rows;\n",
    "#       instead, we can focus on a single output token. In particular, we will focus on the last\n",
    "#       row corresponding to the last output token.\n",
    "print(\"Outputs:\\n\", outputs)\n",
    "print(\n",
    "    \"Outputs dimensions:\", outputs.shape\n",
    ")  # B x T x 2, i.e. batch size x sequence length x number of classes\n",
    "print(\"Last output token:\", outputs[:, -1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting the right output for fine-tuning\n",
    "\n",
    "![Output selection](https://drek4537l1klr.cloudfront.net/raschka/Figures/6-11.png)\n",
    "\n",
    "To understand why we are particularly interested in the last output token only let's take a look at the attention mechanism. We have already explored the attention mechanism, which establishes a relationship between each input token and every other input token, and the concept of a causal attention mask. This mask restricts a token’s focus to its current position and the those before it, ensuring that each token can only be influenced by itself and the preceding tokens (as shown below).\n",
    "\n",
    "The empty cells indicate masked positions due to the causal attention mask, preventing tokens from attending to future tokens. The values in the cells represent attention scores; the last token, time, is the only one that computes attention scores for all preceding tokens.\n",
    "\n",
    "The last token in a sequence accumulates the most information since it is the only token with access to data from all the previous tokens. Therefore, in our spam classification task, we focus on this last token during the fine-tuning process.\n",
    "\n",
    "![Output selection](https://drek4537l1klr.cloudfront.net/raschka/Figures/6-12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation utilities\n",
    "\n",
    "Similar to next token prediction, we use ```softmax``` to compute probabilities for the output logits, in particular, probabilities for each class (spam, not spam) - as shown below.\n",
    "\n",
    "![Computing classification probabilities](https://drek4537l1klr.cloudfront.net/raschka/Figures/6-14.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the probabilities for the last output token.\n",
    "probas = torch.softmax(outputs[:, -1, :], dim=-1)\n",
    "\n",
    "# Compute the class label.\n",
    "# NOTE: {\"ham\": 0, \"spam\": 1}\n",
    "label = torch.argmax(probas)\n",
    "print(\"Inputs:\", inputs_str)\n",
    "print(\"Class label:\", label.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A utility function for computing classification accuracy for a data loader.\n",
    "def calc_accuracy_loader(\n",
    "    data_loader: DataLoader,\n",
    "    model: GPTModel,\n",
    "    device: torch.device,\n",
    "    num_batches: int = None,\n",
    ") -> float:\n",
    "    \"\"\"Compute the accuracy of a model on a data loader.\n",
    "\n",
    "    Args:\n",
    "        data_loader: The data loader to compute the accuracy on.\n",
    "        model: The model to compute the accuracy on.\n",
    "        device: The device to compute the accuracy on.\n",
    "        num_batches: The number of batches to compute the accuracy on. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        The accuracy of the model on the data loader.\n",
    "    \"\"\"\n",
    "    # Set the model to evaluation mode (to avoid tracking gradients).\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize the number of correct predictions and the number of examples.\n",
    "    correct_predictions, num_examples = 0, 0\n",
    "\n",
    "    # If the number of batches is not specified, use all batches.\n",
    "    if num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "\n",
    "    # Iterate over the data loader.\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        # If the number of batches has not been reached, compute the accuracy.\n",
    "        if i < num_batches:\n",
    "            input_batch = input_batch.to(device)\n",
    "            target_batch = target_batch.to(device)\n",
    "\n",
    "            # Compute the logits for the last output token.\n",
    "            with torch.no_grad():\n",
    "                # NOTE: The output shape is B x T x 2, i.e. batch size x sequence length x number\n",
    "                #       of classes. Here, we are only interested in the logits for the last output\n",
    "                #       token.\n",
    "                logits = model(input_batch)[:, -1, :]\n",
    "\n",
    "            # Compute the predicted labels.\n",
    "            # NOTE: dim=-1 computes the argmax over the classes.\n",
    "            predicted_labels = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            # Update the number of examples and the number of correct predictions.\n",
    "            num_examples += predicted_labels.shape[0]\n",
    "            correct_predictions += (predicted_labels == target_batch).sum().item()\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return correct_predictions / num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute baseline accuracy for the not yet fine-tuned model.\n",
    "\n",
    "# Move the model to the device.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gpt.to(device)\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# Compute the accuracy for the training, validation, and test sets.\n",
    "train_accuracy = calc_accuracy_loader(train_loader, gpt, device, num_batches=10)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, gpt, device, num_batches=10)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, gpt, device, num_batches=10)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the loss function\n",
    "\n",
    "However, before we begin fine-tuning the model, we must define the loss function we will optimize during training. Our objective is to maximize the spam classification accuracy of the model, which means that the preceding code should output the correct class labels: 0 for non-spam and 1 for spam. Because classification accuracy is not a differentiable function, we use cross-entropy loss as a proxy to maximize accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(\n",
    "    input_batch: torch.Tensor,\n",
    "    target_batch: torch.Tensor,\n",
    "    model: GPTModel,\n",
    "    device: torch.device,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Compute the loss for a batch of inputs and targets.\n",
    "\n",
    "    Args:\n",
    "        input_batch: The input batch.\n",
    "        target_batch: The target batch.\n",
    "        model: The model.\n",
    "        device: The device.\n",
    "\n",
    "    Returns:\n",
    "        The loss for the batch.\n",
    "    \"\"\"\n",
    "    # Move the input and target batches to the device.\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "\n",
    "    # Compute the logits for the last output token.\n",
    "    logits = model(input_batch)[:, -1, :]\n",
    "\n",
    "    # Compute the loss (only for the last output token).\n",
    "    loss = torch.nn.functional.cross_entropy(logits, target_batch)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(\n",
    "    data_loader: DataLoader,\n",
    "    model: GPTModel,\n",
    "    device: torch.device,\n",
    "    num_batches: int = None,\n",
    ") -> float:\n",
    "    \"\"\"Compute the loss for a data loader.\n",
    "\n",
    "    Args:\n",
    "        data_loader: The data loader.\n",
    "        model: The model.\n",
    "        device: The device.\n",
    "        num_batches: The number of batches to compute the loss on.\n",
    "\n",
    "    Returns:\n",
    "        The loss for the data loader.\n",
    "    \"\"\"\n",
    "    # Initialize the total loss.\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # If the data loader is empty, return NaN.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    # If the number of batches is not specified, use all batches.\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "\n",
    "    # Iterate over the data loader.\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the loss for the training, validation, and test sets.\n",
    "\n",
    "# Disables gradient tracking for efficiency because we are not training yet\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, gpt, device, num_batches=5)\n",
    "    val_loss = calc_loss_loader(val_loader, gpt, device, num_batches=5)\n",
    "    test_loss = calc_loss_loader(test_loader, gpt, device, num_batches=5)\n",
    "\n",
    "print(f\"Training loss: {train_loss:.3f}\")\n",
    "print(f\"Validation loss: {val_loss:.3f}\")\n",
    "print(f\"Test loss: {test_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 3 - Model line-tuning and usage\n",
    "\n",
    "![Fine-tuning on supervised data](https://drek4537l1klr.cloudfront.net/raschka/Figures/6-15.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    model: GPTModel,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    device: torch.device,\n",
    "    eval_iter: int,\n",
    ") -> tuple[float, float]:\n",
    "    \"\"\"Evaluate a model on the training and validation sets.\n",
    "\n",
    "    Args:\n",
    "        model: The model to evaluate.\n",
    "        train_loader: The training data loader.\n",
    "        val_loader: The validation data loader.\n",
    "        device: The device to evaluate the model on.\n",
    "        eval_iter: The number of iterations between evaluations.\n",
    "\n",
    "    Returns:\n",
    "        train_loss: The training loss.\n",
    "        val_loss: The validation loss.\n",
    "    \"\"\"\n",
    "    # Set the model to evaluation mode.\n",
    "    model.eval()\n",
    "\n",
    "    # Compute the loss for the training and validation sets.\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(\n",
    "            train_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "\n",
    "    # Reset the model to training mode.\n",
    "    model.train()\n",
    "\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def train_classifier_simple(\n",
    "    model: GPTModel,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    device: torch.device,\n",
    "    num_epochs: int,\n",
    "    eval_freq: int,\n",
    "    eval_iter: int,\n",
    ") -> tuple[list[float], list[float], list[float], list[float], int, int]:\n",
    "    \"\"\"Train a classifier model.\n",
    "\n",
    "    Args:\n",
    "        model: The model to train.\n",
    "        train_loader: The training data loader.\n",
    "        val_loader: The validation data loader.\n",
    "        optimizer: The optimizer.\n",
    "        device: The device to train the model on.\n",
    "        num_epochs: The number of epochs to train the model.\n",
    "        eval_freq: The frequency of evaluation.\n",
    "        eval_iter: The number of iterations between evaluations.\n",
    "\n",
    "    Returns:\n",
    "        train_losses: The training losses.\n",
    "        val_losses: The validation losses.\n",
    "        train_accs: The training accuracies.\n",
    "        val_accs: The validation accuracies.\n",
    "        examples_seen: The number of examples seen.\n",
    "    \"\"\"\n",
    "    # Initialize the lists for the training and validation losses and accuracies.\n",
    "    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
    "    examples_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop.\n",
    "    for epoch in range(num_epochs):\n",
    "        # Sets model to training mode (to enable gradient tracking, drop out, etc.).\n",
    "        model.train()\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            # Resets loss gradients from the previous batch iteration.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Computes the loss for the current batch.\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "\n",
    "            # Calculates loss gradients.\n",
    "            loss.backward()\n",
    "\n",
    "            # Updates the model parameters using the computed loss gradients.\n",
    "            optimizer.step()\n",
    "\n",
    "            # Updates the number of examples seen and the global step.\n",
    "            examples_seen += input_batch.shape[0]\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step.\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model=model,\n",
    "                    train_loader=train_loader,\n",
    "                    val_loader=val_loader,\n",
    "                    device=device,\n",
    "                    eval_iter=eval_iter,\n",
    "                )\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                print(\n",
    "                    f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                    f\"Train loss {train_loss:.3f}, \"\n",
    "                    f\"Val loss {val_loss:.3f}\"\n",
    "                )\n",
    "\n",
    "        # Calculates accuracy after each epoch\n",
    "        train_accuracy = calc_accuracy_loader(\n",
    "            train_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "        val_accuracy = calc_accuracy_loader(\n",
    "            val_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "        print(f\"Training accuracy: {train_accuracy*100:.2f}% | \", end=\"\")\n",
    "        print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "        train_accs.append(train_accuracy)\n",
    "        val_accs.append(val_accuracy)\n",
    "\n",
    "    return train_losses, val_losses, train_accs, val_accs, examples_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Set the random seed and track training time.\n",
    "start_time = time.time()\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# Initialize the optimizer.\n",
    "optimizer = torch.optim.AdamW(gpt.parameters(), lr=5e-5, weight_decay=0.1)\n",
    "num_epochs = 5\n",
    "\n",
    "# Train the model.\n",
    "train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(\n",
    "    model=gpt,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    num_epochs=num_epochs,\n",
    "    eval_freq=50,\n",
    "    eval_iter=5,\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss visualization\n",
    "\n",
    "The model’s training and validation loss over the five training epochs. Both the training loss, represented by the solid line, and the validation loss, represented by the dashed line, sharply decline in the first epoch and gradually stabilize toward the fifth epoch.  This pattern indicates good learning progress and suggests that the model learned from the training data while generalizing well to the unseen validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation losses.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_values(\n",
    "    epochs_seen: torch.Tensor,\n",
    "    examples_seen: torch.Tensor,\n",
    "    train_values: list[float],\n",
    "    val_values: list[float],\n",
    "    label: str = \"loss\",\n",
    "):\n",
    "    \"\"\"Plot the training and validation losses.\n",
    "\n",
    "    Args:\n",
    "        epochs_seen: The number of epochs seen.\n",
    "        examples_seen: The number of examples seen.\n",
    "        train_values: The training values.\n",
    "        val_values: The validation values.\n",
    "        label: The label for the plot.\n",
    "    \"\"\"\n",
    "    # Create the plot.\n",
    "    fig, ax1 = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    # Plot the training and validation losses against the epochs.\n",
    "    ax1.plot(epochs_seen, train_values, label=f\"Training {label}\")\n",
    "    ax1.plot(epochs_seen, val_values, linestyle=\"-.\", label=f\"Validation {label}\")\n",
    "\n",
    "    # Set the x-axis label.\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(label.capitalize())\n",
    "    ax1.legend()\n",
    "\n",
    "    # Creates a second x-axis for examples seen\n",
    "    ax2 = ax1.twiny()\n",
    "\n",
    "    # Invisible plot for aligning ticks\n",
    "    ax2.plot(examples_seen, train_values, alpha=0)\n",
    "    ax2.set_xlabel(\"Examples seen\")\n",
    "\n",
    "    # Adjusts layout to make room.\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # Save the plot.\n",
    "    plt.savefig(f\"{label}-plot.pdf\")\n",
    "\n",
    "    # Show the plot.\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Create the epochs and examples seen tensors.\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))\n",
    "\n",
    "# Plot the values.\n",
    "plot_values(epochs_tensor, examples_seen_tensor, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification accuracy plot\n",
    "\n",
    "Both the training accuracy (solid line) and the validation accuracy (dashed line) increase substantially in the early epochs and then plateau, achieving almost perfect accuracy scores of 1.0. The close proximity of the two lines throughout the epochs suggests that the model does not overfit the training data very much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_accs))\n",
    "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_accs))\n",
    "\n",
    "plot_values(epochs_tensor, examples_seen_tensor, train_accs, val_accs, label=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance metrics on all data sets\n",
    "\n",
    "The training and test set performances are almost identical. The slight discrepancy between the training and test set accuracies suggests minimal overfitting of the training data. Typically, the validation set accuracy is somewhat higher than the test set accuracy because the model development often involves tuning hyperparameters to perform well on the validation set, which might not generalize as effectively to the test set. This situation is common, but the gap could potentially be minimized by adjusting the model’s settings, such as increasing the dropout rate (```drop_rate```) or the ```weight_decay``` parameter in the optimizer configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy = calc_accuracy_loader(train_loader, gpt, device)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, gpt, device)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, gpt, device)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the model for classification\n",
    "\n",
    "![Model usage](https://drek4537l1klr.cloudfront.net/raschka/Figures/6-18.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_review(\n",
    "    text: str,\n",
    "    model: GPTModel,\n",
    "    tokenizer: tiktoken.Encoding,\n",
    "    device: torch.device,\n",
    "    max_length: Optional[int] = None,\n",
    "    pad_token_id: int = 50256,\n",
    "):\n",
    "    \"\"\"Classify a review using a fine-tuned GPT model.\n",
    "\n",
    "    Args:\n",
    "        text: The review to classify.\n",
    "        model: The fine-tuned GPT model.\n",
    "        tokenizer: The tokenizer.\n",
    "        device: The device to classify the review on.\n",
    "        max_length: The maximum length of the review.\n",
    "        pad_token_id: The padding token ID (defaults to the end-of-text token).\n",
    "\n",
    "    Returns:\n",
    "        The predicted label.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    # Prepares inputs to the model\n",
    "    input_ids = tokenizer.encode(text)\n",
    "\n",
    "    # Determine the maximum supported context length.\n",
    "    supported_context_length = model.pos_emb.weight.shape[1]\n",
    "\n",
    "    # Truncates sequences if they are too long\n",
    "    input_ids = input_ids[: min(max_length, supported_context_length)]\n",
    "\n",
    "    # Determine the maximum sequence length.\n",
    "    max_length = max_length if max_length is not None else supported_context_length\n",
    "\n",
    "    # Pad sequences to the longest sequence length.\n",
    "    input_ids += [pad_token_id] * (max_length - len(input_ids))\n",
    "\n",
    "    # Add a batch dimension.\n",
    "    input_tensor = torch.tensor(input_ids, device=device).unsqueeze(0)\n",
    "\n",
    "    # Model inference without gradient tracking.\n",
    "    with torch.no_grad():\n",
    "        # Logits for the last output token.\n",
    "        logits = model(input_tensor)[:, -1, :]\n",
    "\n",
    "    predicted_label = torch.argmax(logits, dim=-1).item()\n",
    "\n",
    "    # Return the predicted label.\n",
    "    return \"spam\" if predicted_label == 1 else \"not spam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try classifying some examples.\n",
    "text_1 = (\n",
    "    \"You are a winner you have been specially\"\n",
    "    \" selected to receive $1000 cash or a $2000 award.\"\n",
    ")\n",
    "print(\n",
    "    classify_review(\n",
    "        text=text_1,\n",
    "        model=gpt,\n",
    "        tokenizer=tokenizer,\n",
    "        device=device,\n",
    "        max_length=train_dataset.max_length,\n",
    "    )\n",
    ")\n",
    "\n",
    "text_2 = (\n",
    "    \"Hey, just wanted to check if we're still on\" \" for dinner tonight? Let me know!\"\n",
    ")\n",
    "print(\n",
    "    classify_review(\n",
    "        text=text_2,\n",
    "        model=gpt,\n",
    "        tokenizer=tokenizer,\n",
    "        device=device,\n",
    "        max_length=train_dataset.max_length,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model checkpoint to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model checkpoint to disk.\n",
    "torch.save(gpt.state_dict(), \"review_classifier.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model checkpoint from disk.\n",
    "model_state_dict = torch.load(\"review_classifier.pth\", map_location=device)\n",
    "gpt.load_state_dict(model_state_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avpc-off-vehicle-qa-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
